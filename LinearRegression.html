<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Linear Regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<link href="site_libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="site_libs/plotly-main-1.39.2/plotly-latest.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics Notebook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    R Help
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RCommands.html">R Commands</a>
    </li>
    <li>
      <a href="RMarkdownHints.html">R Markdown Hints</a>
    </li>
    <li>
      <a href="RCheatSheetsAndNotes.html">R Cheatsheets &amp; Notes</a>
    </li>
    <li>
      <a href="DataSources.html">Data Sources</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Describing Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="GraphicalSummaries.html">Graphical Summaries</a>
    </li>
    <li>
      <a href="NumericalSummaries.html">Numerical Summaries</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Making Inference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="MakingInference.html">Making Inference</a>
    </li>
    <li>
      <a href="tTests.html">t Tests</a>
    </li>
    <li>
      <a href="WilcoxonTests.html">Wilcoxon Tests</a>
    </li>
    <li>
      <a href="ANOVA.html">ANOVA</a>
    </li>
    <li>
      <a href="Kruskal.html">Kruskal-Wallis</a>
    </li>
    <li>
      <a href="LinearRegression.html">Linear Regression</a>
    </li>
    <li>
      <a href="LogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="ChiSquaredTests.html">Chi Squared Tests</a>
    </li>
    <li>
      <a href="PermutationTests.html">Randomization Testing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Analyses
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./Analyses/AnalysisRubric.html">Analysis Rubric</a>
    </li>
    <li>
      <a href="./Analyses/StudentHousing.html">Good Example Analysis</a>
    </li>
    <li>
      <a href="./Analyses/StudentHousingPOOR.html">Poor Example Analysis</a>
    </li>
    <li>
      <a href="./Analyses/Rent.html">Rent Analysis</a>
    </li>
    <li>
      <a href="./Analyses/Stephanie.html">Stephanie Analysis</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Linear Regression</h1>

</div>


<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
 }
</script>
<hr />
<p>Determine which explanatory variables have a significant effect on the mean of the quantitative response variable.</p>
<hr />
<div id="simple-linear-regression" class="section level2 tabset tabset-fade tabset-pills">
<h2>Simple Linear Regression</h2>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYQuantX.png" width=58px;></p>
</div>
<p>Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable <span class="math inline">\(Y\)</span> and a single quantitative explanatory variable <span class="math inline">\(X\)</span>.</p>
<div id="overview" class="section level3 tabset">
<h3>Overview</h3>
<div style="padding-left:125px;">
<p><strong>Mathematical Model</strong></p>
<p>The true regression model assumed by a regression analysis is given by</p>
<div style="float:right;font-size:.8em;background-color:lightgray;padding:5px;border-radius:4px;">
<a style="color:darkgray;" href="javascript:showhide('simplelinearlatexrcode')">Math Code</a>
</div>
<div id="simplelinearlatexrcode" style="display:none;">
<pre><code>$$
  \underbrace{Y_i}_\text{Some Label} = \overbrace{\beta_0}^\text{y-int} + \overbrace{\beta_1}^\text{slope} \underbrace{X_i}_\text{Some Label} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)
$$</code></pre>
</div>
<center>
<span class="tooltipr"> <span class="math inline">\(Y_i\)</span> <span class="tooltiprtext">The response variable. The “i” denotes that this is the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to <span class="math inline">\(n\)</span>, the sample size.</span> </span><span class="tooltipr"> <span class="math inline">\(=\)</span> <span class="tooltiprtext">This states that we are assuming <span class="math inline">\(Y_i\)</span> was created, or is “equal to” the formula that will follow on the right-hand-side of the equation.</span> </span><span class="tooltipr"> <span class="math inline">\(\underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X_i \ }_\text{true regression relation}\)</span> <span class="tooltiprtext">The true regression relation is a line, a line that is typically unknown in real life. It can be likened to “God’s Law” or “Natural Law”. Something that governs the way the data behaves, but is unkown to us.</span> </span><span class="tooltipr"> <span class="math inline">\(+\)</span> <span class="tooltiprtext">This plus sign emphasizes that the actual data, the <span class="math inline">\(Y_i\)</span>, is created by adding together the value from the true line <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> and an individual error term <span class="math inline">\(\epsilon_i\)</span>, which allows each dot in the regression to be off of the line by a certain amount called <span class="math inline">\(\epsilon_i\)</span>.</span> </span><span class="tooltipr"> <span class="math inline">\(\overbrace{\epsilon_i}^\text{error term}\)</span> <span class="tooltiprtext">Error term for each individual <span class="math inline">\(i\)</span>. The error terms are “random” and unique for each individual. This provides the statistical relationship of the regression. It is what allows each dot to be different, while still coming from the same line, or underlying law.</span> </span><span class="tooltipr"> <span class="math inline">\(\quad \text{where}\)</span> <span class="tooltiprtext">Some extra comments are needed about <span class="math inline">\(\epsilon_i\)</span>…</span> </span><span class="tooltipr"> <span class="math inline">\(\ \overbrace{\epsilon_i \sim N(0, \sigma^2)}^\text{error term normally distributed}\)</span> <span class="tooltiprtext">The error terms <span class="math inline">\(\epsilon_i\)</span> are assumed to be normally distributed with constant variance. Pay special note that the <span class="math inline">\(\sigma\)</span> does not have an <span class="math inline">\(i\)</span> in it, so it is the same for each individual. In other words, the variance is constant. The mean of the errors is zero, which causes the dots to be spread out symmetrically both above and below the line.</span> </span>
</center>
<p><br/></p>
<p>The estimated regression line obtained from a regression analysis, pronounced “y-hat”, is written as</p>
<div style="float:right;font-size:.8em;background-color:lightgray;padding:5px;border-radius:4px;">
<a style="color:darkgray;" href="javascript:showhide('simplelinearlatexrcodeyhat')">Math Code</a>
</div>
<div id="simplelinearlatexrcodeyhat" style="display:none;">
<pre><code>$$
  \underbrace{\hat{Y}_i}_\text{Some Label} = \overbrace{b_0}^\text{est. y-int} + \overbrace{b_1}^\text{est. slope} \underbrace{X_i}_\text{Some Label}
$$</code></pre>
</div>
<center>
<span class="tooltipr"> <span class="math inline">\(\hat{Y}_i\)</span> <span class="tooltiprtext">The estimated average y-value for individual <span class="math inline">\(i\)</span> is denoted by <span class="math inline">\(\hat{Y}_i\)</span>. It is important to recognize that <span class="math inline">\(Y_i\)</span> is the actual value for individual <span class="math inline">\(i\)</span>, and <span class="math inline">\(\hat{Y}_i\)</span> is the average y-value for all individuals with the same <span class="math inline">\(X_i\)</span> value.</span> </span><span class="tooltipr"> <span class="math inline">\(=\)</span> <span class="tooltiprtext">The formula for the average y-value, <span class="math inline">\(\hat{Y}_i\)</span> is equal to what follows…</span> </span><span class="tooltipr"> <span class="math inline">\(\underbrace{\overbrace{\ b_0 \ }^\text{y-intercept} + \overbrace{b_1}^\text{slope} X_i \ }_\text{estimated regression relation}\)</span> <span class="tooltiprtext">Two things are important to notice about this equation. First, it uses <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> instead of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. This is because <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are the estimated y-intercept and slope, respectively, not the true y-intercept <span class="math inline">\(\beta_0\)</span> and true slope <span class="math inline">\(\beta_1\)</span>. Second, this equation does not include <span class="math inline">\(\epsilon_i\)</span>. In other words, it is the estimated regression line, so it only describes the average y-values, not the actual y-values.</span> </span>
</center>
<p><br/></p>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab <strong>The Mathematical Model</strong> for details about these equations.</p>
</div>
<p><strong>Hypotheses</strong></p>
<div style="float:right;font-size:.8em;background-color:lightgray;padding:5px;border-radius:4px;">
<a style="color:darkgray;" href="javascript:showhide('simplelinearhypecodeslope')">Math Code</a>
</div>
<div id="simplelinearhypecodeslope" style="display:none;">
<pre><code>$$
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}
$$

$$
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}
$$</code></pre>
</div>
<div style="clear:right;">

</div>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}^{\quad \text{(most common)}}\quad\quad
\]</span></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}^{\quad\text{(sometimes useful)}}
\]</span></p>
<p><br/></p>
<p>If <span class="math inline">\(\beta_1 = 0\)</span>, then the model reduces to <span class="math inline">\(Y_i = \beta_0 + \epsilon_i\)</span>, which is a flat line. This means <span class="math inline">\(X\)</span> does not improve our understanding of the mean of <span class="math inline">\(Y\)</span> if the null hypothesis is true.</p>
<p>If <span class="math inline">\(\beta_0 = 0\)</span>, then the model reduces to <span class="math inline">\(Y_i = \beta_1 X + \epsilon_i\)</span>, a line going through the origin. This means the average <span class="math inline">\(Y\)</span>-value is <span class="math inline">\(0\)</span> when <span class="math inline">\(X=0\)</span> if the null hypothesis is true.</p>
<p><strong>Assumptions</strong></p>
<p>This regression model is appropriate for the data when five assumptions can be made.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear Relation</strong>: the true regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</p></li>
<li><p><strong>Normal Errors</strong>: the error terms <span class="math inline">\(\epsilon_i\)</span> are normally distributed with a mean of zero.</p></li>
<li><p><strong>Constant Variance</strong>: the variance <span class="math inline">\(\sigma^2\)</span> of the error terms is constant (the same) over all <span class="math inline">\(X_i\)</span> values.</p></li>
<li><p><strong>Fixed X</strong>: the <span class="math inline">\(X_i\)</span> values can be considered fixed and measured without error.</p></li>
<li><p><strong>Independent Errors</strong>: the error terms <span class="math inline">\(\epsilon_i\)</span> are independent.</p></li>
</ol>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab <strong>Residual Plots &amp; Regression Assumptions</strong> for details about checking the regression assumptions.</p>
</div>
<p><strong>Interpretation</strong></p>
<p>The slope is interpreted as, “the change in the average y-value for a one unit change in the x-value.” It <strong>is not</strong> the average change in y. <strong>It is</strong> the change in the average y-value.</p>
<p>The y-intercept is interpreted as, “the average y-value when x is zero.” It is often not meaningful, but is sometimes useful. It just depends if x being zero is meaningful or not within the context of your analysis. For example, knowing the average price of a car with zero miles is useful. However, pretending to know the average height of adult males that weigh zero pounds, is not useful.</p>
<hr />
</div>
</div>
<div id="r-instructions" class="section level3">
<h3>R Instructions</h3>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p><strong>Perform the Regression</strong></p>
<a href="javascript:showhide('simplelinearrcode')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm <span class="tooltiprtext">This is some name you come up with that will become the R object that stores the results of your linear regression <code>lm(...)</code> command.</span> </span><span class="tooltipr">  &lt;-  <span class="tooltiprtext">This is the “left arrow” assignment operator that stores the results of your <code>lm()</code> code into <code>mylm</code> name.</span> </span><span class="tooltipr"> lm( <span class="tooltiprtext">lm(…) is an R function that stands for “Linear Model”. It performs a linear regression analysis for Y ~ X.</span> </span><span class="tooltipr"> Y  <span class="tooltiprtext">Y is your quantitative response variable. It is the name of one of the columns in your data set.</span> </span><span class="tooltipr"> ~  <span class="tooltiprtext">The tilde symbol ~ is used to tell R that Y should be treated as the response variable that is being explained by the explanatory variable X.</span> </span><span class="tooltipr"> X, <span class="tooltiprtext">X is the quantitative explanatory variable (at least it is typically quantitative but could be qualitative) that will be used to explain the average Y-value.</span> </span><span class="tooltipr">  data = NameOfYourDataset <span class="tooltiprtext">NameOfYourDataset is the name of the dataset that contains Y and X. In other words, one column of your dataset would be your response variable Y and another column would be your explanatory variable X.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for the lm(…) function.</span> </span><br/><span class="tooltipr"> summary(mylm) <span class="tooltiprtext">The <code>summary</code> command allows you to print the results of your linear regression that were previously saved in <code>mylm</code> name.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="simplelinearrcode" style="display:none;">
<p>Example output from a regression. Hover each piece to learn more.</p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Call:<br/> lm(formula = dist ~ speed, data = cars) <span class="tooltiprouttext">This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…).</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Residuals: <span class="tooltiprouttext">Residuals are the vertical difference between each point and the line, <span class="math inline">\(Y_i - \hat{Y}_i\)</span>. The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. </span>
</td>
</tr>
<tr>
<td align="right">
<span class="tooltiprout"> min<br/>   -29.069 <span class="tooltiprouttext">“min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1Q<br/>   -9.525 <span class="tooltiprouttext">“1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Median<br/>   -2.272 <span class="tooltiprouttext">“Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -2.272 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-2.272) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3Q<br/>   9.215 <span class="tooltiprouttext">“3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Max</br>   43.201 <span class="tooltiprouttext">“Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Coefficients: <span class="tooltiprouttext">Notice that in your lm(…) you used only <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. You did type out any coefficients, i.e., the <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span> of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.</span> </span>
</td>
</tr>
<tr>
<td align="left">
</td>
<td align="right">
<span class="tooltiprout">   Estimate <span class="tooltiprouttext">To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details.</span>
</td>
<td align="right">
<span class="tooltiprout">   Std. Error <span class="tooltiprouttext">To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   t value <span class="tooltiprouttext">To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   Pr(&gt;|t|) <span class="tooltiprouttext">The “Pr” stands for “Probability” and the “(&gt; |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero.<br/> To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. </span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span class="tooltiprouttext">This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   -17.5791 <span class="tooltiprouttext">This is the estimate of the y-intercept, <span class="math inline">\(\beta_0\)</span>. It is called <span class="math inline">\(b_0\)</span>. It is the average y-value when X is zero.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   6.7584 <span class="tooltiprouttext">This is the standard error of <span class="math inline">\(b_0\)</span>. It tells you how much <span class="math inline">\(b_0\)</span> varies from sample to sample. The closer to zero, the better.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> -2.601 <span class="tooltiprouttext">This is the test statistic t for the test of <span class="math inline">\(\beta_0 = 0\)</span>. It is calculated by dividing the “Estimate” of the intercept (-17.5791) by its standard error (6.7584). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -17.5791 is -2.601 standard errors (6.7584) from zero, which is a fairly surprising distance as shown by the p-value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.0123 <span class="tooltiprouttext">This is the p-value of the test of the hypothesis that <span class="math inline">\(\beta_0 = 0\)</span>. It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use <code>pt(-abs(your t-value), df of your regression)*2</code>.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> * <span class="tooltiprouttext">This is called a “star”. One star means significant at the 0.1 level of <span class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> speed <span class="tooltiprouttext">This is always the name of your X-variable in your lm(Y ~ X, …).</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   3.9324 <span class="tooltiprouttext">This is the estimate of the slope, <span class="math inline">\(\beta_1\)</span>. It is called <span class="math inline">\(b_1\)</span>. It is the change in the average y-value as X is increased by 1 unit.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.4155 <span class="tooltiprouttext">This is the standard error of <span class="math inline">\(b_1\)</span>. It tells you how much <span class="math inline">\(b_1\)</span> varies from sample to sample. The closer to zero, the better.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 9.464 <span class="tooltiprouttext">This is the test statistic t for the test of <span class="math inline">\(\beta_1 = 0\)</span>. It is calculated by dividing the “Estimate” of the slope (3.9324) by its standard error (0.4155). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from zero, which is a really surprising distance as shown by the smallness of the p-value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1.49e-12 <span class="tooltiprouttext">This is the p-value of the test of the hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>. To compute it yourself in R, use <code>pt(-abs(your t-value), df of your regression)*2</code></span> </span>
</td>
<td align="left">
<span class="tooltiprout"> *** <span class="tooltiprouttext">This is called a “star”. Three stars means significant at the 0.01 level of <span class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span> --- </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘’ 1 <span class="tooltiprouttext">These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Residual standard error: <span class="tooltiprouttext">This is the estimate of <span class="math inline">\(\sigma\)</span> in the regression model <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)</span> where <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>. It is the square root of the MSE.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  15.38 <span class="tooltiprouttext">For this particular regression, the estimate of <span class="math inline">\(\sigma\)</span> is 15.38. Squaring this number gives you the MSE, which is the estimate of <span class="math inline">\(\sigma^2\)</span>.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 48 degrees of freedom <span class="tooltiprouttext">This is <span class="math inline">\(n-p\)</span> where <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(p\)</span> is the number of parameters in the regression model. In this case, there is a sample size of 50 and two parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, so 50-2 = 48.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Multiple R-squared: <span class="tooltiprouttext">This is <span class="math inline">\(R^2\)</span>, the percentage of variation in <span class="math inline">\(Y\)</span> that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.6511, <span class="tooltiprouttext">In this particular regression, 65.11% of the variation in stopping distance <code>dist</code> is explained by the regression model using speed of the car.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  Adjusted R-squared: <span class="tooltiprouttext">The adjusted R-squared will always be at least slightly smaller than <span class="math inline">\(R^2\)</span>. The closer to R-squared that it is, the better. When it differs dramatically from <span class="math inline">\(R^2\)</span>, it is a sign that the regression model is over-fitting the data.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.6438 <span class="tooltiprouttext">In this case, the value of 0.6438 is quite close to the original <span class="math inline">\(R^2\)</span> value, so there is no fear of over-fitting with this particular model. That is good.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> F-statistic: <span class="tooltiprouttext">The F-statistic is found as the ratio of the MSR/MSP where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample size and p is the number of parameters in the regression model.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  89.57 <span class="tooltiprouttext">This is the value of the F-statistic for the lm(dist ~ speed, data=cars) regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p = 50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum( (cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR / MSE = 21185.46 / 236.5317 = 89.56711.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 1 and 48 DF, <span class="tooltiprouttext">The 1 degree of freedom is the SSR degrees of freedom (p-1). The 48 is the SSE degrees of freedom (n-p).</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  p-value: 1.49e-12 <span class="tooltiprouttext">The p-value for an F-statistic is found by the code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability of being more extreme than the observed F-statistic in an F distribution with 1 and 48 degrees of freedom.</span> </span>
</td>
</tr>
</table>
</div>
<p><br/></p>
<p><strong>Check Assumptions 1, 2, 3, and 5</strong></p>
<a href="javascript:showhide('assumptionplots')">
<div class="hoverchunk">
<p><span class="tooltipr"> par( <span class="tooltiprtext">The par(…) command stands for “Graphical PARameters”. It allows you to control various aspects of graphics in Base R.</span> </span><span class="tooltipr"> mfrow= <span class="tooltiprtext">This stands for “multiple frames filled by row”, which means, put lots of plots on the same row, starting with the plot on the left, then working towards the right as more plots are created.</span> </span><span class="tooltipr"> c( <span class="tooltiprtext">The combine function c(…) is used to specify how many rows and columns of graphics should be placed together.</span> </span><span class="tooltipr"> 1, <span class="tooltiprtext">This specifies that 1 row of graphics should be produced.</span> </span><span class="tooltipr"> 3 <span class="tooltiprtext">This states that 3 columns of graphics should be produced.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for c(…) function.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for par(…) function.</span> </span><br/><span class="tooltipr"> plot( <span class="tooltiprtext">This version of plot(…) will actually create several regression diagnostic plots by default.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of an lm object that you created previously.</span> </span><span class="tooltipr"> which= <span class="tooltiprtext">This allows you to select “which” regression diagnostic plots should be drawn.</span> </span><span class="tooltipr"> 1 <span class="tooltiprtext">Selecting 1, would give the residuals vs. fitted values plot only.</span> </span><span class="tooltipr"> : <span class="tooltiprtext">The colon allows you to select more than just one plot.</span> </span><span class="tooltipr"> 2 <span class="tooltiprtext">Selecting 2 also gives the Q-Q Plot of residuals. If you wanted to instead you could just use which=1 to get the residuals vs fitted values plot, then you could use qqPlot(mylm$residuals) to create a fancier Q-Q Plot of the residuals.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for plot(…) function.</span> </span><br/><span class="tooltipr"> plot( <span class="tooltiprtext">This version of plot(…) will be used to create a time-ordered plot of the residuals. The order of the residuals is the original order of the x-values in the original data set. If the original data set doesn’t have an order, then this plot is not interesting.</span> </span><span class="tooltipr"> mylm <span class="tooltiprtext">The lm object that you created previously.</span> </span><span class="tooltipr"> $ <span class="tooltiprtext">This allows you to access various elements from the regression that was performed.</span> </span><span class="tooltipr"> residuals <span class="tooltiprtext">This grabs the residuals for each observation in the regression.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for plot(…) function.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="assumptionplots" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<p><br/></p>
<p><strong>Plotting the Regression Line</strong></p>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'BaseScatterplot')">
Base R
</button>
<button class="tablinks" onclick="openTab(event, 'ggplotScatterplot')">
ggplot2
</button>
</div>
<div id="BaseScatterplot" class="tabcontent">
<p>
<p>To add the regression line to a scatterplot use the <code>abline(...)</code> command:</p>
<a href="javascript:showhide('regressionline')">
<div class="hoverchunk">
<p><span class="tooltipr"> plot( <span class="tooltiprtext">The plot(…) function is used to create a scatterplot with a y-axis (the vertical axis) and an x-axis (the horizontal axis).</span> </span><span class="tooltipr"> Y  <span class="tooltiprtext">This is the “response variable” of your regression. The thing you are interested in predicting. This is the name of a “numeric” column of data from the data set called YourDataSet.</span> </span><span class="tooltipr"> ~  <span class="tooltiprtext">The tilde “~” is used to relate Y to X and can be found on the top-left key of your keyboard.</span> </span><span class="tooltipr"> X,  <span class="tooltiprtext">This is the explanatory variable of your regression. It is the name of a “numeric” column of data from YourDataSet. .</span> </span><span class="tooltipr"> data= <span class="tooltiprtext">The data= statement is used to specify the name of the data set where the columns of “X” and “Y” are located.</span> </span><span class="tooltipr"> YourDataSet <span class="tooltiprtext">This is the name of your data set, like KidsFeet or cars or airquality.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for plot(…) function.</span> </span><br/><span class="tooltipr"> abline( <span class="tooltiprtext">This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line.</span> </span><span class="tooltipr"> mylm <span class="tooltiprtext">This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…).</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for abline(…) function.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="regressionline" style="display:none;">
<pre><code>mylm &lt;- lm(dist ~ speed, data = cars)
plot(dist ~ speed, data = cars)
abline(mylm)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<p>You can customize the look of the regression line with</p>
<a href="javascript:showhide('regressionlinecolor')">
<div class="hoverchunk">
<p><span class="tooltipr"> abline( <span class="tooltiprtext">This stands for “a” (intercept) “b” (slope) line. It is a function that allows you to add a line to a plot by specifying just the intercept and slope of the line.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of an lm(…) that you created previoiusly. Since mylm contains the slope and intercept of the estimated line, the abline(…) function will locate these two values from within mylm and use them to add a line to your current plot(…).</span> </span><span class="tooltipr"> lty= <span class="tooltiprtext">The lty= stands for “line type” and allows you to select between 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash.</span> </span><span class="tooltipr"> 1, <span class="tooltiprtext">This creates a solid line. Remember, other options include: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash.</span> </span><span class="tooltipr"> lwd= <span class="tooltiprtext">The lwd= allows you to specify the width of the line. The default width is 1. Using lwd=2 would double the thickness, and so on. Any positive value is allowed.</span> </span><span class="tooltipr"> 1, <span class="tooltiprtext">Default line width. To make a thicker line, us 2 or 3… To make a thinner line, try 0.5, but 1 is already pretty thin.</span> </span><span class="tooltipr"> col= <span class="tooltiprtext">This allows you to specify the color of the line using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque).</span> </span><span class="tooltipr"> “someColor” <span class="tooltiprtext">Type colors() in R for options.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for abline(…) function.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="regressionlinecolor" style="display:none;">
<pre><code>mylm &lt;- lm(dist ~ speed, data = cars)
plot(dist ~ speed, data = cars)
abline(mylm, lty=1, lwd=1, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<p>You can add points to the regression with…</p>
<a href="javascript:showhide('regressionaddpoints')">
<div class="hoverchunk">
<p><span class="tooltipr"> points( <span class="tooltiprtext">This is like plot(…) but adds points to the current plot(…) instead of creating a new plot.</span> </span><span class="tooltipr"> newY  <span class="tooltiprtext">newY should be a column of values from some data set. Or, use points(newX, newY) to add a single point to a graph.</span> </span><span class="tooltipr"> ~  <span class="tooltiprtext">This links Y to X in the plot.</span> </span><span class="tooltipr"> newX,  <span class="tooltiprtext">newX should be a column of values from some data set. It should be the same length as newY. If just a single value, use points(newX, newY) instead.</span> </span><span class="tooltipr"> data=YourDataSet,  <span class="tooltiprtext">If newY and newX come from a dataset, then use data= to tell the points(…) function what data set they come from. If newY and newX are just single values, then data= is not needed.</span> </span><span class="tooltipr"> col=“skyblue”, <span class="tooltiprtext">This allows you to specify the color of the points using either a name of a color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red, percentage green, percentage blue, percent opaque).</span> </span><span class="tooltipr"> pch=16 <span class="tooltiprtext">This allows you to specify the type of plotting symbol to be used for the points. Type ?pch and scroll half way down in the help file that appears to learn about other possible symbols.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for points(…) function.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="regressionaddpoints" style="display:none;">
<pre><code>mylm &lt;- lm(dist ~ speed, data = cars)
plot(dist ~ speed, data = cars)
points(7,40, pch=16, col=&quot;skyblue&quot;, cex=2)
text(7,40, &quot;New Dot&quot;, pos=3, cex=0.5)
points(dist ~ speed, data=filter(cars, mylm$res &gt; 2), cex=.8, col=&quot;red&quot;)
abline(mylm, lty=1, lwd=1, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</p>
</div>
<div id="ggplotScatterplot" class="tabcontent">
<p>
<p>To add the regression line to a scatterplot using the ggplot2 approach, first ensure:</p>
<p><code>library(ggplot2)</code> or <code>library(tidyverse)</code></p>
<p>is loaded. Then, use the <code>geom_smooth(method = lm)</code> command:</p>
<a href="javascript:showhide('ggplot')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic.</span> </span><span class="tooltipr"> YourDataSet,  <span class="tooltiprtext">This is simply the name of your data set, like KidsFeet or starwars.</span> </span><span class="tooltipr"> aes( <span class="tooltiprtext">aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables.</span> </span><span class="tooltipr"> x =  <span class="tooltiprtext">“x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax.</span> </span><span class="tooltipr"> X, <span class="tooltiprtext">This is the explanatory variable of the regression: the variable used to <em>explain</em> the mean of y. It is the name of the “numeric” column of YourDataSet.</span> </span><span class="tooltipr">  y =  <span class="tooltiprtext">“y= ” declares which variable will become the y-axis of the graphic.</span> </span><span class="tooltipr"> Y <span class="tooltiprtext">This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for aes(…) function.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for ggplot(…) function.</span> </span><span class="tooltipr"> + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.</span> </span><br/><span class="tooltipr">   geom_point() <span class="tooltiprtext">geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.</span> </span><span class="tooltipr"> + <span class="tooltiprtext">Here the + is used to add yet another layer to ggplot().</span> </span><br/><span class="tooltipr">   geom_smooth( <span class="tooltiprtext">geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot.</span> </span><span class="tooltipr"> method =  <span class="tooltiprtext">Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve..</span> </span><span class="tooltipr"> “lm”, <span class="tooltiprtext">lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.</span> </span><span class="tooltipr">  se = FALSE <span class="tooltiprtext">se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for the geom_smooth() function.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="ggplot" style="display:none;">
<pre><code>ggplot(cars, aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se=FALSE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<p>There are a number of ways to customize the appearance of the regression line:</p>
<a href="javascript:showhide('ggplotline')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic.</span> </span><span class="tooltipr"> cars,  <span class="tooltiprtext">This is simply the name of your data set, like KidsFeet or starwars.</span> </span><span class="tooltipr"> aes( <span class="tooltiprtext">aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables.</span> </span><span class="tooltipr"> x =  <span class="tooltiprtext">“x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax.</span> </span><span class="tooltipr"> speed,  <span class="tooltiprtext">This is the explanatory variable of the regression: the variable used to <em>explain</em> the mean of y. It is the name of the “numeric” column of YourDataSet.</span> </span><span class="tooltipr"> y =  <span class="tooltiprtext">“y= ” declares which variable will become the y-axis of the grpahic.</span> </span><span class="tooltipr"> dist <span class="tooltiprtext">This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for aes(…) function.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for ggplot(…) function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.</span> </span><br/><span class="tooltipr">   geom_point() <span class="tooltiprtext">geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">Here the + is used to add yet another layer to ggplot().</span> </span><br/><span class="tooltipr">   geom_smooth( <span class="tooltiprtext">geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot.</span> </span><span class="tooltipr"> method =  <span class="tooltiprtext">Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve..</span> </span><span class="tooltipr"> “lm”, <span class="tooltiprtext">lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.</span> </span><span class="tooltipr">  size = 2, <span class="tooltiprtext">Use <em>size = 2</em> to adjust the thickness of the line to size 2.</span> </span><span class="tooltipr">  color = “orange”, <span class="tooltiprtext">Use <em>color = “orange”</em> to change the color of the line to orange.</span> </span><br><span class="tooltipr">   linetype = “dashed”, <span class="tooltiprtext">Use <em>linetype = “dashed”</em> to change the solid line to a dashed line. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc.</span> </span><span class="tooltipr">  se = FALSE <span class="tooltiprtext">se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for the geom_smooth() function.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="ggplotline" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<p>In addition to customizing the regression line, you can customize the points, add points, add lines, and much more.</p>
<a href="javascript:showhide('ggplotpoints')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">Every ggplot2 graphic begins with the ggplot() command, which creates a framework, or coordinate system, that you can add layers to. Without adding any layers, ggplot() produces a blank graphic.</span> </span><span class="tooltipr"> cars,  <span class="tooltiprtext">This is simply the name of your data set, like KidsFeet or starwars.</span> </span><span class="tooltipr"> aes( <span class="tooltiprtext">aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the coordinate system, like x and y variables.</span> </span><span class="tooltipr"> x =  <span class="tooltiprtext">“x = ” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ” are optional phrasesin the ggplot2 syntax.</span> </span><span class="tooltipr"> speed,  <span class="tooltiprtext">This is the explanatory variable of the regression: the variable used to <em>explain</em> the mean of y. It is the name of the “numeric” column of YourDataSet.</span> </span><span class="tooltipr"> y =  <span class="tooltiprtext">“y= ” declares which variable will become the y-axis of the grpahic.</span> </span><span class="tooltipr"> dist <span class="tooltiprtext">This is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for aes(…) function.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for ggplot(…) function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.</span> </span><br/><span class="tooltipr">   geom_point( <span class="tooltiprtext">geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.</span> </span><span class="tooltipr"> size = 1.5, <span class="tooltiprtext">Use <em>size = 1.5</em> to change the size of the points.</span> </span><span class="tooltipr">  color = “skyblue” <span class="tooltiprtext">Use <em>color = “skyblue”</em> to change the color of the points to Brother Saunders’ favorite color.</span> </span><span class="tooltipr">  alpha = 0.5 <span class="tooltiprtext">Use <em>alpha = 0.5</em> to change the transparency of the points to 0.5.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of geom_point() function. </span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot().</span> </span><br><span class="tooltipr">   geom_smooth( <span class="tooltiprtext">geom_smooth() is a smoothing function that you can use to add different lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot.</span> </span><span class="tooltipr"> method =  <span class="tooltiprtext">Use “method = ” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line or curve..</span> </span><span class="tooltipr"> “lm”, <span class="tooltiprtext">lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line onto the graphic. The regression line is modeled using y ~ x, which variables were declared in the initial ggplot() aesthetic.</span> </span><span class="tooltipr">  color = “navy”, <span class="tooltiprtext">Use <em>color = “navy”</em> to change the color of the line to navy blue.</span> </span><span class="tooltipr">  size = 1.5, <span class="tooltiprtext">Use <em>size = 1.5</em> to adjust the thickness of the line to 1.5.</span> </span><span class="tooltipr">  se = FALSE <span class="tooltiprtext">se stands for “standard error”. Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of geom_smooth() function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot().</span> </span><br><span class="tooltipr">   geom_hline( <span class="tooltiprtext">Use geom_hline() to add a horizontal line at a specified y-intercept. You can also use geom_vline(xintercept = some_number) to add a vertical line to the graph.</span> </span><span class="tooltipr"> yintercept = <span class="tooltiprtext">Use “yintercept =” to tell geom_hline() that you are going to declare a y intercept for the horizontal line.</span> </span><span class="tooltipr">  75 <span class="tooltiprtext">75 is the value of the y-intercept.</span> </span><span class="tooltipr"> , color = “firebrick” <span class="tooltiprtext">Use <em>color = “firebrick”</em> to change the color of the horizontal line to firebrick red.</span> </span><span class="tooltipr"> , size = 1, <span class="tooltiprtext">Use <em>size = 1</em> to adjust the thickness of the horizontal line to size 1.</span> </span><br><span class="tooltipr">              linetype = “longdash” <span class="tooltiprtext">Use <em>linetype = “longdash”</em> to change the solid line to a dashed line with longer dashes. Some linetype options include “dashed”, “dotted”, “longdash”, “dotdash”, etc.</span> </span><span class="tooltipr"> , alpha = 0.5 <span class="tooltiprtext">Use <em>alpha = 0.5</em> to change the transparency of the horizontal line to 0.5.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of geom_hline function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot().</span> </span><br><span class="tooltipr">   geom_segment( <span class="tooltiprtext">geom_segment() allows you to add a line segment to ggplot() by using specified start and end points.</span> </span><span class="tooltipr"> x = <span class="tooltiprtext">“x =” tells geom_segment() that you are going to declare the x-coordinate for the starting point of the line segment.</span> </span><span class="tooltipr">  14, <span class="tooltiprtext">14 is a number on the x-axis of your graph. It is the x-coordinate of the starting point of the line segment.</span> </span><span class="tooltipr">  y =<br />
<span class="tooltiprtext">“y =” tells geom_segment() that you are going to declare the y-coordinate for the starting point of the line segment.</span> </span><span class="tooltipr">  75, <span class="tooltiprtext">75 is a number on the y-axis of your graph. It is the y-coordinate of the starting point of the line segment.</span> </span><span class="tooltipr">  xend = <span class="tooltiprtext">“xend =” tells geom_segment() that you are going to declare the x-coordinate for the end point of the line segment.</span> </span><span class="tooltipr">  14, <span class="tooltiprtext">14 is a number on the x-axis of your graph. It is the x-coordinate of the end point of the line segment.</span> </span><span class="tooltipr">  yend = <span class="tooltiprtext">“yend =” tells geom_segment() that you are going to declare the y-coordinate for the end point of the line segment.</span> </span><span class="tooltipr">  38, <span class="tooltiprtext">38 is a number on the y-axis of your graph. It is the y-coordinate of the end point of the line segment.</span> </span><br><span class="tooltipr">                size = 1 <span class="tooltiprtext">Use <em>size = 1</em> to adjust the thickness of the line segment.</span> </span><span class="tooltipr"> , color = “lightgray” <span class="tooltiprtext">Use <em>color = “lightgray”</em> to change the color of the line segment to light gray.</span> </span><span class="tooltipr"> , linetype = “longdash” <span class="tooltiprtext">Use *linetype = “longdash* to change the solid line segment to a dashed one. Some linetype options include”dashed“,”dotted“,”longdash“,”dotdash“, etc.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for geom_segment() function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot().</span> </span><br><span class="tooltipr">   geom_point( <span class="tooltiprtext">geom_point() can also be used to add individual points to the graph. Simply declare the x and y coordinates of the point you want to plot.</span> </span><span class="tooltipr"> x = <span class="tooltiprtext">“x =” tells geom_point() that you are going to declare the x-coordinate for the point.</span> </span><span class="tooltipr">  14, <span class="tooltiprtext">14 is a number on the x-axis of your graph. It is the x-coordinate of the point.</span> </span><span class="tooltipr">  y = <span class="tooltiprtext">“y =” tells geom_point() that you are going to declare the y-coordinate for the point.</span> </span><span class="tooltipr">  75 <span class="tooltiprtext">75 is a number on the y-axis of your graph. It is the y-coordinate of the point.</span> </span><span class="tooltipr"> , size = 3 <span class="tooltiprtext">Use <em>size = 3</em> to make the point stand out more.</span> </span><span class="tooltipr"> , color = “firebrick” <span class="tooltiprtext">Use <em>color = “firebrick”</em> to change the color of the point to firebrick red.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of the geom_point() function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot().</span> </span><br><span class="tooltipr">   geom_text( <span class="tooltiprtext">geom_text() allows you to add customized text anywhere on the graph. It is very similar to the base R equivalent, text(…).</span> </span><span class="tooltipr"> x = <span class="tooltiprtext">“x =” tells geom_text() that you are going to declare the x-coordinate for the text.</span> </span><span class="tooltipr">  14, <span class="tooltiprtext">14 is a number on the x-axis of your graph. It is the x-coordinate of the text.</span> </span><span class="tooltipr">  y = <span class="tooltiprtext">“y =” tells geom_text() that you are going to declare the y-coordinate for the text.</span> </span><span class="tooltipr">  84, <span class="tooltiprtext">84 is a number on the y-axis of your graph. It is the y-coordinate of the text.</span> </span><span class="tooltipr">  label = <span class="tooltiprtext">“label =” tells geom_text() that you are going to give it the label.</span> </span><span class="tooltipr">  “My Point (14, 75)”, <span class="tooltiprtext"><em>“My Point (14, 75)”</em> is the text that will appear on the graph.</span> </span><br><span class="tooltipr">             color = “navy” <span class="tooltiprtext">Use <em>color = “navy”</em> to change the color of the text to navy blue.</span> </span><span class="tooltipr"> , size = 3 <span class="tooltiprtext">Use <em>size = 3</em> to change the size of the text.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of the geom_text() function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot().</span> </span><br><span class="tooltipr">   theme_minimal() <span class="tooltiprtext">Add a minimalistic theme to the graph. There are many other themes that you can try out.</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="ggplotpoints" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</p>
</div>
<p><br/></p>
<p><strong>Accessing Parts of the Regression</strong></p>
<p>Finally, note that the <code>mylm</code> object contains the <code>names(mylm)</code> of</p>
<a href="javascript:showhide('coeff')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$coefficients <span class="tooltiprtext">Contains two values. The first is the estimated <span class="math inline">\(y\)</span>-intercept. The second is the estimated slope.</span> </span></p>
</div>
<p></a></p>
<div id="coeff" style="display:none;">
<pre><code>## (Intercept)       speed 
##  -17.579095    3.932409</code></pre>
</div>
<a href="javascript:showhide('resid')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$residuals <span class="tooltiprtext">Contains the residuals from the regression in the same order as the actual dataset.</span> </span></p>
</div>
<p></a></p>
<div id="resid" style="display:none;">
<pre><code>##          1          2          3          4          5          6 
##   3.849460  11.849460  -5.947766  12.052234   2.119825  -7.812584 
##          7          8          9         10         11         12 
##  -3.744993   4.255007  12.255007  -8.677401   2.322599 -15.609810 
##         13         14         15         16         17         18 
##  -9.609810  -5.609810  -1.609810  -7.542219   0.457781   0.457781 
##         19         20         21         22         23         24 
##  12.457781 -11.474628  -1.474628  22.525372  42.525372 -21.407036 
##         25         26         27         28         29         30 
## -15.407036  12.592964 -13.339445  -5.339445 -17.271854  -9.271854 
##         31         32         33         34         35         36 
##   0.728146 -11.204263   2.795737  22.795737  30.795737 -21.136672 
##         37         38         39         40         41         42 
## -11.136672  10.863328 -29.069080 -13.069080  -9.069080  -5.069080 
##         43         44         45         46         47         48 
##   2.930920  -2.933898 -18.866307  -6.798715  15.201285  16.201285 
##         49         50 
##  43.201285   4.268876</code></pre>
</div>
<a href="javascript:showhide('fit')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$fitted.values <span class="tooltiprtext">The values of <span class="math inline">\(\hat{Y}\)</span> in the same order as the original dataset.</span> </span></p>
</div>
<p></a></p>
<div id="fit" style="display:none;">
<pre><code>##         1         2         3         4         5         6         7 
## -1.849460 -1.849460  9.947766  9.947766 13.880175 17.812584 21.744993 
##         8         9        10        11        12        13        14 
## 21.744993 21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 
##        15        16        17        18        19        20        21 
## 29.609810 33.542219 33.542219 33.542219 33.542219 37.474628 37.474628 
##        22        23        24        25        26        27        28 
## 37.474628 37.474628 41.407036 41.407036 41.407036 45.339445 45.339445 
##        29        30        31        32        33        34        35 
## 49.271854 49.271854 49.271854 53.204263 53.204263 53.204263 53.204263 
##        36        37        38        39        40        41        42 
## 57.136672 57.136672 57.136672 61.069080 61.069080 61.069080 61.069080 
##        43        44        45        46        47        48        49 
## 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 76.798715 
##        50 
## 80.731124</code></pre>
</div>
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$… <span class="tooltiprtext">several other things that will not be explained here.</span> </span></p>
</div>
<p><br/></p>
<p><strong>Making Predictions</strong></p>
<a href="javascript:showhide('prediction')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a previously performed lm(…) that was saved into the name <code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">  data.frame( <span class="tooltiprtext">To specify the values of <span class="math inline">\(x\)</span> that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…).</span> </span><span class="tooltipr"> X= <span class="tooltiprtext">The value for <code>X=</code> should be whatever x-variable name was used in the original regression. For example, if <code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original regression, then this code would read <code>speed =</code> instead of <code>X=</code>… Further, the value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>speed=12</code> for example.</span> </span><span class="tooltipr"> Xh <span class="tooltiprtext">The value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>12</code>, as in <code>speed=12</code> for example.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for the data.frame(…) function.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for the predict(…) function.</span> </span></p>
</div>
<p></a></p>
<div id="prediction" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>predict(mylm, data.frame(speed = 12))</code></p>
<table class="rconsole">
<tr>
<td align="right">
<span class="tooltiprout"> 1<br/>   29.60981 <span class="tooltiprouttext">The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet.</span> </span>
</td>
</tr>
</table>
</div>
<a href="javascript:showhide('predictionInterval')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a previously performed lm(…) that was saved into the name <code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">  data.frame( <span class="tooltiprtext">To specify the values of <span class="math inline">\(x\)</span> that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…).</span> </span><span class="tooltipr"> X= <span class="tooltiprtext">The value for <code>X=</code> should be whatever x-variable name was used in the original regression. For example, if <code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original regression, then this code would read <code>speed =</code> instead of <code>X=</code>… Further, the value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>speed=12</code> for example.</span> </span><span class="tooltipr"> Xh <span class="tooltiprtext">The value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>12</code>, as in <code>speed=12</code> for example.</span> </span><span class="tooltipr"> ), <span class="tooltiprtext">Closing parenthesis for the data.frame(…) function.</span> </span><span class="tooltipr">  interval= <span class="tooltiprtext">This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval.</span> </span><span class="tooltipr"> “prediction” <span class="tooltiprtext">This specifies that a prediction interval will be included with the predicted value. A prediction interval gives you a 95% confidence interval that captures 95% of the data, or <span class="math inline">\(Y_i\)</span> values for the specific <span class="math inline">\(X\)</span>-value specified in the prediction.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of the predict(…) function.</span> </span></p>
</div>
<p></a></p>
<div id="predictionInterval" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>predict(mylm, data.frame(speed = 12), interval = &quot;prediction&quot;)</code></p>
<table class="rconsole">
<tr>
<td align="right">
<span class="tooltiprout">   fit <span class="tooltiprouttext">The “fit” is the predicted value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   lwr <span class="tooltiprouttext">The “lwr” is the lower bound.</span>
</td>
<td align="right">
<span class="tooltiprout">   upr <span class="tooltiprouttext">The “upr” is the upper bound.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> 1 29.60981 <span class="tooltiprouttext">In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -1.749529 <span class="tooltiprouttext">This is the lower bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us the stopping distance could be as quick as -1.749529 feet (or 0 because distance can’t go negative).</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> 60.96915 <span class="tooltiprouttext">This is the upper bound of the prediction interval. While we predict a stopping distance of 29.60981 feet, this prediction interval reminds us that the actual stopping distance could be as high as 60.96915 feet.</span> </span>
</td>
</tr>
</table>
</div>
<a href="javascript:showhide('predictionConfidence')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a previously performed lm(…) that was saved into the name <code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">  data.frame( <span class="tooltiprtext">To specify the values of <span class="math inline">\(x\)</span> that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…).</span> </span><span class="tooltipr"> X= <span class="tooltiprtext">The value for <code>X=</code> should be whatever x-variable name was used in the original regression. For example, if <code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original regression, then this code would read <code>speed =</code> instead of <code>X=</code>… Further, the value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>speed=12</code> for example.</span> </span><span class="tooltipr"> Xh <span class="tooltiprtext">The value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>12</code>, as in <code>speed=12</code> for example.</span> </span><span class="tooltipr"> ), <span class="tooltiprtext">Closing parenthesis for the data.frame(…) function.</span> </span><span class="tooltipr">  interval= <span class="tooltiprtext">This optional command allows you to specify if the predicted value should be accompanied by either a confidence interval or a prediction interval.</span> </span><span class="tooltipr"> “confidence” <span class="tooltiprtext">This specifies that a confidence interval for the prediction should be provided. This is of use whenever your interest is in just estimating the average y-value, not the actual y-values.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of the predict(…) function.</span> </span></p>
</div>
<p></a></p>
<div id="predictionConfidence" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>predict(mylm, data.frame(speed = 12), interval = &quot;confidence&quot;)</code></p>
<table class="rconsole">
<tr>
<td align="right">
<span class="tooltiprout">   fit <span class="tooltiprouttext">The “fit” is the predicted value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   lwr <span class="tooltiprouttext">The “lwr” is the lower bound.</span>
</td>
<td align="right">
<span class="tooltiprout">   upr <span class="tooltiprouttext">The “upr” is the upper bound.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> 1 29.60981 <span class="tooltiprouttext">In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> 24.39514 <span class="tooltiprouttext">This is the lower bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is greater than this value.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> 34.82448 <span class="tooltiprouttext">This is the upper bound of the confidence interval. We are 95% confident that the average stopping distance of cars going 12 mph is less than this value.</span> </span>
</td>
</tr>
</table>
</div>
<p><br/></p>
<p><strong>Finding Confidence Intervals for Model Parameters</strong></p>
<a href="javascript:showhide('confint')">
<div class="hoverchunk">
<p><span class="tooltipr"> confint( <span class="tooltiprtext">The R function confint(…) allows you to use an lm(…) object to compute confidence intervals for one or more parameters (like <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span>) in your model.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a previously performed lm(…) that was saved into the name <code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">  level = <span class="tooltiprtext">“level =” tells the confint(…) function that you are going to declare at what level of confidence you want the interval. The default is “level = 0.95.” If you want to find 95% confidence intervals for your parameters, then just run <code>confint(mylm)</code>.</span> </span><span class="tooltipr">  someConfidenceLevel <span class="tooltiprtext">someConfidenceLevel is simply a confidence level you choose when you want something other than a 95% confidence interval. Some examples of appropriate levels include 0.90 and 0.99.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for confint(..) function.</span> </span></p>
</div>
<p></a></p>
<div id="confint" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>confint(mylm, level = 0.90)</code></p>
<table class="rconsole">
<tr>
<td align="left">
 
</td>
<td align="right">
<span class="tooltiprout">   5 % <span class="tooltiprouttext">The lower bound of a 90% confidence interval occurs at the 5th percentile. This is because at 90% confidence, 10% is left in the tails, with 5% on each end. The upper bound will thus end at the 95th percentile, hence the 5% and 95% as the column names.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   95 % <span class="tooltiprouttext">The upper bound of a 90% confidence interval ends at the 95th percentile.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span class="tooltiprouttext">This row of output specifies a confidence interval for <span class="math inline">\(\beta_0\)</span>, the true y-intercept.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -28.914514 <span class="tooltiprouttext">This is the lower bound for the confidence interval of the y-intercept, <span class="math inline">\(\beta_0\)</span>. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -6.243676 <span class="tooltiprouttext">This is the upper bound for the confidence interval for <span class="math inline">\(\beta_0\)</span>, the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> speed <span class="tooltiprouttext">This row of the output provides the upper and lower bound for the confidence interval for <span class="math inline">\(\beta_1\)</span>, the true slope. In this case, you can be 90% confident that the true slope lies between 3.235501 and 4.629317.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3.235501 <span class="tooltiprouttext">This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 4.629317 <span class="tooltiprouttext">This is the upper bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.235501 and 4.629317.</span> </span>
</td>
</td>
</tr>
</table>
<p><br/> <br/></p>
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>confint(mylm, level = 0.95)</code></p>
<table class="rconsole">
<tr>
<td align="left">
 
</td>
<td align="right">
<span class="tooltiprout">   2.5 % <span class="tooltiprouttext">The lower bound of a 95% confidence interval occurs at the 2.5th percentile. This is because at 95% confidence, 5% is left in the tails, with 2.5% on each end. The upper bound will thus end at the 97.5th percentile, hence the 2.5% and 97.5% as the column names for the lower and upper bounds, respectively.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   97.5 % <span class="tooltiprouttext">The upper bound of a 95% confidence interval ends at the 97.5th percentile.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span class="tooltiprouttext">This row of output specifies a confidence interval for <span class="math inline">\(\beta_0\)</span>, the true y-intercept.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -31.167850 <span class="tooltiprouttext">This is the lower bound for the confidence interval of the y-intercept, <span class="math inline">\(\beta_0\)</span>. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -3.990340 <span class="tooltiprouttext">This is the upper bound for the confidence interval for <span class="math inline">\(\beta_0\)</span>, the y-intercpet. In this example, the confidence interval for the y-intercept does not make sense because you cannot have negative distance.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> speed <span class="tooltiprouttext">This row of the output provides the upper and lower bound for the confidence interval for <span class="math inline">\(\beta_1\)</span>, the true slope. In this case, you can be 90% confident that the true slope lies between 3.096964 and 4.767853.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3.096964 <span class="tooltiprouttext">This is the lower bound of the confidence interval. In this case, you can be 90% confident that the slope lies between 3.096964 and 4.767853</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 4.767853 <span class="tooltiprouttext">This is the upper bound of the confidence interval. In this case, you can be 95% confident that the slope lies between 3.096964 and 4.767853</span> </span>
</td>
</td>
</tr>
</table>
</div>
<hr />
</div>
</div>
<div id="explanation" class="section level3">
<h3>Explanation</h3>
<div style="padding-left:125px;">

<p>Linear regression has a rich mathematical theory behind it. This is because it uses a mathematical function and a random error term to describe the regression relation between a response variable <span class="math inline">\(Y\)</span> and an explanatory variable called <span class="math inline">\(X\)</span>.</p>
<div style="padding-left:30px;color:darkgray;">
<p>Expand each element below to learn more.</p>
</div>
<p><span style="color:steelblue;font-size:.8em;padding-left:160px;">Regression Cheat Sheet</span> <a href="javascript:showhide('regressioncheatsheet')" style="font-size:.6em;color:skyblue;">(Expand)</a></p>
<div id="regressioncheatsheet" style="display:none;font-size:.7em;">
<table style="width:71%;">
<colgroup>
<col width="9%" />
<col width="23%" />
<col width="11%" />
<col width="12%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Pronunciation</th>
<th>Meaning</th>
<th>Math</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(Y_i\)</span><span class="tooltiprtext"> <code>$Y_i$</code></span> </span><span class="tooltipr"></td>
<td>“why-eye”</td>
<td>The data</td>
<td><span class="tooltipr"> <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)\)</span><span class="tooltiprtext"> <code>$Y_i = \beta_0 + \beta_1 X_i +</code> <code>\epsilon_i \quad \text{where} \</code> <code>\epsilon_i \sim N(0, \sigma^2)$</code></span> </span><span class="tooltipr"></td>
<td><code>YourDataSet$YourYvariable</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(\hat{Y}_i\)</span><span class="tooltiprtext"> <code>$\hat{Y}_i$</code></span> </span><span class="tooltipr"></td>
<td>“why-hat-eye”</td>
<td>The fitted line</td>
<td><span class="tooltipr"> <span class="math inline">\(\hat{Y}_i = b_0 + b_1 X_i\)</span><span class="tooltiprtext"> <code>$\hat{Y}_i = b_0 + b_1 X_i$</code></span></td>
<td><code>lmObject$fitted.values</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(E\{Y_i\}\)</span><span class="tooltiprtext"> <code>$E\{Y_i\}$</code></span> </span><span class="tooltipr"></td>
<td>“expected value of why-eye”</td>
<td>True mean y-value</td>
<td><span class="tooltipr"><span class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span><span class="tooltiprtext"> <code>$E\{Y_i\} = \beta_0 + \beta_1 X_i$</code></span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(\beta_0\)</span><span class="tooltiprtext"> <code>$\beta_0$</code></span> </span><span class="tooltipr"></td>
<td>“beta-zero”</td>
<td>True y-intercept</td>
<td><code>&lt;none&gt;</code></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(\beta_1\)</span><span class="tooltiprtext"> <code>$\beta_1$</code></span> </span><span class="tooltipr"></td>
<td>“beta-one”</td>
<td>True slope</td>
<td><code>&lt;none&gt;</code></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(b_0\)</span><span class="tooltiprtext"> <code>$b_0$</code></span> </span><span class="tooltipr"></td>
<td>“b-zero”</td>
<td>Estimated y-intercept</td>
<td><span class="tooltipr"><span class="math inline">\(b_0 = \bar{Y} - b_1\bar{X}\)</span><span class="tooltiprtext"> <code>$b_0 = \bar{Y} - b_1\bar{X}</code></span></td>
<td><code>b_0 &lt;- mean(Y) - b_1*mean(X)$</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(b_1\)</span><span class="tooltiprtext"> <code>$b_1$</code></span> </span><span class="tooltipr"></td>
<td>“b-one”</td>
<td>Estimated slope</td>
<td><span class="tooltipr"><span class="math inline">\(b_1 = \frac{\sum X_i(Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2}\)</span><span class="tooltiprtext"> <code>$b_1 = \frac{\sum X_i(Y_i - \bar{Y})}</code> <code>{\sum(X_i - \bar{X})^2}$</code></span></td>
<td><code>b_1 &lt;- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(\epsilon_i\)</span><span class="tooltiprtext"> <code>$\epsilon_i$</code></span> </span><span class="tooltipr"></td>
<td>“epsilon-eye”</td>
<td>Distance of dot to true line</td>
<td><span class="tooltipr"><span class="math inline">\(\epsilon_i = Y_i - E\{Y_i\}\)</span><span class="tooltiprtext"> <code>$\epsilon_i = Y_i - E\{Y_i\}$</code></span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(r_i\)</span><span class="tooltiprtext"> <code>$r_i$</code></span> </span><span class="tooltipr"></td>
<td>“r-eye” or “residual-eye”</td>
<td>Distance of dot to estimated line</td>
<td><span class="tooltipr"><span class="math inline">\(r_i = Y_i - \hat{Y}_i\)</span><span class="tooltiprtext"> <code>$r_i = Y_i - \hat{Y}_i$</code></span></td>
<td><code>lmObject$residuals</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(\sigma^2\)</span><span class="tooltiprtext"> <code>$\sigma^2$</code></span> </span><span class="tooltipr"></td>
<td>“sigma-squared”</td>
<td>Variance of the <span class="math inline">\(\epsilon_i\)</span></td>
<td><span class="tooltipr"><span class="math inline">\(Var\{\epsilon_i\} = \sigma^2\)</span><span class="tooltiprtext"><code>$Var\{\epsilon_i\} = \sigma^2$</code></span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(MSE\)</span><span class="tooltiprtext"> <code>$MSE$</code></span> </span><span class="tooltipr"></td>
<td>“mean squared error”</td>
<td>Estimate of <span class="math inline">\(\sigma^2\)</span></td>
<td><span class="tooltipr"><span class="math inline">\(MSE = \frac{SSE}{n-p}\)</span><span class="tooltiprtext"><code>$MSE = \frac{SSE}{n-p}$</code></span></td>
<td><code>sum( lmObject$res^2 ) / (n - p)</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(SSE\)</span><span class="tooltiprtext"> <code>$SSE$</code></span> </span><span class="tooltipr"></td>
<td>“sum of squared error” (residuals)</td>
<td>Measure of dot’s total deviation from the line</td>
<td><span class="tooltipr"><span class="math inline">\(SSE = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2\)</span><span class="tooltiprtext"><code>$SSE = \sum_{i=1}^n</code> <code>(Y_i - \hat{Y}_i)^2$</code></span></td>
<td><code>sum( lmObject$res^2 )</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(SSR\)</span><span class="tooltiprtext"> <code>$SSR$</code></span> </span><span class="tooltipr"></td>
<td>“sum of squared regression error”</td>
<td>Measure of line’s deviation from y-bar</td>
<td><span class="tooltipr"> <span class="math inline">\(SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2\)</span><span class="tooltiprtext"><code>$SSR = \sum_{i=1}^n</code> <code>(\hat{Y}_i - \bar{Y})^2$</code></span></td>
<td><code>sum( (lmObject$fit - mean(YourData$Y))^2 )</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(SSTO\)</span><span class="tooltiprtext"> <code>$SSTO$</code></span> </span><span class="tooltipr"></td>
<td>“total sum of squares”</td>
<td>Measure of total variation in Y</td>
<td><span class="tooltipr"><span class="math inline">\(SSR + SSE = SSTO = \sum_{i=1}^n (Y_i - \bar{Y})^2\)</span><span class="tooltiprtext"><code>$SSR + SSE = SSTO = \sum_{i=1}^n</code> <code>(Y_i - \bar{Y})^2$</code></span></td>
<td><code>sum( (YourData$Y - mean(YourData$Y)^2 )</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(R^2\)</span><span class="tooltiprtext"> <code>$R^2$</code></span> </span><span class="tooltipr"></td>
<td>“R-squared”</td>
<td>Proportion of variation in Y explained by the regression</td>
<td><span class="tooltipr"><span class="math inline">\(R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}\)</span><span class="tooltiprtext"><code>$R^2 = \frac{SSR}{SSTO} = 1</code> <code>- \frac{SSE}{SSTO}$</code></span></td>
<td><code>SSR/SSTO</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(\hat{Y}_h\)</span><span class="tooltiprtext"> <code>$\hat{Y}_h$</code></span> </span><span class="tooltipr"></td>
<td>“why-hat-aitch”</td>
<td>Estimated mean y-value for some x-value called <span class="math inline">\(X_h\)</span></td>
<td><span class="tooltipr"><span class="math inline">\(\hat{Y}_h = b_0 + b_1 X_h\)</span><span class="tooltiprtext"><code>$\hat{Y}_h = b_0 + b_1 X_h$</code></span></td>
<td><code>predict(lmObject, data.frame(XvarName=#))</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(X_h\)</span><span class="tooltiprtext"> <code>$X_h$</code></span> </span><span class="tooltipr"></td>
<td>“ex-aitch”</td>
<td>Some x-value, not necessarily one of the <span class="math inline">\(X_i\)</span> values used in the regression</td>
<td><span class="tooltipr"><span class="math inline">\(X_h =\)</span> some number<span class="tooltiprtext"><code>$X_h = $</code></span></td>
<td><code>Xh = #</code></td>
</tr>
<tr class="even">
<td>Confidence Interval</td>
<td>“confidence interval”</td>
<td>Estimated bounds at a certain level of confidence for a parameter</td>
<td><span class="math inline">\(b_0 \pm t^* \cdot s_{b_0}\)</span> or <span class="math inline">\(b_1 \pm t^* \cdot s_{b_1}\)</span></td>
<td><code>confint(mylm, level = someConfidenceLevel)</code></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td><span class="math inline">\(b_0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td><span class="math inline">\(b_1\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\epsilon_i\)</span></td>
<td><span class="math inline">\(r_i\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(MSE\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\sigma\)</span></td>
<td><span class="math inline">\(\sqrt{MSE}\)</span>, the Residual standard error</td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
<div id="the-mathematical-model-expand" class="section level4">
<h4>The Mathematical Model <a href="javascript:showhide('mathmodel1')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(Y_i\)</span>, <span class="math inline">\(\hat{Y}_i\)</span>, and <span class="math inline">\(E\{Y_i\}\)</span>…</span></p>
<div id="mathmodel1" style="display:none;">
<p>There are three main elements to the mathematical model of regression. Each of these three elements is pictured below in the “Regression Relation Diagram.”</p>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>Study both the three bullet points and their visual representations in the plot below for a clearer understanding.</p>
</div>
<ol style="list-style-type: decimal">
<li>The <strong>true line</strong>, i.e., the regression relation:</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(\underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\text{equation of a line}\)</span></p>
</div>
<p><a href="javascript:showhide('readmoretrueline')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="readmoretrueline" style="display:none;">
<p>The true line is shown by the dotted line in the graph pictured below. This is typically unobservable. Think of it as “natural law” or “God’s law”. It is some true line that is unknown to us.</p>
<p>The regression relation <span class="math inline">\(E\{Y\} = \beta_0 + \beta_1 X\)</span> creates the line of regression where <span class="math inline">\(\beta_0\)</span> is the <span class="math inline">\(y\)</span>-intercept of the line and <span class="math inline">\(\beta_1\)</span> is the slope of the line. The regression relationship provides the average <span class="math inline">\(Y\)</span>-value, denoted <span class="math inline">\(E\{Y_i\}\)</span>, for a given <span class="math inline">\(X\)</span>-value, denoted by <span class="math inline">\(X_i\)</span>.</p>
<p>Note: <span class="math inline">\(E\{Y\}\)</span> is pronounced “the expected value of y” because, well… the mean is the typical, average, or “expected” value.</p>
</div>
</div>
<ol start="2" style="list-style-type: decimal">
<li>The <strong>dots</strong>, i.e., the regression relation plus an error term:</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}} + \underbrace{\epsilon_i}_\text{error term} \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)\)</span></p>
</div>
<p><a href="javascript:showhide('readmoredots')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="readmoredots" style="display:none;">
<p>This is shown by the dots in the graph below. This is the data. In regression, the assumption is that the y-value for individual <span class="math inline">\(i\)</span>, denoted by <span class="math inline">\(Y_i\)</span>, was “created” by adding an error term <span class="math inline">\(\epsilon_i\)</span> to each individual’s “expected” value <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>. Note the “order of creation” would require first knowing an indivual’s x-value, <span class="math inline">\(X_i\)</span>, then their expected value from the regression relation <span class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span> and then adding their <span class="math inline">\(\epsilon_i\)</span> value to the result. The <span class="math inline">\(\epsilon_i\)</span> allows each individual to deviate from the line. Some individuals deviate dramatically, some deviate only a little, but all dots vary some distance <span class="math inline">\(\epsilon_i\)</span> from the line.</p>
<p>Note: <span class="math inline">\(Y_i\)</span> is pronounced “why-eye” because it is the y-value for individual <span class="math inline">\(i\)</span>. Sometimes also called “why-sub-eye” because <span class="math inline">\(i\)</span> is in the subscript of <span class="math inline">\(Y\)</span>.</p>
</div>
</div>
<ol start="3" style="list-style-type: decimal">
<li>The <strong>estimated line</strong>, i.e., the line we get from a sample of data.</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(\underbrace{\hat{Y}_i}_{\substack{\text{estimated mean} \\ \text{y-value}}} = \underbrace{b_0 + b_1 X_i}_\text{estimated regression equation}\)</span></p>
</div>
<p><a href="javascript:showhide('readmoreestimatedline')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="readmoreestimatedline" style="display:none;">
<p>The estimated line is shown by the solid line in the graph below. <span class="math inline">\(\hat{Y}\)</span> is the estimated regression equation obtained from the sample of data. It is the estimator of the true regression equation <span class="math inline">\(E\{Y\}\)</span>. So <span class="math inline">\(\hat{Y}\)</span> is interpreted as the estimated average (or mean) <span class="math inline">\(Y\)</span>-value for any given <span class="math inline">\(X\)</span>-value. Thus, <span class="math inline">\(b_0\)</span> is the estimated y-intercept and <span class="math inline">\(b_1\)</span> is the estimated slope. The b’s are sample statistics, like <span class="math inline">\(\bar{x}\)</span> and the <span class="math inline">\(\beta\)</span>’s are population parameters like <span class="math inline">\(\mu\)</span>. The <span class="math inline">\(b\)</span>’s estimate the <span class="math inline">\(\beta\)</span>’s.</p>
<p>Note: <span class="math inline">\(\hat{Y}_i\)</span> is pronounced “why-hat-eye” and is known as the “estimated y-value” or “fitted y-value” because it is the y-value you get from <span class="math inline">\(b_0 + b_1 X_i\)</span>. It is always different from <span class="math inline">\(Y_i\)</span> because dots are rarely if ever exactly on the estimated regression line.</p>
</div>
</div>
<p>This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (the dots) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line).</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Something to ponder: The true line, when coupled with the error terms, “creates” the data. The estimated (or fitted) line uses the sampled data to try to “re-create” the true line.</p>
<p>We could loosely call this the “order of creation” as shown by the following diagram.</p>
<pre class="r"><code>par(mfrow=c(1,3), mai=c(.2,.2,.4,.1))
plot(y ~ x, col=&quot;white&quot;,  main=&quot;A Law is Given&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;)
curve(beta0 + beta1*x, add=TRUE, lty=2)
plot(y ~ x, pch=16, main=&quot;Data is Created&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;)
curve(beta0 + beta1*x, add=TRUE, lty=2)
plot(y ~ x, pch=16, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;The Law is Estimated&quot;)
curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;)
curve(beta0 + beta1*x, add=TRUE, lty=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<table style="width:97%;">
<colgroup>
<col width="29%" />
<col width="30%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th>A Law is Given</th>
<th>Data is Created</th>
<th>The Law is Estimated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span></td>
<td><span class="math inline">\(Y_i = E\{Y_i\} + \epsilon_i\)</span></td>
<td><span class="math inline">\(\hat{Y}_i = b_0 + b_1 X_i\)</span></td>
</tr>
<tr class="even">
<td>The true line is the “law”.</td>
<td>The <span class="math inline">\(Y_i\)</span> are created by adding <span class="math inline">\(\epsilon_i\)</span> to <span class="math inline">\(E\{Y_i\}\)</span> where <span class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span>.</td>
<td>The law is estimated with <span class="math inline">\(\hat{Y}_i\)</span> which is given with <code>lm(...)</code>.</td>
</tr>
</tbody>
</table>
<p>Click open the “Code” buttom below to the right to find code that runs a simulation demonstrating this “order of creation”.</p>
<pre class="r"><code>## Simulating Data from a Regression Model
## This R-chunk is meant to be played in your R Console.
## It allows you to explore how the various elements
## of the regression model combine together to &quot;create&quot;
## data and then use the data to &quot;re-create&quot; the line.

set.seed(101) #Allows us to always get the same &quot;random&quot; sample
              #Change to a new number to get a new sample

  n &lt;- 30 #set the sample size

  X_i &lt;- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45.

  beta0 &lt;- 3 #Our choice for the y-intercept. 

  beta1 &lt;- 1.8 #Our choice for the slope. 

  sigma &lt;- 2.5 #Our choice for the std. deviation of the error terms.

  epsilon_i &lt;- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i &lt;- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model

  fabData &lt;- data.frame(y=Y_i, x=X_i) #Store the data as data

  View(fabData) 
  
  #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.

  fab.lm &lt;- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.

  abline(fab.lm) #Add the estimated regression line to your plot.

# Now for something you can&#39;t do in real life... but since we created the data...

  abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). 

  legend(&quot;topleft&quot;, legend=c(&quot;True Line&quot;, &quot;Estimated Line&quot;), lty=c(2,1), bty=&quot;n&quot;) #Add a legend to your plot specifying which line is which.</code></pre>
</div>
<p><br /></p>
</div>
<div id="interpreting-the-model-parameters-expand" class="section level4">
<h4>Interpreting the Model Parameters <a href="javascript:showhide('interpretingparameters')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(\beta_0\)</span> (intercept) and <span class="math inline">\(\beta_1\)</span> (slope), estimated by <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, interpreted as…</span></p>
<div id="interpretingparameters" style="display:none;">
<p>The interpretation of <span class="math inline">\(\beta_0\)</span> is only meaningful if <span class="math inline">\(X=0\)</span> is in the scope of the model. If <span class="math inline">\(X=0\)</span> is in the scope of the model, then the intercept is interpreted as the average y-value, denoted <span class="math inline">\(E\{Y\}\)</span>, when <span class="math inline">\(X=0\)</span>.</p>
<p>The interpretation of <span class="math inline">\(\beta_1\)</span> is the amount of increase (or decrease) in the average y-value, denoted <span class="math inline">\(E\{Y\}\)</span>, per unit change in <span class="math inline">\(X\)</span>. It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”.</p>
<p>To better see this, consider the three graphics shown below.</p>
<pre class="r"><code>par(mfrow=c(1,3))
hist(mtcars$mpg, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Number of Vehicles&quot;, xlab=&quot;Gas Mileage (mpg)&quot;, col=&quot;skyblue&quot;)
boxplot(mpg ~ cyl, data=mtcars, border=&quot;skyblue&quot;, boxwex=0.5, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Number of Cylinders of Engine (cyl)&quot;)
plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=&quot;skyblue&quot;, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Quarter Mile Time (qsec)&quot;)
abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=&quot;darkgray&quot;)
mtext(side=3, text=&quot;Automatic Transmissions Only (am==0)&quot;, cex=0.5)
abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=&quot;gray&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<table style="width:88%;">
<colgroup>
<col width="27%" />
<col width="27%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th>The Histogram</th>
<th>The Boxplot</th>
<th>The Scatterplot</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The <strong>histogram</strong> on the left shows gas mileages of vehicles from the mtcars data set. The average gas mileage is 20.09.</td>
<td>The <strong>boxplot</strong> in the middle shows that if we look at gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we could talk about the change in the means across cylinders, and would see that the mean is decreasing, first by <span class="math inline">\(26.66 - 19.74 = 6.92\)</span> mpg, then by <span class="math inline">\(19.74 - 15.1 = 4.64\)</span> mpg.</td>
<td>The <strong>scatterplot</strong> on the right shows that the average gas mileage (for just automatic transmission vehicles) increases by a slope of 1.44 for each 1 second increase in quarter mile time. In other words, the line gives the average y-value for any x-value. Thus, the slope of the line is the change in the average y-value.</td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
</div>
<div id="residuals-and-errors-expand" class="section level4">
<h4>Residuals and Errors <a href="javascript:showhide('residualsanderrors')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(r_i\)</span>, the residual, estimates <span class="math inline">\(\epsilon_i\)</span>, the true error…</span></p>
<div id="residualsanderrors" style="display:none;">
<p>Residuals are the difference between the observed value of <span class="math inline">\(Y_i\)</span> (the point) and the predicted, or estimated value, for that point called <span class="math inline">\(\hat{Y_i}\)</span>. The errors are the true distances between the observed <span class="math inline">\(Y_i\)</span> and the actual regression relation for that point, <span class="math inline">\(E\{Y_i\}\)</span>.</p>
<p>We will denote a <strong>residual</strong> for individual <span class="math inline">\(i\)</span> by <span class="math inline">\(r_i\)</span>, <span class="math display">\[
  r_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}} - \underbrace{\hat{Y}_i}_{\substack{\text{Predicted} \\ \text{Y-value}}} \quad \text{(residual)}
\]</span> The residual <span class="math inline">\(r_i\)</span> estimates the true <strong>error</strong> for individual <span class="math inline">\(i\)</span>, <span class="math inline">\(\epsilon_i\)</span>, <span class="math display">\[
  \epsilon_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}} - \underbrace{E\{Y_i\}}_{\substack{\text{True Mean} \\ \text{Y-value}}} \quad \text{(error)}
\]</span></p>
<p>In summary…</p>
<div style="padding-left:30px;">
<table style="width:53%;">
<colgroup>
<col width="23%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th>Residual <span class="math inline">\(r_i\)</span></th>
<th>Error <span class="math inline">\(\epsilon_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Distance between the dot <span class="math inline">\(Y_i\)</span> and the estimated line <span class="math inline">\(\hat{Y}_i\)</span></td>
<td>Distance between the dot <span class="math inline">\(Y_i\)</span> and the true line <span class="math inline">\(E\{Y_i\}\)</span>.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(r_i = Y_i - \hat{Y}_i\)</span></td>
<td><span class="math inline">\(\epsilon_i = Y_i - E\{Y_i\}\)</span></td>
</tr>
<tr class="odd">
<td>Known</td>
<td>Typically Unknown</td>
</tr>
</tbody>
</table>
</div>
<p>As shown in the graph below, the residuals are known values and they estimate the unknown (but true) error terms.</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Keep in mind the idea that the errors <span class="math inline">\(\epsilon_i\)</span> “created” the data and that the residuals <span class="math inline">\(r_i\)</span> are computed after using the data to “re-create” the line.</p>
<p>Residuals have many uses in regression analysis. They allow us to</p>
<ol style="list-style-type: decimal">
<li>diagnose the regression assumptions,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Assumptions” section below for more details.</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>estimate the regression relation,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Estimating the Model Parameters” section below for more details.</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>estimate the variance of the error terms,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Estimating the Model Variance” section below for more details.</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>and assess the fit of the regression relation.</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Assessing the Fit of a Regression” section below for more details.</p>
</div>
</div>
<p><br /></p>
</div>
<div id="assessing-the-fit-of-a-regression-expand" class="section level4">
<h4>Assessing the Fit of a Regression <a href="javascript:showhide('assessingthefit')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(R^2\)</span>, SSTO, SSR, and SSE…</span></p>
<div id="assessingthefit" style="display:none;">

<p>Not all regressions are created equally as the three plots below show. Sometimes the dots are a clustered very tightly to the line. At other times, the dots spread out fairly dramatically from the line.</p>
<pre class="r"><code>par(mfrow=c(1,3), mai=c(.1,.1,.5,.1))
set.seed(2)
x &lt;- runif(30,0,20)
y1 &lt;- 2 + 3.5*x + rnorm(30,0,2)
y2 &lt;- 2 + 3.5*x + rnorm(30,0,8)
y3 &lt;- 2 + 3.5*x + rnorm(30,0,27)
plot(y1 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Excellent Fit&quot;)
abline(lm(y1 ~ x), col=&quot;gray&quot;)
plot(y2 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Good Fit&quot;)
abline(lm(y2 ~ x), col=&quot;gray&quot;)
plot(y3 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Poor Fit&quot;)
abline(lm(y3 ~ x), col=&quot;gray&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>A common way to measure the fit of a regression is with <a href="NumericalSummaries.html#correlation">correlation</a>. While this can be a useful measurement, there is greater insight in using the square of the correlation, called <span class="math inline">\(R^2\)</span>. Before you can understand <span class="math inline">\(R^2\)</span>, you must understand three important “sums of squares”.</p>
<div style="padding-left:30px;">
<p><a href="javascript:showhide('sumsofsquaresread')" style="font-size:.8em;color:skyblue;">(Read more about sums…)</a></p>
<div id="sumsofsquaresread" style="display:none;">
<p>A sum is just a fancy word for adding things together. <span class="math display">\[
  1 + 2 + 3 + 4 + 5 + 6 = 21
\]</span></p>
<p>Long sums get tedious to write out by hand. So we use the symbol <span class="math inline">\(\Sigma\)</span> to denote the word “sum”. Further, we use a subscript <span class="math inline">\(\Sigma_{i=1}\)</span> to state what value the sum is beginning with, and a superscript <span class="math inline">\(\Sigma_{i=1}^6\)</span> to state the value we are ending at. This gives <span class="math display">\[
  \sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21
\]</span></p>
<p>Test your knowledge, do you see why the answer is 6 to the sum below? <span class="math display">\[
  \sum_{i=1}^3 i = 6
\]</span></p>
<p>Computing sums in R is fairly easy. Type the following codes in your R Console.</p>
<p><code>sum(1:6) #gives the answer of 21</code></p>
<p><code>sum(1:3) #gives the answer of 6</code></p>
<p>However, sums really become useful when used with a data set.</p>
<p>Each row of a data set represents an “individual’s” data. We can reference each individual with a row number. In the data below, individual 3, denoted by <span class="math inline">\(i=3\)</span>, has a <code>speed</code> of 7 and a <code>dist</code> of 4.</p>
<pre class="r"><code>pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3)</code></pre>
<table style="width:40%;">
<colgroup>
<col width="18%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Individual</th>
<th align="center">speed</th>
<th align="center">dist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">4</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">10</td>
</tr>
<tr class="odd">
<td align="center"><strong>3</strong></td>
<td align="center"><strong>7</strong></td>
<td align="center"><strong>4</strong></td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">7</td>
<td align="center">22</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">8</td>
<td align="center">16</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">9</td>
<td align="center">10</td>
</tr>
</tbody>
</table>
<p>To compute the sum of the <strong>speed</strong> column, use <code>sum(speed)</code>. If we divided this sum by 6, we would get the mean of speed <code>mean(speed)</code>. In fact, the two most used statistics <code>mean(...)</code> and <code>sd(...)</code> both use sums. Take a moment to review the formulas for <a href="NumericalSummaries.html#mean">mean</a> and <a href="NumericalSummaries.html#standard-deviation">standard deviation</a>. It is strongly recommended that you study the Explanation tab for both as well. We’ll wait. See you back here shortly.</p>
<p>…</p>
<p>Welcome back.</p>
<p>Suppose we let <code>X = speed</code> and <code>Y = dist</code>. Then <span class="math inline">\(X_3 = 7\)</span> and <span class="math inline">\(Y_3 = 4\)</span> because we are accessing row 3 of both the <span class="math inline">\(X\)</span> (or speed) column and <span class="math inline">\(Y\)</span> (or dist) column. (Remember from the above discussion that for individual #3, the speed was 7 and the dist was 4.) Further, <code>sum(speed)</code> would be written mathematically as <span class="math inline">\(\sum_{i=1}^6 X_i\)</span> and <code>sum(dist)</code> would be written as <span class="math inline">\(\sum_{i=1}^6 Y_i\)</span>.</p>
</div>
</div>
<table>
<colgroup>
<col width="32%" />
<col width="36%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Sum of Squared Errors</strong></th>
<th><strong>Sum of Squares Regression</strong></th>
<th><strong>Total Sum of Squares</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\text{SSE} = \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2\)</span></td>
<td><span class="math inline">\(\text{SSR} = \sum_{i=1}^n \left(\hat{Y}_i - \bar{Y}\right)^2\)</span></td>
<td><span class="math inline">\(\text{SSTO} = \sum_{i=1}^n \left(Y_i - \bar{Y}\right)^2\)</span></td>
</tr>
<tr class="even">
<td>Measures how much the residuals deviate from the line.</td>
<td>Measures how much the regression line deviates from the average y-value.</td>
<td>Measures how much the y-values deviate from the average y-value.</td>
</tr>
<tr class="odd">
<td>Equals SSTO - SSR</td>
<td>Equals SSTO - SSE</td>
<td>Equals SSE + SSR</td>
</tr>
<tr class="even">
<td><code>sum( (Y - mylm$fit)^2 )</code></td>
<td><code>sum( (mylm$fit - mean(Y))^2 )</code></td>
<td><code>sum( (Y - mean(Y))^2 )</code></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<hr style="border-color:#d5d5d5; border-style:solid;"/>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>It is important to remember that SSE and SSR split up SSTO, so that <span class="math display">\[
  \text{SSTO} = \text{SSE} + \text{SSR}
\]</span> This implies that if SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works.</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>The above graphs reveal that the idea of correlation is tightly linked with sums of squares. In fact, the correlation squared is equal to SSR/SSTO. And this fraction, SSR/SSTO is called <span class="math inline">\(R^2\)</span> (“r-squared”).</p>
<p><strong>R-Squared (<span class="math inline">\(R^2\)</span>)</strong> <span class="math display">\[
  \underbrace{R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}}_\text{Interpretation: Proportion of variation in Y explained by the regression.}
\]</span></p>
<p>The smallest <span class="math inline">\(R^2\)</span> can be is zero, and the largest it can be is 1. This is because <span class="math inline">\(SSR\)</span> must be between 0 and SSTO, inclusive.</p>
</div>
<p><br /></p>
</div>
<div id="residual-plots-regression-assumptions-expand" class="section level4">
<h4>Residual Plots &amp; Regression Assumptions <a href="javascript:showhide('assumptions1')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">Residuals vs. fitted-values, Q-Q Plot of the residuals, and residuals vs. order plots…</span></p>
<div id="assumptions1" style="display:none;">
<p>There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate.</p>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>Each assumption is labeled in the regression equation below.</p>
</div>
<ol style="list-style-type: decimal">
<li>The regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered fixed and measured without error.</li>
<li>The error terms are independent.</li>
</ol>
<p><span style="color:darkgray;">Regression Equation</span> <span class="math display">\[
  Y_i = \underbrace{\beta_0 + \beta_1 \overbrace{X_i}^\text{#4}}_{\text{#1}} + \epsilon_i \quad \text{where} \ \overbrace{\epsilon_i \sim}^\text{#5} \overbrace{N(0}^\text{#2}, \overbrace{\sigma^2}^\text{#3})
\]</span></p>
<p>Residuals are used to diagnose departures from the regression assumptions.</p>
<p><a href="javascript:showhide('moreassumptionsdetail')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="moreassumptionsdetail" style="display:none;">
<p>As shown above, the regression equation makes several claims, or assumptions, about the error terms <span class="math inline">\(\epsilon_i\)</span>, specifically 2, 3, and 5 of the regression assumptions are hidden inside the statement <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> as shown here <span class="math display">\[
  \epsilon_i \underbrace{\sim}_{\substack{\text{Independent} \\ \text{Errors}}} \overbrace{N}^{\substack{\text{Normally} \\ \text{distributed}}}(\underbrace{0}_{\substack{\text{mean of} \\ \text{zero}}}, \underbrace{\sigma^2}_{\substack{\text{Constant} \\ \text{Variance}}})
\]</span></p>
<p>While the actual error terms (<span class="math inline">\(\epsilon_i\)</span>) are unknown in real life, the residuals (<span class="math inline">\(r_i\)</span>) are known. Thus, we can use the residuals to check if the assumptions of the regression appear to be satisfied or not.</p>
</div>
<p><br /></p>
<div style="padding-left:15px;">
<h5 id="residuals-versus-fitted-values-plot-checks-assumptions-1-and-3">Residuals versus Fitted-values Plot: Checks Assumptions #1 and #3</h5>
<table width="90%">
<tr>
<td with="15%">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-21-1.png" width="144" /></p>
</td>
<td width="75%">
<p>The linear relationship and constant variance assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the <span class="math inline">\(\hat{Y}_i\)</span>. The residuals are the <span class="math inline">\(r_i\)</span>. This plot compares the residual to the magnitude of the fitted-value. No discernable pattern in this plot is desirable.</p>
<p>| <a href="javascript:showhide('residualsvsfittedvalues')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="residualsvsfittedvalues" style="display:none;">
<p><a href="javascript:showhide('residualsvsfittedvaluesread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="residualsvsfittedvaluesread" style="display:none;">
<p>The residuals versus fitted values plot checks for departures from the linear relation assumption and the constant variance assumption.</p>
<ul>
<li><p>The linear relation is assumed to be satisfied if there are no apparent trends in the plot.</p></li>
<li><p>The constant variance assumption is assumed to be satisfied if the vertical spread of the residuals remains roughly consistent across all fitted values.</p></li>
</ul>
<p>The left column of plots below show scenarios that would be considered not linear. The right column of plots show scenarios that would be considered linear, but lacking constant variance. The middle column of plots shows scenarios that would satisfy both assumptions, linear and constant variance.</p>
</div>
<pre class="r"><code>set.seed(2)
X &lt;- rnorm(30,15,3)
notLin &lt;- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8))
notLin.lm &lt;- lm(Y~X, data=notLin)
set.seed(15)
Lin &lt;- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3))
Lin.lm &lt;- lm(Y~X, data=Lin)
par(mfrow=c(3,3),  mai=c(.25,.25,.25,.25), mgp=c(1,.75,0))
  plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Not Linear&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(notLin.lm$fitted.values,notLin.lm$residuals)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
  plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, 
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Good: Linear, Constant Variance&quot;, 
       cex.main=0.95, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0)

  set.seed(6)
notCon &lt;- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5))
notCon.lm &lt;- lm(Y~X, data=notCon)
LinO &lt;- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3))
LinO[1] &lt;- LinO[1]^2
LinO.lm &lt;- lm(Y~X, data=LinO)
  plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, main=&quot;Unconstant Variance&quot;, cex.main=0.95, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
#  plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, main=&quot;Outliers&quot;, cex.main=0.95)
#  abline(h=0)

  
  tmp &lt;- lm(height ~ age, data=Loblolly)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(tmp$fitted.values,tmp$residuals)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
  
  tmp &lt;- lm(Girth ~ Volume, data=trees[-31,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0)

  tmp &lt;- lm(Height ~ Volume, data=trees)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) 
  abline(h=0)
  
  
  
  
  tmp &lt;- lm(mpg ~ disp, data=mtcars)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(tmp$fitted.values,tmp$residuals, f=.4)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0) 
  
  
  tmp &lt;- lm(weight ~ repwt, data=Davis[-12,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0) 

  tmp &lt;- lm(weight ~ repht, data=Davis[-12,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) 
  abline(h=0) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<p><br /></p>
<h5 id="q-q-plot-of-the-residuals-checks-assumption-2">Q-Q Plot of the Residuals: Checks Assumption #2</h5>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-23-1.png" width="144" />
</td>
<td width="75%">
<p>The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption.</p>
<p>| <a href="javascript:showhide('qqplots')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="qqplots" style="display:none;">
<p><a href="javascript:showhide('qqplotsread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="qqplotsread" style="display:none;">
<p>There are four main trends that occur in a normal probability plot. Examples of each are plotted below with a histogram of the data next to the normal probability plot.</p>
<p>Often the plot is called a Q-Q Plot, which stands for quantile-quantile plot. The idea is to compare the observed distribution of data to what the distribution should look like in theory if it was normal. Q-Q Plots are more general than normal probability plots because they can be used with any theoretical distribution, not just the normal distribution.</p>
</div>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

set.seed(123)

  tmp &lt;- rnorm(100)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Normal&quot;, col=&quot;skyblue&quot;)
  
  tmp &lt;- Davis$weight
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Right-skewed&quot;,
       breaks=15, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

  tmp &lt;- rbeta(100, 5,1)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Left-skewed&quot;,
       breaks=seq(min(tmp),max(tmp), length.out=13), col=&quot;firebrick&quot;)
  
  tmp &lt;- rbeta(100,2,2)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Heavy-tailed&quot;, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-24-2.png" width="672" /></p>
</div>
<p><br /></p>
<h5 id="residuals-versus-order-plot-checks-assumption-5">Residuals versus Order Plot: Checks Assumption #5</h5>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-25-1.png" width="144" />
</td>
<td width="75%">
<p>When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated.</p>
<p>| <a href="javascript:showhide('resorderplots')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="resorderplots" style="display:none;">
<p><a href="javascript:showhide('resorderplotsread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="resorderplotsread" style="display:none;">
<p>Plotting the residuals against the order in which the data was collected provides insight as to whether or not the observations can be considered independent. If the plot shows no trend, then the error terms are considered independent and the regression assumption satisfied. If there is a visible trend in the plot, then the regression assumption is likely violated.</p>
</div>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

  tmp &lt;- lm(mpg ~ disp, data=mtcars)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Good: No Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)

  tmp &lt;- lm(height ~ age, data=Loblolly)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Questionable: General Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;orangered&quot;)

  tmp &lt;- lm(hp ~ qsec, data=mtcars)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Questionable: Interesting Patterns&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;orangered&quot;)
  
  tmp &lt;- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),])
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Bad: Obvious Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
</div>
<p><br /></p>
<h5 id="problems-from-failed-assumptions">Problems from Failed Assumptions</h5>
<p>There are various problems that can arise when certain of the regression assupmtions are not satisfied.</p>
<p><strong>Lack of Linearity</strong></p>
<p>When the linearity assumption is violated, pretty much everything we obtain from the regression summary is no longer meaningful.</p>
<ul>
<li>The y-intercept estimate can be drastically off from its actual true value.</li>
<li><p>Important model information is lost by trying to use a simple slope term <span class="math inline">\(\beta_1\)</span> to describe the model with respect to <span class="math inline">\(X\)</span>.</p></li>
<li><p>The residual standard error will be much higher than it otherwise would be because of curvature patterns in the data that the line cannot capture. Thus, R-squared will be lower than it otherwise should be.</p></li>
<li><p>P-values can become non-significant, when in fact there is a strong pattern in the data, but that pattern just cannot be captured by a simple line.</p></li>
</ul>
<p>*Normality of the errors is often put into question as well when a simplified line is used to try to capture a more complicated curved model.</p>
<p>The plot below demonstrate these difficulties.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 7.5                     #True slope

beta_2 &lt;- -0.25                   #True bend

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- rnorm(n, 0, sigma)   #normally distributed errors

Y_i &lt;- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i 
                                  #Sample of Y-values from model


# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;Non-Linear Relation&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

curve(beta_0 + beta_1*x + beta_2*x^2, col=&quot;gray&quot;, lty=2, add=TRUE) 
                                 #Add True line to plot
      

                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (True value:&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (True value:&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (True value:&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p><br/></p>
<p><strong>Non-normal Error Terms</strong></p>
<p>When the normality assumption for the error terms is violated, not all is lost. In fact, the estimate of the slope and intercept are still often fairly meaningful. However, it is unwise to put too much trust in the residual standard error as an estimate of the standard deviation <span class="math inline">\(\sigma\)</span>. This is because the standard deviation in skewed distributions does not carry the same meaning it has in normal distributions.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 3.5                     #True slope

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- rchisq(n, 1)*3 - 1 #non-normally distributed errors

Y_i &lt;- beta_0 + beta_1*X_i + epsilon_i 
                                  #Sample of Y-values from model


# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;Normality Assumption Violated&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

abline(beta_0, beta_1,           #Add True line to plot
       col=&quot;gray&quot;, lty=2)

                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p><strong>Unconstant Variance</strong></p>
<p>**X</p>
</div>
<p><br /></p>
</div>
<div id="estimating-the-model-parameters-expand" class="section level4">
<h4>Estimating the Model Parameters <a href="javascript:showhide('estimatingparameters')" style="font-size:.6em;color:skyblue;" id="estMod">(Expand)</a></h4>
<p><span class="expand-caption">How to get <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>: least squares &amp; maximum likelihood…</span></p>
<div id="estimatingparameters" style="display:none;">
<p>There are two approaches to estimating the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the regression model. The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood (see below). Fortunately, for simple linear regression, the estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> obtained from either method are identical. The estimates for the true parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are typically denoted by <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, respectively, and are given by the following formulas.</p>
<table style="width:74%;">
<colgroup>
<col width="29%" />
<col width="31%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter Estimate</th>
<th>Mathematical Formula</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Slope</td>
<td><span class="math inline">\(b_1 = \frac{\sum X_i(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}\)</span></td>
<td><code>b_1 &lt;- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )</code></td>
</tr>
<tr class="even">
<td>Intercept</td>
<td><span class="math inline">\(b_0 = \bar{Y} - b_1\bar{X}\)</span></td>
<td><code>b_0 &lt;- mean(Y) - b_1*mean(X)</code></td>
</tr>
</tbody>
</table>
It is important to note that these estimates are entirely determined from the observed data <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. When the regression equation is written using the estimates instead of the parameters, we use the notation <span class="math inline">\(\hat{Y}\)</span>, which is the estimator of <span class="math inline">\(E\{Y\}\)</span>. Thus, we write
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1 X_i
\end{equation}\]</span>
which is directly comparable to the true, but unknown values
<span class="math display">\[\begin{equation}
  E\{Y_i\} = \beta_0 + \beta_1 X_i. 
  \label{exp}
\end{equation}\]</span>
<h5 id="leastSquares">Least Squares</h5>
<p>To estimate the model parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> using least squares, we start by defining the function <span class="math inline">\(Q\)</span> as the sum of the squared errors, <span class="math inline">\(\epsilon_i\)</span>. <span class="math display">\[
  Q = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1 X_i))^2
\]</span> Then we use the function Q as if it were a function of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Ironically, the values of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are considered fixed. However, this makes sense because once a particular data set has been observed, these values are all known for that data set. What we don’t know are the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>This <a href="https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html">least squares applet</a> is a good way to explore how various choices of the slope and intercept yield different values of the “sum of squared residuals”. But it turns out that there is one “best” choice of the slope and intercept that yields a “smallest” value of the “sum of squared residuals.” This best choice can actually be found using calculus by taking the partial derivatives of <span class="math inline">\(Q\)</span> with respect to both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. <span class="math display">\[
  \frac{\partial Q}{\partial \beta_0} = -2\sum (Y_i - \beta_0 - \beta_1X_i)
\]</span> <span class="math display">\[
  \frac{\partial Q}{\partial \beta_1} = -2\sum X_i(Y_i-\beta_0-\beta_1X_i)
\]</span> Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize <span class="math inline">\(Q\)</span> for a given set of data. After all the calculations are completed we find the values of the parameter estimators <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> (of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively) are as stated previously.</p>
<h5 id="mle">Maximum Likelihood</h5>
<p>The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> which minime the least squares <span class="math inline">\(Q\)</span> function, we choose the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the <span class="math inline">\(Y_i\)</span> for all observations <span class="math inline">\(i=1,\ldots,n\)</span>. We can do this rather simply by using the assumption that the errors, <span class="math inline">\(\epsilon_i\)</span> are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if <span class="math inline">\(f(Y_i)\)</span> denotes the probability density function for <span class="math inline">\(Y_i\)</span>, then the joint probability density for all <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(f(Y_1,\ldots,Y_n)\)</span> is given by <span class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) 
\]</span> Since each <span class="math inline">\(Y_i\)</span> is assumed to be normally distributed with mean <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (see model ()) we have that <span class="math display">\[
  f(Y_i) = \frac{1}{\sqrt{2\pi}\sigma}\exp{\left[-\frac{1}{2}\left(\frac{Y_i-\beta_0-\beta_1X_i}{\sigma}\right)^2\right]}
\]</span> which provides the joint probability as <span class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> The likelihood function <span class="math inline">\(L\)</span> is then given by consider the <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> fixed and the parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span> as the variables in the function. <span class="math display">\[
  L(\beta_0,\beta_1,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> Instead of taking partial derivatives of <span class="math inline">\(L\)</span> directly (with respect to all parameters) we take the partial derivatives of the <span class="math inline">\(\log\)</span> of <span class="math inline">\(L\)</span>, which is easier to work with. In a similar, but more difficult calculation, to that of minimizing <span class="math inline">\(Q\)</span>, we obtain the values of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma^2\)</span> which maximize the log of <span class="math inline">\(L\)</span>, and which therefore maximize <span class="math inline">\(L\)</span>. (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate <span class="math inline">\(\hat{\sigma}^2\)</span> of <span class="math inline">\(\sigma^2\)</span>. <span class="math display">\[
  \hat{\sigma}^2 = \frac{\sum(Y_i-\hat{Y}_i)^2}{n}
\]</span></p>
</div>
<p><br /></p>
</div>
<div id="estimating-the-model-variance-expand" class="section level4">
<h4>Estimating the Model Variance <a href="javascript:showhide('estimatingvariance')" style="font-size:.6em;color:skyblue;" id="varEst">(Expand)</a></h4>
<p><span class="expand-caption">Estimating <span class="math inline">\(\sigma^2\)</span> with MSE…</span></p>
<div id="estimatingvariance" style="display:none;">
As shown previously, we can obtain estimates for the model parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> with either least squares estimation or maximum likelihood estimation. It turns out that these estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. On the other hand, the maximum likelihood estimate <span class="math inline">\(\hat{\sigma}^2\)</span> of the model variance <span class="math inline">\(\sigma^2\)</span> is a biased estimator. It is consistently wrong in its estimates of <span class="math inline">\(\sigma^2\)</span>. Without going into all the details, <span class="math inline">\(\hat{\sigma}^2\)</span> is a biased estimator of <span class="math inline">\(\sigma^2\)</span> because its denominator needs to represent the degrees of freedom associated with the numerator. Since <span class="math inline">\(\hat{Y}_i\)</span> in the numerator of <span class="math inline">\(\hat{\sigma}^2\)</span> is defined by
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1X_i
  \label{hatY}
\end{equation}\]</span>
it follows that two means, <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{Y}\)</span>, must be estimated from the data to obtain <span class="math inline">\(\hat{Y}_i\)</span>, see formulas () and () for details. Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for <span class="math inline">\(\hat{\sigma}^2\)</span> should be <span class="math inline">\(n-2\)</span> instead of <span class="math inline">\(n\)</span>. Some incredibly long calculations will show that the estimator
<span class="math display">\[\begin{equation}
  s^2 = MSE = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2}
\end{equation}\]</span>
is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. Here <span class="math inline">\(MSE\)</span> stands for mean squared error, which is the most obvious name for a formula that squares the errors <span class="math inline">\(Y_i-\hat{Y}_i\)</span> then adds them up and divides by their degrees of freedom. Similarly, we call the numerator <span class="math inline">\(\sum(Y_i-\hat{Y}_i)^2\)</span> the sum of the squared errors, denoted by <span class="math inline">\(SSE\)</span>. It is also important to note that the errors are often denoted by <span class="math inline">\(e_i = Y_i-\hat{Y}_i\)</span>. Putting this all together we get the following equivalent statements for <span class="math inline">\(MSE\)</span>.
<span class="math display">\[\begin{equation}
  s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2} = \frac{\sum e_i^2}{n-2}
\end{equation}\]</span>
<p>As a final note, even though the <span class="math inline">\(E\{MSE\} = \sigma^2\)</span>, <span class="math inline">\(MSE\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, it unfortunately isn’t true that <span class="math inline">\(\sqrt{MSE}\)</span> is an unbiased estimator of <span class="math inline">\(\sigma\)</span>. This presents a few problems later on.</p>
</div>
<p><br /></p>
</div>
<div id="transformations-expand" class="section level4">
<h4>Transformations <a href="javascript:showhide('transformations')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(Y&#39;\)</span>, <span class="math inline">\(X&#39;\)</span>, and returning to the original space…</span></p>
<div id="transformations" style="display:none;">
<p>Y transformations are denoted by y-prime, written <span class="math inline">\(Y&#39;\)</span>, and consist of raising <span class="math inline">\(Y\)</span> to some power called <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[
  Y&#39; = Y^\lambda \quad \text{(Y Transformation)}
\]</span></p>
<table>
<thead>
<tr class="header">
<th align="center">Value of <span class="math inline">\(\lambda\)</span></th>
<th>Transformation to Use</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-2</td>
<td><span class="math inline">\(Y&#39; = Y^{-2} = 1/Y^2\)</span></td>
<td><code>lm(Y^-2 ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">-1</td>
<td><span class="math inline">\(Y&#39; = Y^{-1} = 1/Y\)</span></td>
<td><code>lm(Y^-1 ~ X)</code></td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td><span class="math inline">\(Y&#39; = \log(Y)\)</span></td>
<td><code>lm(log(Y) ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">0.5</td>
<td><span class="math inline">\(Y&#39; = \sqrt(Y)\)</span></td>
<td><code>lm(sqrt(Y) ~ X)</code></td>
</tr>
<tr class="odd">
<td align="center">1</td>
<td><span class="math inline">\(Y&#39; = Y\)</span></td>
<td><code>lm(Y ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td><span class="math inline">\(Y&#39; = Y^2\)</span></td>
<td><code>lm(Y^2 ~ X)</code></td>
</tr>
</tbody>
</table>
<p>Using “maximum-likelihood” estimation, the Box-Cox procedure can actually automatically detect the “optimal” value of <span class="math inline">\(\lambda\)</span> to consider for a Y-transformation. Keep in mind however, that simply accepting a suggested Y-transformation without considering the scatterplot and diagnostic plots first, is unwise.</p>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'ScatterplotView')">
Scatterplot Recognition
</button>
<button class="tablinks" onclick="openTab(event, 'BoxCoxView')">
Box-Cox Suggestion
</button>
<button class="tablinks" onclick="openTab(event, 'YTransExample')">
An Example
</button>
</div>
<div id="ScatterplotView" class="tabcontent" style="display:block;">
<p>
<h6 id="scatterplot-recognition">Scatterplot Recognition</h6>
<p>The following panel of scatterplots can give you a good feel for when to try different values of <span class="math inline">\(\lambda\)</span>.</p>
<pre class="r"><code>set.seed(15)
N &lt;- 300
X &lt;- runif(N, 5, 50)
Y &lt;- 25 + 3.5*X + rnorm(N, 0, 20)

Ya &lt;- 1/sqrt(Y)   #1/Y^2   Lam = -2
Yb &lt;- 1/Y         #1/Y     Lam = -1
Yc &lt;- exp(.02*Y)  #log(Y)  Lam =  0
Yd &lt;- Y^2         #sqrt(Y) Lam =  0.5
Ye &lt;- Y           #Y       Lam =  1
Yf &lt;- sqrt(Y)     #Y^2     Lam =  2


par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0))

plot(Ya ~ X, main=expression(paste(&quot;Use &quot;, lambda == -2)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Ya^-2 ~ X))
curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Yb ~ X, main=expression(paste(&quot;Use &quot;, lambda == -1)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Yb^-1 ~ X))
curve(1/(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Yc ~ X, main=expression(paste(&quot;Use &quot;, lambda == 0, &quot; i.e., log(...)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(log(Yc) ~ X))
curve(exp(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Yd ~ X, main=expression(paste(&quot;Use &quot;, lambda == 0.5)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(sqrt(Yd) ~ X))
curve((b[1] + b[2]*x)^2, add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Ye ~ X, main=expression(paste(&quot;Use &quot;, lambda == 1, &quot; (No Transformation)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Ye ~ X))
curve((b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Yf ~ X, main=expression(paste(&quot;Use &quot;, lambda == 2)), 
ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Yf^2 ~ X))
curve(sqrt(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</p>
</div>
<div id="BoxCoxView" class="tabcontent">
<p>
<h6 id="box-cox-suggestion">Box-Cox Suggestion</h6>
<p>The <code>boxCox(...)</code> function in <code>library(car)</code> can also be helpful on finding values of <span class="math inline">\(\lambda\)</span> to try.</p>
<pre class="r"><code>par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0))

boxCox(lm(Ya ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == -2)), line=.5)

boxCox(lm(Yb ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == -1)), line=.5)

boxCox(lm(Yc ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 0, &quot; i.e., log(...)&quot;)), line=.5)

boxCox(lm(Yd ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 0.5)), line=.5)

boxCox(lm(Ye ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 1, &quot; (No Transformation)&quot;)), line=.5)

boxCox(lm(Yf ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 2)), line=.5)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
</p>
</div>
<div id="YTransExample" class="tabcontent">
<p>
<h6 id="an-example">An Example</h6>
<p>Suppose <span class="math inline">\(\lambda = 0.5\)</span>, so that <span class="math inline">\(Y&#39; = \sqrt{Y}\)</span>.</p>
<p>A regression is performed using <code>sqrt(Y)</code>:</p>
<p><code>cars.lm.t &lt;- lm(sqrt(dist) ~ speed, data=cars)</code></p>
<p><code>summary(cars.lm.t)</code></p>
<table>
<thead>
<tr class="header">
<th> </th>
<th>Estimate</th>
<th>Std. Error</th>
<th>t value</th>
<th>Pr(&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>(Intercept)</strong></td>
<td>1.277</td>
<td>0.4844</td>
<td>2.636</td>
<td>0.01126</td>
</tr>
<tr class="even">
<td><strong>speed</strong></td>
<td>0.3224</td>
<td>0.02978</td>
<td>10.83</td>
<td>1.773e-14</td>
</tr>
</tbody>
</table>
<p>Then,</p>
<p><span class="math display">\[
  \hat{Y}_i&#39; = 1.277 + 0.3224 X_i
\]</span></p>
<p>And replacing <span class="math inline">\(\hat{Y}_i&#39; = \sqrt{\hat{Y}_i}\)</span> we have</p>
<p><span class="math display">\[
  \sqrt{\hat{Y}_i} = 1.277 + 0.3224 X_i
\]</span></p>
<p>Solving for <span class="math inline">\(\hat{Y}_i\)</span> gives</p>
<p><span class="math display">\[
  \hat{Y}_i = (1.277 + 0.3224 X_i)^2
\]</span></p>
<p>Which, using <code>curve((1.277 + 0.3224*x)^2, add=TRUE)</code> (see code for details) looks like this:</p>
<pre class="r"><code>plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1,
     xlab=&quot;Speed of the Vehicle (mph) \n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;,
     main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;)
mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5)
legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;)

curve( (1.277 + 0.3224*x)^2, add=TRUE, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</p>
</div>
<p><br /></p>
<h5 id="x-transformations">X-Transformations</h5>
<p>X-transformations are more difficult to recognize than y-transformations. This is partially because there is no Box-Cox method to automatically search for them.</p>
<p>The best indicator that you should consider an x-transformation is when the variance of the residuals is constant across all fitted-values, but linearity is clearly violated.</p>
<p>The following panel of scatterplots can give you a good feel for when to try different values of an x-transformation.</p>
<pre class="r"><code>set.seed(15)
N &lt;- 300
X &lt;- runif(N, 5, 50)
Y &lt;- 25 + 3.5*X + rnorm(N, 0, 20)

Xa &lt;- 1/sqrt(X)   #1/X^2   Lam = -2
Xb &lt;- 1/X         #1/X     Lam = -1
Xc &lt;- exp(.02*X)  #log(X)  Lam =  0
Xd &lt;- X^2         #sqrt(X) Lam =  0.5
Xe &lt;- X           #X       Lam =  1
Xf &lt;- sqrt(X)     #X^2     Lam =  2


par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0))

plot(Y ~ Xa, main=expression(paste(&quot;Use &quot;, X*minute == X^-2)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xa^-2)))
curve(b[1] + b[2]*x^-2, add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Y ~ Xb, main=expression(paste(&quot;Use &quot;, X*minute == X^-1)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xb^-1)))
curve(b[1] + b[2]*x^-1, add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xc, main=expression(paste(&quot;Use &quot;, X*minute == log(X))), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ log(Xc)))
curve(b[1] + b[2]*log(x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Y ~ Xd, main=expression(paste(&quot;Use &quot;, X*minute == sqrt(X))), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ sqrt(Xd)))
curve(b[1] + b[2]*sqrt(x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xe, main=expression(paste(&quot;Use &quot;, X*minute == X, &quot; (No Transformation)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ Xe))
curve((b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xf, main=expression(paste(&quot;Use &quot;, X*minute == X^2)), 
ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xf^2)))
curve(b[1] + b[2]*x^2, add=TRUE, col=&quot;green&quot;, lwd=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
<p><br /></p>
</div>
<div id="inference-for-the-model-parameters-expand" class="section level4">
<h4>Inference for the Model Parameters <a href="javascript:showhide('inference1')" style="font-size:.6em;color:skyblue;" id="infModelParam">(Expand)</a></h4>
<p><span class="expand-caption">t test formulas, sampling distributions, confidence intervals, and F tests…</span></p>
<div id="inference1" style="display:none;">
<p>We are sometimes interested in making inference about <span class="math inline">\(\beta_0\)</span>, the y-intercept. However, most inference in regression is focused on the slope, <span class="math inline">\(\beta_1\)</span>. Recall that the interpretation of <span class="math inline">\(\beta_1\)</span> is the amount of increase (or decrease) in the expected value (average value) of <span class="math inline">\(Y\)</span> per unit change in <span class="math inline">\(X\)</span>.</p>
<p>Two types of inference about <span class="math inline">\(\beta_1\)</span>, or similarly <span class="math inline">\(\beta_0\)</span> when applicable, are of interest.</p>
<table class="fancytable">
<tr>
<th>
<strong>Hypotheses</strong>
</th>
<th>
<strong>Test Statistic</strong>
</th>
<th>
<strong>P-value</strong>
</th>
</tr>
<tr>
<td style="text-align:center;width:25%;">
<p><span class="math inline">\(H_0: \beta_0 =\)</span> <span class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a number}\)</span> <span class="tooltiprtext">This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown.</span> </span></p>
<p><span class="math inline">\(H_a: \beta_0\)</span><span class="tooltiprbold"> <span class="math inline">\(\,\neq\,\)</span> <span class="tooltiprtext">You could use <span class="math inline">\(&gt;\)</span> or <span class="math inline">\(&lt;\)</span> instead of <span class="math inline">\(\neq\)</span> for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses <span class="math inline">\(\neq\)</span>.</span> </span><span class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a number}\)</span> <span class="tooltiprtext">This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown.</span> </span></p>
</td>
<td style="text-align:center;width:25%;">
<p><span class="tooltiprbold"> <span class="math display">\[t = \frac{b_0 - \overbrace{0}^\text{a number}}{s_{b_0}}\]</span> <span class="tooltiprtext">This is the formula for the test statistic. It measures how far the estimated y-intercept <span class="math inline">\(b_0\)</span> is from the null hypothesis for <span class="math inline">\(\beta_0\)</span> in units of “standard errors of <span class="math inline">\(b_0\)</span>”. Thus the division by <span class="math inline">\(s_{b_0}\)</span>. Though the hypothesized value of <span class="math inline">\(\beta_0\)</span> is typically 0, it could be any number.</span> </span></p>
</td>
<td style="text-align:center;width:50%;">
<img src="LinearRegression_files/figure-html/unnamed-chunk-33-1.png" width="672" />
</td>
</tr>
<tr>
<td style="text-align:center;width:25%;">
<p><span class="math inline">\(H_0: \beta_1 =\)</span> <span class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a number}\)</span> <span class="tooltiprtext">This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown.</span> </span></p>
<p><span class="math inline">\(H_a: \beta_1\)</span><span class="tooltiprbold"> <span class="math inline">\(\,\neq\,\)</span> <span class="tooltiprtext">You could use <span class="math inline">\(&gt;\)</span> or <span class="math inline">\(&lt;\)</span> instead of <span class="math inline">\(\neq\)</span> for the alternative hypothesis. By default, the p-value from summary(mylm) in R uses <span class="math inline">\(\neq\)</span>.</span> </span><span class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a number}\)</span> <span class="tooltiprtext">This could be any number, not just 0. However, the default summar(mylm) output in R only shows the test statistic and p-value for the test that uses 0. To test a different value, you would need to compute the test statistic and p-value by hand using the formula shown.</span> </span></p>
</td>
<td style="text-align:center;width:25%;">
<p><span class="tooltiprbold"> <span class="math display">\[t = \frac{b_1 - \overbrace{0}^\text{a number}}{s_{b_1}}\]</span> <span class="tooltiprtext">This is the formula for the test statistic. It measures how far the estimated slope <span class="math inline">\(b_1\)</span> is from the null hypothesis for <span class="math inline">\(\beta_1\)</span> in units of “standard errors of <span class="math inline">\(b_1\)</span>”. Thus the division by <span class="math inline">\(s_{b_1}\)</span>. Though the hypothesized value of <span class="math inline">\(\beta_1\)</span> is typically 0, it could be any number.</span> </span></p>
</td>
<td>
<p>Left-tailed p-value = <code>pt(-abs(tvalue), degrees of freedom)</code>.</p>
Double it to get the two-sided p-value.
</td>
</tr>
</table>
<table class="fancytable">
<tr>
<th>
<strong>Confidence Interval</strong>
</th>
<th>
<strong>Formula</strong>
</th>
<th>
<strong>Standard Error</strong>
</th>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(\beta_0\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(b_0 \pm\)</span><span class="tooltiprbold"> <span class="math inline">\(t^*\)</span> <span class="tooltiprtext">This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom <span class="math inline">\(n-p\)</span> (sample size - number of parameters in the regression model).</span> </span><span class="tooltiprbold"> <span class="math inline">\(\cdot\)</span> <span class="tooltiprtext">The critical value is multiplied by the standard error of <span class="math inline">\(b_0\)</span>.</span> </span><span class="tooltiprbold"> <span class="math inline">\(s_{b_0}\)</span> <span class="tooltiprtext">The standard error of <span class="math inline">\(b_0\)</span>, denoted by <span class="math inline">\(s_{b_0}\)</span> is provided in the regression summary output under the column header called “Std. Error” for the “(Intercept)” row of the output. It is calculated using the formula shown below.</span> </span>
</td>
<td style="text-align:center;">
<span class="tooltiprbold"> <span class="math display">\[s^2_{b_0} = MSE\left[\frac{1}{n} + \frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]\]</span> <span class="tooltiprtext">This is called the “estimated variance of <span class="math inline">\(b_0\)</span>”. Taking the square root of this number gives the “standard error of <span class="math inline">\(b_0\)</span>”.</span> </span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(\beta_1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(b_1 \pm\)</span><span class="tooltiprbold"> <span class="math inline">\(t^*\)</span> <span class="tooltiprtext">This is called the “critical value” and denotes the number of standard deviations that are needed to obtain a 95% confidence interval from a t distribution with degrees of freedom <span class="math inline">\(n-p\)</span> (sample size - number of parameters in the regression model).</span> </span><span class="tooltiprbold"> <span class="math inline">\(\cdot\)</span> <span class="tooltiprtext">The critical value is multiplied by the standard error of <span class="math inline">\(b_1\)</span>.</span> </span><span class="tooltiprbold"> <span class="math inline">\(s_{b_1}\)</span> <span class="tooltiprtext">The standard error of <span class="math inline">\(b_1\)</span>, denoted by <span class="math inline">\(s_{b_1}\)</span> is provided in the regression summary output under the column header called “Std. Error”. It is calculated using the formula shown below.</span> </span>
</td>
<td style="text-align:center;">
<span class="tooltiprbold"> <span class="math display">\[s^2_{b_1} = \frac{MSE}{\sum(X_i-\bar{X})^2}\]</span> <span class="tooltiprtext">This is called the “estimated variance of <span class="math inline">\(b_1\)</span>”. Taking the square root of this number gives the “standard error of <span class="math inline">\(b_1\)</span>”.</span> </span>
</td>
</tr>
</table>
<p>To be more exact, the types of inference we are interested in are the following.</p>
<ol style="list-style-type: decimal">
<li><p>Determine if there is evidence of a meaningful linear relationship in the data. If <span class="math inline">\(\beta_1 = 0\)</span>, then there is no relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(E\{Y\}\)</span>. Hence we might be interested in testing the hypotheses <span class="math display">\[
  H_0: \beta_1 = 0
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq 0 
\]</span></p></li>
<li><p>Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form <span class="math display">\[
  H_0: \beta_1 = \beta_{10}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} 
\]</span> where <span class="math inline">\(\beta_{10}\)</span> is some hypothesized number.</p></li>
<li><p>To provide a confidence interval for the true value of <span class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p><br /></p>
<p>Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the <strong>sampling distribution</strong> of the estimate <span class="math inline">\(b_1\)</span> of the parameter <span class="math inline">\(\beta_1\)</span>. And, while we are at it, we may as well come to understand the sampling distribution of the estimate <span class="math inline">\(b_0\)</span> of the parameter <span class="math inline">\(\beta_0\)</span>.</p>
<div style="padding-left:30px;color:darkgray;font-size:.8em;">
<p>Review <a href="http://statistics.byuimath.com/index.php?title=Lesson_6:_Distribution_of_Sample_Means_%26_The_Central_Limit_Theorem#Introduction_to_Sampling_Distributions">sampling distributions</a> from Math 221.</p>
</div>
<p>Since <span class="math inline">\(b_1\)</span> is an estimate, it will vary from sample to sample, even though the truth, <span class="math inline">\(\beta_1\)</span>, remains fixed. (The same holds for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(\beta_0\)</span>.) It turns out that the sampling distribution of <span class="math inline">\(b_1\)</span> (where the <span class="math inline">\(X\)</span> values remain fixed from study to study) is normal with mean and variance: <span class="math display">\[
  \mu_{b_1} = \beta_1
\]</span> <span class="math display">\[
  \sigma^2_{b_1} = \frac{\sigma^2}{\sum(X_i-\bar{X})^2}
\]</span></p>
<pre class="r"><code>## Simulation to Show relationship between Standard Errors

##-----------------------------------------------
## Edit anything in this area... 

n &lt;- 100 #sample size
Xstart &lt;- 30 #lower-bound for x-axis
Xstop &lt;- 100 #upper-bound for x-axis

beta_0 &lt;- 2 #choice of true y-intercept
beta_1 &lt;- 3.5 #choice of true slope
sigma &lt;- 13.8 #choice of st. deviation of error terms

## End of Editable area.
##-----------------------------------------------


# Create X, which will be used in the next R-chunk.
X &lt;- rep(seq(Xstart,Xstop, length.out=n/2), each=2) 

## After playing this chunk, play the next chunk as well.</code></pre>
<p>To see that this is true, consider the regression model with values specified for each parameter as follows.</p>
<p><span class="math display">\[
  Y_i = \overbrace{\beta_0}^{2} + \overbrace{\beta_1}^{3.5} X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \overbrace{\sigma^2}^{\sigma=13.8})
\]</span></p>
<p>Using the equations above for <span class="math inline">\(\mu_{b_1}\)</span> and <span class="math inline">\(\sigma^2_{b_1}\)</span> we obtain that the mean of the sampling distribution of <span class="math inline">\(b_1\)</span> will be</p>
<p><span class="math inline">\(\mu_{b_1} = \beta_1 = 3.5\)</span></p>
<p>Further, we see that the variance of the sampling distribution of <span class="math inline">\(b_1\)</span> will be</p>
<p><span class="math inline">\(\sigma^2_{b_1} = \frac{\sigma^2}{\sum(X_i-\bar{X})^2} = \frac{13.8^2}{4.25\times 10^{4}}\)</span></p>
<p>Taking the square root of the variance, the standard deviation of the sampling distribution of <span class="math inline">\(b_1\)</span> will be</p>
<p><span class="math inline">\(\sigma_{b_1} = 0.067\)</span>.</p>
<p>That’s very nice. But to really believe it, let’s run a simulation ourselves. The “Code” below is worth studying. It runs a simulation that (1) takes a sample of data from the true regression relation, (2) fits the sampled data with an estimated regression equation (gray lines in the plot), and (3) computes the estimated values of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_0\)</span> for that regression.</p>
<p>After doing this many, many times, the results of every single regression are plotted (in gray lines, which creates a gray shaded region because there are so many lines) in the scatterplot below. Further, each obtained estimate of <span class="math inline">\(b_0\)</span> is plotted in the histogram on the left (below the scatterplot) and each obtained estimate of <span class="math inline">\(b_1\)</span> is plotted in the histogram on the right. Looking at the histograms carefully, it can be seen that the mean of each histogram is very close to the true parameter value of <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span>, respectively. Also, the “Std. Error” of each histogram is incredibly close (if not exact to 3 decimal places) to the computed value of <span class="math inline">\(\sigma_{b_0}\)</span> and <span class="math inline">\(\sigma_{b_1}\)</span>, respectively. Amazing!</p>
<pre class="r"><code>N &lt;- 5000 #number of times to pull a random sample
storage_b0 &lt;- storage_b1 &lt;- storage_rmse &lt;- rep(NA, N)
for (i in 1:N){
  Y &lt;- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
  mylm &lt;- lm(Y ~ X)
  storage_b0[i] &lt;- coef(mylm)[1]
  storage_b1[i] &lt;- coef(mylm)[2]
  storage_rmse[i] &lt;- summary(mylm)$sigma
}


layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart &lt;- 0 #min(0,min(Y)) 
Ystop &lt;- 500 #max(max(Y), 0)
Yrange &lt;- Ystop - Ystart

plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col=&quot;gray&quot;,
     main=&quot;Regression Lines from many Samples (gray lines) \n Plus Residual Standard Deviation Lines (green lines)&quot;)
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)
text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1)


for (i in 1:N){
  abline(storage_b0[i], storage_b1[i], col=&quot;darkgray&quot;)  
}
abline(beta_0, beta_1, col=&quot;green&quot;, lwd=3)
abline(beta_0+sigma, beta_1, col=&quot;green&quot;, lwd=2)
abline(beta_0-sigma, beta_1, col=&quot;green&quot;, lwd=2)
abline(beta_0+2*sigma, beta_1, col=&quot;green&quot;, lwd=1)
abline(beta_0-2*sigma, beta_1, col=&quot;green&quot;, lwd=1)
abline(beta_0+3*sigma, beta_1, col=&quot;green&quot;, lwd=.5)
abline(beta_0-3*sigma, beta_1, col=&quot;green&quot;, lwd=.5)

par(mai=c(1,.6,.5,.01))

  addnorm &lt;- function(m,s, col=&quot;firebrick&quot;){
    curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
    lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
    lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
    lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
    lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
    lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
    lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
    lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
    legend(&quot;topleft&quot;, legend=paste(&quot;Std. Error = &quot;, round(s,3)), cex=0.7, bty=&quot;n&quot;)
  }

  h0 &lt;- hist(storage_b0, 
             col=&quot;skyblue3&quot;, 
             main=&quot;Sampling Distribution\n Y-intercept&quot;,
             xlab=expression(paste(&quot;Estimates of &quot;, beta[0], &quot; from each Sample&quot;)),
             freq=FALSE, yaxt=&#39;n&#39;, ylab=&quot;&quot;)
  m0 &lt;- mean(storage_b0)
  s0 &lt;- sd(storage_b0)
  addnorm(m0,s0, col=&quot;green&quot;)
  
  h1 &lt;- hist(storage_b1, 
             col=&quot;skyblue3&quot;, 
             main=&quot;Sampling Distribution\n Slope&quot;,
             xlab=expression(paste(&quot;Estimates of &quot;, beta[1], &quot; from each Sample&quot;)),
             freq=FALSE, yaxt=&#39;n&#39;, ylab=&quot;&quot;)
  m1 &lt;- mean(storage_b1)
  s1 &lt;- sd(storage_b1)
  addnorm(m1,s1, col=&quot;green&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-35-1.png" width="768" /></p>
<div style="padding-left:15px;">
<h5 id="tTests">t Tests</h5>
<p>Using the information above about the sampling distributions of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_0\)</span>, an immediate choice of statistical test to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10} 
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} 
\]</span> where <span class="math inline">\(\beta_{10}\)</span> can be zero, or any other value, is a t test given by <span class="math display">\[
  t = \frac{b_1 - \beta_{10}}{s_{b_1}}
\]</span> where <span class="math inline">\(s^2_{b_1} = \frac{MSE}{\sum(X_i-\bar{X})^2}\)</span>. (You may want to review the section “Estimating the Model Variance” of this file to know where MSE came from.) With quite a bit of work it has been shown that <span class="math inline">\(t\)</span> is distributed as a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. The nearly identical test statistic for testing <span class="math display">\[
  H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_0 \neq \beta_{00} 
\]</span> is given by <span class="math display">\[
  t = \frac{b_0 - \beta_{00}}{s_{b_0}}
\]</span> where <span class="math inline">\(s^2_{b_0} = MSE\left[\frac{1}{n}+\frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]\)</span>. This version of <span class="math inline">\(t\)</span> has also been shown to be distributed as a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<h5 id="confidence-intervals">Confidence Intervals</h5>
<p>Creating a confidence interval for either <span class="math inline">\(\beta_1\)</span> or <span class="math inline">\(\beta_0\)</span> follows immediately from these results using the formulas <span class="math display">\[
  b_1 \pm t^*_{n-2}\cdot s_{b_1}
\]</span> <span class="math display">\[
  b_0 \pm t^*_{n-2}\cdot s_{b_0}
\]</span> where <span class="math inline">\(t^*_{n-2}\)</span> is the critical value from a t distribution with <span class="math inline">\(n-2\)</span> degrees of freedom corresponding to the chosen confidence level.</p>
<p><br /></p>
<h5 id="Ftests">F tests</h5>
<p>Another way to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10}  \quad\quad \text{or} \quad\quad H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} \quad\quad \ \ \quad \quad H_a: \beta_0 \neq \beta_{00}
\]</span> is with an <span class="math inline">\(F\)</span> Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an <span class="math inline">\(F\)</span> test is very general and can be used in many places that a t test cannot.</p>
<p>In its most general form, the <span class="math inline">\(F\)</span> test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that <span class="math inline">\(H_0:\beta_1=0\)</span> against the alternative that <span class="math inline">\(H_a: \beta_1\neq 0\)</span>, we are essentially comparing two models against each other. If <span class="math inline">\(\beta_1=0\)</span>, then the corresponding model would be <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. If <span class="math inline">\(\beta_1\neq0\)</span>, then the model remains <span class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the <span class="math inline">\(F\)</span> Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example.</p>
<p>Say we wanted to test the hypothesis that <span class="math inline">\(H_0:\beta_1 = 2.5\)</span> against the alternative that <span class="math inline">\(\beta_1\neq2.5\)</span>. Then the null, or reduced model, would be <span class="math inline">\(E\{Y_i\}=\beta_0+2.5X_i\)</span>. The alternative, or full model, would be <span class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. Thus, the null (reduced) model contains only one “free” parameter because <span class="math inline">\(\beta_1\)</span> has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model.</p>
<p>Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing <span class="math inline">\(H_0: \beta_1=0\)</span> against <span class="math inline">\(H_a:\beta_1\neq0\)</span> we have the partition <span class="math display">\[
  \underbrace{Y_i-\bar{Y}}_{Total} = \underbrace{\hat{Y}_i - \bar{Y}}_{Regression} + \underbrace{Y_i-\hat{Y}_i}_{Error}
\]</span> The reason we use <span class="math inline">\(\bar{Y}\)</span> for the null model is that <span class="math inline">\(\bar{Y}\)</span> is the unbiased estimator of <span class="math inline">\(\beta_0\)</span> for the null model, <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. Thus we would compute the following sums of squares: <span class="math display">\[
  SSTO = \sum(Y_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSR = \sum(\hat{Y}_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSE = \sum(Y_i-\hat{Y}_i)^2
\]</span> and note that <span class="math inline">\(SSTO = SSR + SSE\)</span>. Important to note is that <span class="math inline">\(SSTO\)</span> uses the difference between the observations <span class="math inline">\(Y_i\)</span> and the null (reduced) model. The <span class="math inline">\(SSR\)</span> uses the diffences between the alternative (full) and null (reduced) model. The <span class="math inline">\(SSE\)</span> uses the differences between the observations <span class="math inline">\(Y_i\)</span> and the alternative (full) model. From these we could set up a General <span class="math inline">\(F\)</span> table of the form</p>
<table style="width:60%;">
<colgroup>
<col width="12%" />
<col width="12%" />
<col width="6%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Sum Sq</th>
<th>Df</th>
<th>Mean Sq</th>
<th>F Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Error</td>
<td><span class="math inline">\(SSR\)</span></td>
<td><span class="math inline">\(df_R-df_F\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\cdot\frac{df_F}{SSE}\)</span></td>
</tr>
<tr class="even">
<td>Residual Error</td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(df_F\)</span></td>
<td><span class="math inline">\(\frac{SSE}{df_F}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total Error</td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(df_R\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<p><br /></p>
</div>
<div id="prediction-and-confidence-intervals-for-haty_h-expand" class="section level4">
<h4>Prediction and Confidence Intervals for <span class="math inline">\(\hat{Y}_h\)</span> <a href="javascript:showhide('predictionintervals')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">predict(…, interval=“prediction”)… </span></p>
<div id="predictionintervals" style="display:none;">
<p>It is a common mistake to assume that averages (means) describe individuals. They do not. So, when providing predictions on individuals, it is crucial to capture the variability of individuals around the line.</p>
<table style="width:69%;">
<colgroup>
<col width="15%" />
<col width="12%" />
<col width="22%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th>Interval</th>
<th>R Code</th>
<th>Math Equation</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prediction</td>
<td><span style="font-size:.8em;"><code>predict(..., interval=&quot;prediction&quot;)</code></span></td>
<td><span class="math inline">\(\hat{Y}_i \pm t^* \cdot s_{\text{Pred}\ Y}\)</span></td>
<td>Predict an individual’s value.</td>
</tr>
<tr class="even">
<td>Confidence</td>
<td><span style="font-size:.8em;"><code>predict(..., interval=&quot;confidence&quot;)</code></span></td>
<td><span class="math inline">\(\hat{Y}_i \pm t^* \cdot s_{\hat{Y}}\)</span></td>
<td>Estimate location of the mean y-value.</td>
</tr>
</tbody>
</table>
<p><code>predict(mylm, data.frame(XvarName = number), interval=...)</code></p>
<p><br /> <br /></p>
<p>For example, consider this graph. Then <a href="javascript:showhide('predictionintervalsgraph')" style="color:skyblue;">click here</a> to read about the graph.</p>
<div id="predictionintervalsgraph" style="padding-left:30px;padding-right:30px;font-size:.9em;display:none;">
<p>Notice the three dots above 15 mph in the graph. Each of these dots show a car that was going 15 mph when it applied the brakes. However, stopping distances of the three individual cars differ with one at 20 feet, one at 26 feet and one at 54 feet.</p>
<p>The regression line represents the average stopping distance of cars. In this case, cars going 15 mph are estimated to have an average stopping distance of about 40 feet, as shown by the line. But individual vehicles, all going the same speed of 15 mph, varied from stopping distances of 20 feet up to 54 feet!</p>
<p>So, to predict that a car going 15 mph will take 41.4 feet to stop, doesn’t tell the whole story. Far more revealing is the complete statement, “Cars going 15 mph are predicted to take anywhere from 10.2 to 72.6 feet to stop, with an average stopping distance of 41.4 feet.” This is called the “prediction interval” and is shown in the graph in blue. It is obtained in R with the codes:</p>
<p><code>cars.lm &lt;- lm(dist ~ speed, data=cars)</code></p>
<p><code>predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;)</code></p>
<pre class="r"><code>cars.lm &lt;- lm(dist ~ speed, data=cars)
pander(predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;))</code></pre>
<table style="width:33%;">
<colgroup>
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">fit</th>
<th align="center">lwr</th>
<th align="center">upr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">41.41</td>
<td align="center">10.17</td>
<td align="center">72.64</td>
</tr>
</tbody>
</table>
</div>
<pre class="r"><code>plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1,
     xlab=&quot;Speed of the Vehicle (mph) \n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;,
     main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;)
mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5)
legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;)
points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=&quot;firebrick2&quot;, cex=1.5)

cars.lm &lt;- lm(dist ~ speed, data=cars)
abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3))
abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2))
abline(v=15, lty=2, col=&quot;firebrick&quot;)

preds &lt;- predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;)
lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12)
lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8))
lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8))</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>Now, for the details behind prediction intervals and confidence intervals.</p>
<p>Let’s begin by recalling some details (from the section “Inference for the Model Parameters”) about the standard error of the y-intercept, <span class="math inline">\(b_0\)</span>. Recall that the y-intercept is the average y-value for the given x-value of <span class="math inline">\(x=0\)</span>. Recall further that the formula for the standard error of <span class="math inline">\(b_0\)</span> is given by the formula</p>
<p><span class="math display">\[
  s^2_{b_0} = MSE\left[\frac{1}{n} + \frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]
\]</span></p>
<p>If we wanted to be more exact with this formula, we would write it as</p>
<p><span class="math display">\[
  s^2_{b_0} = MSE\left[\frac{1}{n} + \frac{(0-\bar{X})^2}{\sum(X_i-\bar{X})^2}\right]
\]</span></p>
<p>Did you notice the addition of <span class="math inline">\((0 - \bar{X})^2\)</span> instead of just <span class="math inline">\(\bar{X}^2\)</span> in the numerator of the right-most part of the equation? This more complete statement obviously would reduce to just <span class="math inline">\(\bar{X}^2\)</span>, but that is only because <span class="math inline">\(X=0\)</span> when we are working with the y-intercept, <span class="math inline">\(b_0\)</span>. We could be working with other values of <span class="math inline">\(X\)</span> than just zero.</p>
<div class="note">
<p>Let’s take a quick detour and talk notation for a second. Typically, <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> are used to denote the x-value and y-value of points that are contained in our data set. When we want to reference a point that wasn’t within our original data set, we use the notation <span class="math inline">\(X_h\)</span> and <span class="math inline">\(Y_h\)</span>. (The letter h is close to i, but different from i, so why not. There is really no other reason to use h.) Thus, <span class="math inline">\(Y_h\)</span> is the y-value for the <span class="math inline">\(X_h\)</span> x-value, neither of which were included in our original regression of <span class="math inline">\(X_i\)</span>’s and <span class="math inline">\(Y_i\)</span>’s.</p>
</div>
<p>Now, back to the previous discussion. If <span class="math inline">\(X_h = 0\)</span>, then <span class="math inline">\(\hat{Y}_h\)</span> is the y-intercept, so <span class="math inline">\(\hat{Y}_h = b_0\)</span> when <span class="math inline">\(X_h=0\)</span>. So, we could write,</p>
<p><span class="math display">\[
  s^2_{\hat{Y}_h} = MSE\left[\frac{1}{n} + \frac{(X_h-\bar{X})^2}{\sum(X_i-\bar{X})^2}\right]
\]</span></p>
<p>Did you notice how the <span class="math inline">\(b_0\)</span> in <span class="math inline">\(s_{b_0}\)</span> was replaced with <span class="math inline">\(\hat{Y}_h\)</span> to get <span class="math inline">\(s_{\hat{Y}_h}\)</span> and the 0 in <span class="math inline">\((0 - \bar{X})^2\)</span> was replaced with <span class="math inline">\(X_h\)</span> to get <span class="math inline">\((X_h - \bar{X})^2\)</span>? Interesting. We now have a formula that would give us the standard error of <span class="math inline">\(\hat{Y}_h\)</span> for any <span class="math inline">\(X_h\)</span> value, not just <span class="math inline">\(X_h = 0\)</span>, or the y-intercept, <span class="math inline">\(b_0\)</span>. That is fantastic. It would look like this if plotted. Notice how the gray region is showing the standard error for each <span class="math inline">\(\hat{Y}_h\)</span> value? (It is technically showing the confidence interval for <span class="math inline">\(E\{Y_h\}\)</span> at every possible <span class="math inline">\(X_h\)</span> value, but that is just <span class="math inline">\(\hat{Y}_h \pm t^* \cdot s_{\hat{Y}_h}\)</span>.)</p>
<pre class="r"><code>ggplot(cars, aes(x=speed, y=dist)) + 
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, color=&quot;skyblue&quot;) +
  theme_bw()</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p><strong>Confidence Interval for <span class="math inline">\(\hat{Y}_h\)</span></strong></p>
<p><span class="math display">\[
  \hat{Y}_h \pm t^* s_{\hat{Y}_h} \quad \text{where} \ s_{\hat{Y}_h}^2 = MSE\left[\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum(X_i - \bar{X})^2}\right]
\]</span></p>
<p>The confidence interval is a wonderful tool for estimating <span class="math inline">\(E\{Y_h\}\)</span>, the “true” average y-value for a given x-value of <span class="math inline">\(X_h\)</span>. However, it is not valuable for predicting an individual dot, or <span class="math inline">\(Y_h\)</span> value. Notice how few of the dots of the regression are actually contained within the confidence interval band in the plot? The confidence interval does not really predict where the dots will land, just where the average y-value is located for each x-value.</p>
<p>Remember the 68-95-99.7 Rule of the normal distribution? If not, here is a link back to that concept in the <a href="https://byuistats.github.io/BYUI_M221_Book/Lesson05.html#normal-probability-computations">Math 221</a> textbook. This rule states that roughly 95% of data, when normally distributed, will be between <span class="math inline">\(z=-2\)</span> and <span class="math inline">\(z=2\)</span> standard deviations from the mean. So, is going two “residual standard errors” to both sides of the regression line enough to capture 95% of the data? The answer is, not quite. The reason for this is because our knowledge of where the true mean lies is uncertain. (Notice the confidence interval band shown in the plot.) However, adding two standard errors to the edges of the confidence band would get us in the right place. In other words, there are two sources of variability at play here, (1) our uncertaintity in where the regression line is sitting, and (2) the natural variability of the data points around the line. Thus, the “prediction interval” requires accounting for both of these sources of variability to produce the following equation.</p>
<p><strong>Prediction Interval for <span class="math inline">\(Y_h\)</span></strong></p>
<p><span class="math display">\[
  \hat{Y}_h \pm t^* s_{Pred \hat{Y}_h} \quad \text{where} \ s_{Pred \hat{Y}_h}^2 = MSE\left[\frac{1}{n} + 1 + \frac{(X_h - \bar{X})^2}{\sum(X_i - \bar{X})^2}\right]
\]</span></p>
<p>This formula provides a useful band for identifying a region where we are 95% confident that a new observation for <span class="math inline">\(Y_h\)</span> will land, given the value of <span class="math inline">\(X_h\)</span>.</p>
<p>It looks as follows. Notice the prediction interval is much wider than the confidence interval. This is because data varies far more than do means. Prediction is for where the individual data points will land, confidence is for where the mean will land.</p>
<pre class="r"><code>cars.lm &lt;- lm(dist ~ speed, data=cars)
predy &lt;- predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;)

ggplot(cars, aes(x=speed, y=dist)) + 
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, color=&quot;skyblue&quot;) +
  geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + 
  geom_point(aes(x=15, y=predy[1]), cex=2, color=&quot;skyblue&quot;, pch=15) +
  theme_bw()</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
<p><br/></p>
</div>
<div id="lowess-and-loess-curves-expand" class="section level4">
<h4>Lowess (and Loess) Curves <a href="javascript:showhide('lowesscurves')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">A non-parametric approach to estimating <span class="math inline">\(E\{Y_i\}\)</span>… </span></p>
<div id="lowesscurves" style="display:none;">
<p>Robust <strong>lo</strong>cally <strong>wei</strong>ghted regression and <strong>s</strong>moothing <strong>s</strong>catterplots (LOWESS), is an effective way to visually model the average y-value.</p>
<hr />
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>air2 &lt;- na.omit(select(airquality, Temp, Ozone))

# Just quickly draw the lowess curve:
plot(Temp ~ Ozone, data=air2, pch=16, col=&quot;darkgray&quot;)
lines(lowess(air2$Ozone, air2$Temp), col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<pre class="r"><code>## OR optionally, 
## allow for predictions as well as the graph:
# plot(Temp ~ Ozone, data=air2, pch=16, col=&quot;darkgray&quot;)
# air2 &lt;- arrange(air2, desc(Ozone))
# mylo &lt;- loess(Temp ~ Ozone, data=air2, degree=1)
# lines(mylo$fit ~ Ozone, data=air2)</code></pre>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>air2 &lt;- na.omit(select(airquality, Temp, Ozone))

# Just quickly draw the lowess curve:
ggplot(air2, aes(x=Ozone, y=Temp)) +
  geom_point(color=&quot;darkgray&quot;) + 
  geom_smooth(se=F, method=&quot;loess&quot;, method.args = list(degree=1)) + #Note, degree=2 by default.
  theme_bw()</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<pre class="r"><code>## OR optionally, 
## allow for predictions as well as the graph:
# air2 &lt;- arrange(air2, desc(Ozone))
# mylo &lt;- loess(Temp ~ Ozone, data=air2, degree=1)
# ggplot(air2, aes(x=Ozone, y=Temp)) +
#   geom_point() +
#   geom_line(data=air2, aes(y=mylo$fit, x=Ozone))</code></pre>
</td>
</tr>
</table>
<hr />
<p><br /></p>
<table style="width:97%;">
<colgroup>
<col width="48%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Advantages</strong></th>
<th><strong>Disadvantages</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Quick. Good at ignoring outliers. Good at capturing the general pattern in the data. Good for making predictions within the scope of the data.</td>
<td>No mathematical model. Not interpretable. No p-values. No adjusted R-squared.</td>
</tr>
</tbody>
</table>
<p><strong>How it Works</strong></p>
<p>The Lowess curve localizes the regression model to a “neighborhood” of points, and then joins these localized regressions together into a smooth line. It minimizes the effect of outliers, and let’s the data “speak for itself”.</p>
<p>As a downside, it is not interpretable, and has no final way to write the model mathematically. All the same, it is a very powerful tool for identifying an appropriate model, or verifying the fit of a model, or making predictions when no reasonable model does an adequate job.</p>
<p>Study this graphic and the explanations below to learn how it works.</p>
<p><em>Recommendation: run the code in this “Code” chunk to the right in your Console, and flip through the resulting graphics.</em></p>
<pre class="r"><code>X &lt;- cars$speed
Y &lt;- cars$dist
X &lt;- X[!is.na(X) &amp; !is.na(Y)]
Y &lt;- Y[!is.na(X) &amp; !is.na(Y)]
f &lt;- 1/2
n &lt;- length(X)

lfit &lt;- rep(NA,n)
for (xh in 1:n){
 xdists &lt;- X - X[xh]
 nn &lt;- floor(n*f)
 r &lt;- sort(abs(xdists))[nn]
 xdists.nbrhd &lt;- which(abs(xdists) &lt; r)
 w &lt;- rep(0, length(xdists))
 w[xdists.nbrhd] &lt;- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3
 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w),   
      col=rgb(.2,.2,.2,.3), cex=1.5, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;&quot;, ylab=&quot;&quot;)
 points(Y[xh] ~ X[xh], pch=16, col=&quot;orange&quot;)
 lmc &lt;- lm(Y ~ X, weights=w)
 curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;orange&quot;, add=TRUE)
 lines(lfit[1:xh] ~ X[1:xh], col=&quot;gray&quot;)
 
 #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2))
 cat(&quot;\n\n&quot;)
 readline(prompt=paste0(&quot;Center point is point #&quot;, xh, &quot;... Press [enter] to continue...&quot;))
 

 MADnotThereYet &lt;- TRUE
 count &lt;- 0
 while(MADnotThereYet){
   
      readline(prompt=paste0(&quot;\n   Adjusting line to account for outliers in the y-direction... Press [enter] to continue...&quot;))   
   
   curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;wheat&quot;, add=TRUE)

   MAD &lt;- median(abs(lmc$res))
   resm &lt;- lmc$res/(6*MAD)
   resm[resm&gt;1] &lt;- 1
   bisq &lt;- (1-resm^2)^2
   w &lt;- w*bisq
   obs &lt;- coef(lmc)
   lmc &lt;- lm(Y ~ X, weights=w)
 
   curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;orange&quot;, add=TRUE)

   count &lt;- count + 1
   if ( (sum(abs(obs-lmc$coef))&lt;.1) | (count &gt; 3))
     MADnotThereYet &lt;- FALSE
       
 }

   curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;green&quot;, add=TRUE)
   points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=&quot;green&quot;)
   

  readline(prompt=paste0(&quot;\n   Use final line to get fitted value for this point... Press [enter] to continue to next point...&quot;))
 
 lfit[xh] &lt;- predict(lmc, data.frame(X=X[xh]))
 lines(lfit[1:xh] ~ X[1:xh], col=&quot;gray&quot;)
 

 if (xh == n){
     readline(prompt=paste0(&quot;\n  Press [enter] to see actual Lowess curve...&quot;))
    lines(lowess(X,Y, f=f), col=&quot;firebrick&quot;)
    legend(&quot;topleft&quot;, bty=&quot;n&quot;, legend=&quot;Actual lowess Curve using lowess(...)&quot;, col=&quot;firebrick&quot;, lty=1)
 }
  
  
}</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<ol style="list-style-type: decimal">
<li><p>Select a fraction of the data to use for the “neighborhood” of points (shown in blue in the graph above). The <code>lowess</code> function in R uses “f=2/3” and the <code>loess</code> function uses “span=0.75” for this value, which selects the nearest two-thirds or 75% of the data, respectively, depending on which function you use. For this example, we set the fraction of points at 50%. Both functions can be set to whatever you want.</p></li>
<li><p>Pick any point in the regression, eventually selecting all points one at a time. The selected point becomes the “center” of a “neighborhood” of points surrounding it. In this example, the center point is in orange, and the neighboring points are in blue.</p></li>
<li><p>Use the points within the neighborhood to fit a regression line. However, make the regression depend most on points closest to “center” and least on points furthest from “center.” This is called a weighted regression. Weights are decided according to what is called the tricubic weight function, so that the weight <span class="math inline">\(w\)</span> given to point <span class="math inline">\(j\)</span> of the neighborhood of points is defined by <span class="math display">\[
  w_j = \left(1- \left( \frac{|X_c - X_j|}{\max_k |X_c - X_k|}\right)^3\right)^3
\]</span> where <span class="math inline">\(X_c\)</span> is the x-value of the “center” dot and <span class="math inline">\(X_j\)</span> is the x-value of any other dot in the neighborhood.</p></li>
<li><p>The fitted-value of <span class="math inline">\(\hat{Y}_c\)</span> is obtained for the center point <span class="math inline">\(X_c\)</span> of the current regression. This point is used as the Lowess (or Loess) curve’s value at that particular x-value. Well, almost. It’s a first guess at where this value will end up, but there’s a little more to the algorithm before we are done. Initial guesses for each of these fitted values are obtained for each point in the regression.</p></li>
<li><p>Now each local regression for each neighborhood is re-run a few times in such a way the the effect of outliers is minimized. The final line for each neighborhood is obtained by the following steps.</p>
<ul>
<li>Compute all residuals for points in the neighborhood of the current regression, denoted by <span class="math inline">\(r_i\)</span>.</li>
<li>Then compute the MAD, median absolute deviation, of the residuals <span class="math inline">\(MAD = \text{median} (|r_1|, |r_2|, \ldots)\)</span>.</li>
<li>Divide all residuals by 6 times the MAD: <span class="math inline">\(u_i = r_i/(6\cdot MAD)\)</span> (If <span class="math inline">\(r_i &gt; 6\cdot MAD\)</span> then set <span class="math inline">\(u_i = 0\)</span>.)</li>
<li>Compute what are called bisquare weights using the formula: <span class="math inline">\(b_i = (1 - u_i^2)^2\)</span></li>
<li>Perform a regression using the weights <span class="math inline">\(w_i = w_i b_i\)</span></li>
<li>Repeat the above process with the new weights <span class="math inline">\(w_i\)</span> until the weights stop changing very much.</li>
</ul></li>
<li><p>The final fitted values for each <span class="math inline">\(X\)</span>-value in the regression are obtained from the final regression line for each neighborhood. These fitted values make up the Lowess (or loess) curve.</p></li>
</ol>
<p>Note that the default of the <code>loess</code> function in R is to use quadratic regressions in each neighborhood instead of linear regressions. This can be controlled with the <code>loess</code> option of “degree=2” (quadratic fits) or “degree = 1”. In the <code>lowess</code> function only a linear regression in each neighborhood is allowed.</p>
</div>
<p><br /></p>
<hr />
</div>
</div>
</div>
</div>
<div id="section" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a href="./Analyses/Linear%20Regression/Examples/BodyWeightSLR.html">bodyweight</a>, <a href="./Analyses/Linear%20Regression/Examples/carsSLR.html">cars</a></p>
</div>
<hr />
</div>
<div id="multiple-linear-regression" class="section level2 tabset tabset-fade tabset-pills">
<h2>Multiple Linear Regression</h2>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYMultX.png" width=108px;></p>
</div>
<p>Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable <span class="math inline">\(Y_i\)</span>.</p>
<div id="overview-1" class="section level3">
<h3>Overview</h3>
<div style="padding-left:125px;">
<p>The multiple linear regression model is given by combining (1) a linear model for the mean y-value, denoted by <span class="math inline">\(E\{Y_i\}\)</span>, and (2) a normally distributed error term, <span class="math inline">\(\epsilon_i\)</span>.</p>
<p><span class="math display">\[
  Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_{p-1} X_{p-1,i}}_{E\{Y_i\}}}^\text{&quot;The Model&quot;} + \epsilon_i 
\]</span></p>
<p>where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>.</p>
<hr />
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'LearnMoresimpleLinearModel')">
Simple
</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreQuadraticModel')">
Quadratic
</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreCubicModel')">
Cubic
</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreTwoLinesModel')">
Two-Lines
</button>
<button class="tablinks" onclick="openTab(event, 'LearnMorethreeDModel')">
3D
</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreHDModel')">
HD
</button>
</div>
<div id="LearnMoresimpleLinearModel" class="tabcontent" style="display:block;">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-44-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}}}^\text{Simple Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p><br/></p>
<p>The Simple Linear Regression model uses a single x-variable once: <span class="math inline">\(X_i\)</span>.</p>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Slope of the line</td>
</tr>
</tbody>
</table>
</p>
</div>
<div id="LearnMoreQuadraticModel" class="tabcontent">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-45-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i + \beta_2 X_i^2}_{E\{Y_i\}}}^\text{Quadratic Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p><br/></p>
<p>The Quadratic model uses the same <span class="math inline">\(X\)</span>-variable twice, once with a <span class="math inline">\(\beta_1 X_i\)</span> term and once with a <span class="math inline">\(\beta_2 X_i^2\)</span> term. The <span class="math inline">\(X_i^2\)</span> term is called the “quadratic” term.</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Controls the x-position of the vertex of the parabola by <span class="math inline">\(\frac{-\beta_1}{2\cdot\beta_2}\)</span>.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see <span class="math inline">\(\beta_1\)</span>’s explanation.</td>
</tr>
</tbody>
</table>
<p><span style="padding-left:15px;"><a href="javascript:showhide('quadraticmodelelexample')" style="font-size:1.1em;color:skyblue;">(Show Example…)</a></span></p>
<div id="quadraticmodelelexample" style="display:none;">
<p><strong>An Example</strong></p>
<p>Using the <code>airquality</code> data set, we run the following “quadratic” regression.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Temp} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\text{y-int}} + \overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{term}}} \underbrace{X_{i}}_\text{Month} + \overbrace{\beta_2}^{\stackrel{\text{quadratic}}{\text{term}}}  \underbrace{X_{i}^2}_\text{I(Month^2)} + \epsilon_i
\]</span></p>
<a href="javascript:showhide('quadraticregressionexamplecode')">
<div class="hoverchunk">
<p><span class="tooltipr"> lm.quad &lt;- <span class="tooltiprtext">A name we made up for our “quadratic” regression.</span> </span><span class="tooltipr"> lm( <span class="tooltiprtext">R function lm used to perform linear regressions in R. The lm stands for “linear model”.</span> </span><span class="tooltipr"> Temp <span class="tooltiprtext">Y-variable, should be quantitative.</span> </span><span class="tooltipr">  ~  <span class="tooltiprtext">The tilde <code>~</code> is what lm(…) uses to state the regression equation <span class="math inline">\(Y_i = ...\)</span>. Notice that the <code>~</code> is not followed by <span class="math inline">\(\beta_0 + \beta_1\)</span> like <span class="math inline">\(Y_i = ...\)</span>. Instead, <span class="math inline">\(X_{1i}\)</span> is the first term following <code>~</code>. This is because the <span class="math inline">\(\beta\)</span>’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject).</span> </span><span class="tooltipr"> Month <span class="tooltiprtext"><span class="math inline">\(X_{i}\)</span>, should be quantitative.</span> </span><span class="tooltipr">  +  <span class="tooltiprtext">The plus <code>+</code> is used between each term in the model. Note that only the x-variables are included in the lm(…) from the <span class="math inline">\(Y_i = ...\)</span> model. No beta’s are included.</span> </span><span class="tooltipr"> I(Month^2) <span class="tooltiprtext"><span class="math inline">\(X_{i}^2\)</span>, where the function I(…) protects the squaring of Month from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.</span> </span><span class="tooltipr"> , data=airquality <span class="tooltiprtext">This is the data set we are using for the regression.</span> </span><span class="tooltipr"> )<br />
<span class="tooltiprtext">Closing parenthsis for the lm(…) function.</span> </span><span class="tooltipr">     <br />
<span class="tooltiprtext">Press Enter to run the code.</span> </span><span class="tooltipr" style="float:right;">  …  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
</a>
<div id="quadraticregressionexamplecode" style="display:none;">
<p>Pay special attention to how the lm(…) code uses <span class="math inline">\(Y_i \sim X_{i} + X_{i}^2\)</span> and drops all <span class="math inline">\(\beta\)</span>’s and <span class="math inline">\(\epsilon\)</span> from the model statement. This is because the estimates for the <span class="math inline">\(\beta\)</span>’s and <span class="math inline">\(\epsilon\)</span> are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in <code>lmObject$residuals</code>.</p>
</div>
<pre class="r"><code>lm.quad &lt;- lm(Temp ~ Month + I(Month^2), data=airquality)
pander(summary(lm.quad)$coefficients)</code></pre>
<table style="width:88%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">-95.73</td>
<td align="center">15.24</td>
<td align="center">-6.281</td>
<td align="center">3.458e-09</td>
</tr>
<tr class="even">
<td align="center"><strong>Month</strong></td>
<td align="center">48.72</td>
<td align="center">4.489</td>
<td align="center">10.85</td>
<td align="center">1.29e-20</td>
</tr>
<tr class="odd">
<td align="center"><strong>I(Month^2)</strong></td>
<td align="center">-3.283</td>
<td align="center">0.3199</td>
<td align="center">-10.26</td>
<td align="center">4.737e-19</td>
</tr>
</tbody>
</table>
<p>The <strong>estimates</strong> shown above approximate the <span class="math inline">\(\beta\)</span>’s in the regression model: <span class="math inline">\(\beta_0\)</span> is estimated by the (Intercept) value of -95.73, <span class="math inline">\(\beta_1\)</span> is estimated by the <code>Month</code> value of 48.72, and <span class="math inline">\(\beta_2\)</span> is estimated by the <code>I(Month^2)</code> value of -3.283.</p>
<p>Because the estimate of the <span class="math inline">\(\beta_2\)</span> term is negative, this parabola will “open down”. The vertex of this parabola will be at <span class="math inline">\(-(48.72)/(2\cdot (-3.283)) = 7.420043\)</span>, and the y-intercept is -95.73.</p>
<p><span class="math display">\[
\hat{Y}_i = \overbrace{-95.73}^\text{y-int} + \overbrace{48.72}^{\stackrel{\text{slope}}{\text{term}}} X_{i} + \overbrace{-3.283}^{\stackrel{\text{quadratic}}{\text{term}}} X_{i}^2
\]</span></p>
<p>The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above.</p>
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>plot(Temp ~ Month, data=airquality, col=&quot;skyblue&quot;, pch=21, bg=&quot;gray83&quot;, main=&quot;Quadratic Model using airquality data set&quot;, cex.main=1)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.quad)
# Then b will have 3 estimates:
# b[1] is the estimate of beta_0: 35.38
# b[2] is the estimate of beta_1: -7.099
# b[3] is the estimate of beta_2: 0.4759
curve(b[1] + b[2]*x + b[3]*x^2, col=&quot;skyblue&quot;, lwd=2, add=TRUE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.quad)
# Then b will have 3 estimates:
# b[1] is the estimate of beta_0: 35.38
# b[2] is the estimate of beta_1: -7.099
# b[3] is the estimate of beta_2: 0.4759

ggplot(airquality, aes(y=Temp, x=Month)) +
  geom_point(pch=21, bg=&quot;gray83&quot;, color=&quot;skyblue&quot;) +
  #geom_smooth(method=&quot;lm&quot;, se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=&quot;skyblue&quot;) +
  labs(title=&quot;Quadratic Model using airquality data set&quot;) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
</td>
</tr>
</table>
</div>
</p>
</div>
<div id="LearnMoreCubicModel" class="tabcontent" style="display:none;">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-49-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \beta_3 X_i^3}_{E\{Y_i\}}}^\text{Cubic Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p><br/></p>
<p>The Cubic model uses the same <span class="math inline">\(X\)</span>-variable thrice, once with a <span class="math inline">\(\beta_1 X_i\)</span> term, once with a <span class="math inline">\(\beta_2 X_i^2\)</span> term, and once with a <span class="math inline">\(\beta_3 X_i^3\)</span> term. The <span class="math inline">\(X_i^3\)</span> term is called the “cubic” term.</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>No clear interpretation, but could be called the “base slope coefficient” and contributes to the position of the inflection points of the cubic function.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>No clear interpretation, but it also contributes to the location of the inflection points.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_3\)</span></td>
<td>This is the coefficient of the cubic term. No clear interpretation, but it determines the concavity of the model by its sign.</td>
</tr>
</tbody>
</table>
<p><span style="padding-left:15px;"><a href="javascript:showhide('cubicmodelelexample')" style="font-size:1.1em;color:skyblue;">(Show Example…)</a></span></p>
<div id="cubicmodelelexample" style="display:none;">
<p><strong>An Example</strong></p>
<p>Using the <code>CO2</code> data set, we run the following “cubic” regression.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{uptake} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\text{y-int}} + \overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{term}}} \underbrace{X_{i}}_\text{conc} + \overbrace{\beta_2}^{\stackrel{\text{quadratic}}{\text{term}}}  \underbrace{X_{i}^2}_\text{I(conc^2)} +  \overbrace{\beta_3}^{\stackrel{\text{cubic}}{\text{term}}}  \underbrace{X_{i}^3}_\text{I(conc^3)} + \epsilon_i
\]</span></p>
<a href="javascript:showhide('cubicregressionexamplecode')">
<div class="hoverchunk">
<p><span class="tooltipr"> lm.cubic &lt;- <span class="tooltiprtext">A name we made up for our “cubic” regression.</span> </span><span class="tooltipr"> lm( <span class="tooltiprtext">R function lm used to perform linear regressions in R. The lm stands for “linear model”.</span> </span><span class="tooltipr"> uptake <span class="tooltiprtext">Y-variable, should be quantitative.</span> </span><span class="tooltipr">  ~  <span class="tooltiprtext">The tilde <code>~</code> is what lm(…) uses to state the regression equation <span class="math inline">\(Y_i = ...\)</span>. Notice that the <code>~</code> is not followed by <span class="math inline">\(\beta_0 + \beta_1\)</span> like <span class="math inline">\(Y_i = ...\)</span>. Instead, <span class="math inline">\(X_i\)</span> is the first term following <code>~</code>. This is because the <span class="math inline">\(\beta\)</span>’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject).</span> </span><span class="tooltipr"> conc <span class="tooltiprtext"><span class="math inline">\(X_{i}\)</span>, should be quantitative.</span> </span><span class="tooltipr">  +  <span class="tooltiprtext">The plus <code>+</code> is used between each term in the model. Note that only the x-variables are included in the lm(…) from the <span class="math inline">\(Y_i = ...\)</span> model. No beta’s are included.</span> </span><span class="tooltipr"> I(conc^2) <span class="tooltiprtext"><span class="math inline">\(X_{i}^2\)</span>, where the function I(…) protects the squaring of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.</span> </span><span class="tooltipr">  +  <span class="tooltiprtext">The plus <code>+</code> is used between each term in the model. Note that only the x-variables are included in the lm(…) from the <span class="math inline">\(Y_i = ...\)</span> model. No beta’s are included.</span> </span><span class="tooltipr"> I(conc^3) <span class="tooltiprtext"><span class="math inline">\(X_{i}^3\)</span>, where the function I(…) protects the cubing of conc from how lm(…) would otherwise interpret that statement. The I(…) function must be used anytime you raise an x-variable to a power in the lm(…) statement.</span> </span><span class="tooltipr"> , data=airquality <span class="tooltiprtext">This is the data set we are using for the regression.</span> </span><span class="tooltipr"> )<br />
<span class="tooltiprtext">Closing parenthsis for the lm(…) function.</span> </span><span class="tooltipr">     <br />
<span class="tooltiprtext">Press Enter to run the code.</span> </span><span class="tooltipr" style="float:right;">  …  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
</a>
<div id="cubicregressionexamplecode" style="display:none;">
<p>Pay special attention to how the lm(…) code uses <span class="math inline">\(Y_i \sim X_{i} + X_{i}^2\)</span> and drops all <span class="math inline">\(\beta\)</span>’s and <span class="math inline">\(\epsilon\)</span> from the model statement. This is because the estimates for the <span class="math inline">\(\beta\)</span>’s and <span class="math inline">\(\epsilon\)</span> are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in <code>lmObject$residuals</code>.</p>
</div>
<pre class="r"><code>lm.cubic &lt;- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2)
pander(summary(lm.cubic)$coefficients)</code></pre>
<table style="width:89%;">
<colgroup>
<col width="25%" />
<col width="18%" />
<col width="18%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">-1.483</td>
<td align="center">5.043</td>
<td align="center">-0.2941</td>
<td align="center">0.7694</td>
</tr>
<tr class="even">
<td align="center"><strong>conc</strong></td>
<td align="center">0.1814</td>
<td align="center">0.0416</td>
<td align="center">4.36</td>
<td align="center">3.83e-05</td>
</tr>
<tr class="odd">
<td align="center"><strong>I(conc^2)</strong></td>
<td align="center">-0.0003063</td>
<td align="center">9.067e-05</td>
<td align="center">-3.378</td>
<td align="center">0.00113</td>
</tr>
<tr class="even">
<td align="center"><strong>I(conc^3)</strong></td>
<td align="center">1.601e-07</td>
<td align="center">5.512e-08</td>
<td align="center">2.905</td>
<td align="center">0.004745</td>
</tr>
</tbody>
</table>
<p>The <strong>estimates</strong> shown above approximate the <span class="math inline">\(\beta\)</span>’s in the regression model: <span class="math inline">\(\beta_0\)</span> is estimated by the (Intercept) value of -1.483, <span class="math inline">\(\beta_1\)</span> is estimated by the <code>conc</code> value of 0.1814, <span class="math inline">\(\beta_2\)</span> is estimated by the <code>I(conc^2)</code> value of -0.0003063, and <span class="math inline">\(\beta_3\)</span> is estimated by the <code>I(conc^3)</code> value of 1.601e-07, which translates to 0.0000001601.</p>
<p>Because the estimate of the <span class="math inline">\(\beta_3\)</span> term is positive, this cubic model will “open up”. In other words, as the function moves from left to right, it will go off to positive infinity (up). If the term would have been negative, then the function would head to negative infinity (down) instead.</p>
<p><span class="math display">\[
\hat{Y}_i = \overbrace{-1.483}^\text{y-int} + \overbrace{0.1814}^{\stackrel{\text{slope}}{\text{term}}} X_{i} + \overbrace{-0.0003063}^{\stackrel{\text{quadratic}}{\text{term}}} X_{i}^2 + \overbrace{1.601e-07}^{\stackrel{\text{cubic}}{\text{term}}} X_{i}^3
\]</span></p>
<p>The regression function is drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the equation above.</p>
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>plot(uptake ~ conc, data=CO2, col=&quot;skyblue&quot;, pch=21, bg=&quot;gray83&quot;, main=&quot;Cubic Model using CO2 data set&quot;, cex.main=1)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.cubic)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -1.483
# b[2] is the estimate of beta_1: 0.1814
# b[3] is the estimate of beta_2: -0.0003063
# b[4] is the estimate of beta_3: 1.601e-07
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=&quot;skyblue&quot;, lwd=2, add=TRUE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.cubic)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -1.483
# b[2] is the estimate of beta_1: 0.1814
# b[3] is the estimate of beta_2: -0.0003063
# b[4] is the estimate of beta_3: 1.601e-07

ggplot(CO2, aes(y=uptake, x=conc)) +
  geom_point(pch=21, bg=&quot;gray83&quot;, color=&quot;skyblue&quot;) +
  #geom_smooth(method=&quot;lm&quot;, se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=&quot;skyblue&quot;) +
  labs(title=&quot;Cubic Model using CO2 data set&quot;) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
</td>
</tr>
</table>
<p>It should be stated, that the cubic function is not the best fit for this data. However, it is a lot better than just a simple line, or a quadratic model, as shown below.</p>
<pre class="r"><code>plot(uptake ~ conc, data=CO2, col=&quot;skyblue&quot;, pch=21, bg=&quot;gray83&quot;, main=&quot;Cubic Model using CO2 data set&quot;, cex.main=1)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.cubic)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -1.483
# b[2] is the estimate of beta_1: 0.1814
# b[3] is the estimate of beta_2: -0.0003063
# b[4] is the estimate of beta_3: 1.601e-07
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=&quot;skyblue&quot;, lwd=2, add=TRUE)
b &lt;- coef(lm(uptake ~ conc + I(conc^2), data=CO2))
curve(b[1] + b[2]*x + b[3]*x^2, col=&quot;firebrick&quot;, lwd=2, add=TRUE)
b &lt;- coef(lm(uptake ~ conc, data=CO2))
curve(b[1] + b[2]*x, col=&quot;orange&quot;, lwd=2, add=TRUE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
</div>
</p>
</div>
<div id="LearnMoreTwoLinesModel" class="tabcontent">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-54-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{1i} X_{2i}}_{E\{Y_i\}}}^\text{Two-lines Model} + \epsilon_i 
\]</span></p>
<p><span class="math display">\[
 X_{2i} = \left\{\begin{array}{ll} 1, &amp; \text{Group B} \\ 0, &amp; \text{Group A} \end{array}\right.
\]</span></p>
</td>
</tr>
</table>
<p>The so called “two-lines” model uses a quantitative <span class="math inline">\(X_{1i}\)</span> variable and a 0,1 indicator variable <span class="math inline">\(X_{2i}\)</span>. It is a basic example of how a “dummy variable” or “indicator variable” can be used to turn qualitative variables into quantitative terms. In this case, the indicator variable <span class="math inline">\(X_{2i}\)</span>, which is either 0 or 1, produces two separate lines: one line for Group A, and one line for Group B.</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Controls the slope of the “base-line” of the model, the “Group 0” line.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Controls the <strong>change in y-intercept</strong> for the second line in the model as compared to the y-intercept of the “base-line” line.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_3\)</span></td>
<td>Called the “interaction” term. Controls the <strong>change in the slope</strong> for the second line in the model as compared to the slope of the “base-line” line.</td>
</tr>
</tbody>
</table>
<p><span style="padding-left:15px;"><a href="javascript:showhide('twolinesmodelexample')" style="font-size:1.1em;color:skyblue;">(Show Example…)</a></span></p>
<div id="twolinesmodelexample" style="display:none;">
<p><strong>An Example</strong></p>
<p>Using the <code>mtcars</code> data set, we run the following “two-lines” regression. Note that <code>am</code> has only 0 or 1 values: <code>View(mtcars)</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{mpg} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} + \overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}} \underbrace{X_{1i}}_\text{qsec} + \overbrace{\beta_2}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{am} + \overbrace{\beta_3}^{\stackrel{\text{change in}}{\text{slope}}} \underbrace{X_{1i}X_{2i}}_\text{qsec:am} + \epsilon_i
\]</span></p>
<a href="javascript:showhide('twolinesregressionexamplecode')">
<div class="hoverchunk">
<p><span class="tooltipr"> lm.2lines &lt;- <span class="tooltiprtext">A name we made up for our “two-lines” regression.</span> </span><span class="tooltipr"> lm( <span class="tooltiprtext">R function lm used to perform linear regressions in R. The lm stands for “linear model”.</span> </span><span class="tooltipr"> mpg <span class="tooltiprtext">Y-variable, should be quantitative.</span> </span><span class="tooltipr">  ~  <span class="tooltiprtext">The tilde <code>~</code> is what lm(…) uses to state the regression equation <span class="math inline">\(Y_i = ...\)</span>. Notice that the <code>~</code> is not followed by <span class="math inline">\(\beta_0 + \beta_1\)</span> like <span class="math inline">\(Y_i = ...\)</span>. Instead, <span class="math inline">\(X_{1i}\)</span> is the first term following <code>~</code>. This is because <span class="math inline">\(\beta\)</span>’s are going to be estimated by the lm(…). These estimates can be found using summary(lmObject).</span> </span><span class="tooltipr"> qsec <span class="tooltiprtext"><span class="math inline">\(X_{1i}\)</span>, should be quantitative.</span> </span><span class="tooltipr">  +  <span class="tooltiprtext">The plus <code>+</code> is used between each term in the model. Note that only the x-variables are included in the lm(…) from the <span class="math inline">\(Y_i = ...\)</span> model. No beta’s are included.</span> </span><span class="tooltipr"> am <span class="tooltiprtext"><span class="math inline">\(X_{2i}\)</span>, an indicator or 0,1 variable. This term allows the y-intercept of the two lines to differ.</span> </span><span class="tooltipr">  +  <span class="tooltiprtext">The plus <code>+</code> is used between each term in the model. Note that only the x-variables are included in the lm(…) from the <span class="math inline">\(Y_i = ...\)</span> model. No beta’s are included.</span> </span><span class="tooltipr"> qsec:am <span class="tooltiprtext"><span class="math inline">\(X_{1i}X_{2i}\)</span> the interaction term. This allows the slopes of the two lines to differ.</span> </span><span class="tooltipr"> , data=mtcars <span class="tooltiprtext">This is the data set we are using for the regression.</span> </span><span class="tooltipr"> )<br />
<span class="tooltiprtext">Closing parenthsis for the lm(…) function.</span> </span><span class="tooltipr">     <br />
<span class="tooltiprtext">Press Enter to run the code.</span> </span><span class="tooltipr" style="float:right;">  …  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
</a>
<div id="twolinesregressionexamplecode" style="display:none;">
<p>Pay special attention to how the lm(…) code uses <span class="math inline">\(Y_i \sim X_{1i} + X_{2i} + X_{1i}X_{2i}\)</span> and drops all <span class="math inline">\(\beta\)</span>’s and <span class="math inline">\(\epsilon\)</span> from the model statement. This is because the estimates for the <span class="math inline">\(\beta\)</span>’s and <span class="math inline">\(\epsilon\)</span> are given by the output of the lm(…) funtion in the “Estimates” column of summary(….) and in <code>lm.2lines$residuals</code>.</p>
</div>
<pre class="r"><code>lm.2lines &lt;- lm(mpg ~ qsec + am + qsec:am, data=mtcars)
pander(summary(lm.2lines)$coefficients)</code></pre>
<table style="width:86%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">-9.01</td>
<td align="center">8.218</td>
<td align="center">-1.096</td>
<td align="center">0.2823</td>
</tr>
<tr class="even">
<td align="center"><strong>qsec</strong></td>
<td align="center">1.439</td>
<td align="center">0.45</td>
<td align="center">3.197</td>
<td align="center">0.003432</td>
</tr>
<tr class="odd">
<td align="center"><strong>am</strong></td>
<td align="center">-14.51</td>
<td align="center">12.48</td>
<td align="center">-1.163</td>
<td align="center">0.2548</td>
</tr>
<tr class="even">
<td align="center"><strong>qsec:am</strong></td>
<td align="center">1.321</td>
<td align="center">0.7017</td>
<td align="center">1.883</td>
<td align="center">0.07012</td>
</tr>
</tbody>
</table>
<p>The <strong>estimates</strong> shown above approximate the <span class="math inline">\(\beta\)</span>’s in the regression model: <span class="math inline">\(\beta_0\)</span> is estimated by the (Intercept), <span class="math inline">\(\beta_1\)</span> is estimated by the <code>qsec</code> value of 1.439, <span class="math inline">\(\beta_2\)</span> is estimated by the <code>am</code> value of -14.51, and <span class="math inline">\(\beta_3\)</span> is estimated by the <code>qsec:am</code> value of 1.321.</p>
<p>This gives two separate equations of lines.</p>
<p><strong>Automatic Transmission (am==0, <span class="math inline">\(X_{2i} = 0\)</span>) Line</strong></p>
<p><span class="math display">\[
\hat{Y}_i = \overbrace{-9.01}^{\stackrel{\text{y-int}}{\text{baseline}}} + \overbrace{1.439}^{\stackrel{\text{slope}}{\text{baseline}}} X_{1i}
\]</span></p>
<p><strong>Manual Transmission (am==1 , <span class="math inline">\(X_{2i} = 1\)</span>) Line</strong></p>
<p><span class="math display">\[
\hat{Y}_i = \underbrace{(\overbrace{-9.01}^{\stackrel{\text{y-int}}{\text{baseline}}} + \overbrace{-14.51}^{\stackrel{\text{change in}}{\text{y-int}}})}_{\stackrel{\text{y-intercept}}{-23.52}} + \underbrace{(\overbrace{1.439}^{\stackrel{\text{slope}}{\text{baseline}}} +\overbrace{1.321}^{\stackrel{\text{change in}}{\text{slope}}})}_{\stackrel{\text{slope}}{2.76}} X_{1i}
\]</span></p>
<p>These lines are drawn as follows. Be sure to look at the “Code” to understand how this graph was created using the ideas in the two equations above.</p>
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>plot(mpg ~ qsec, data=mtcars, col=c(&quot;skyblue&quot;,&quot;orange&quot;)[as.factor(am)], pch=21, bg=&quot;gray83&quot;, main=&quot;Two-lines Model using mtcars data set&quot;, cex.main=1)

legend(&quot;topleft&quot;, legend=c(&quot;Baseline (am==0)&quot;, &quot;Changed-line (am==1)&quot;), bty=&quot;n&quot;, lty=1, col=c(&quot;skyblue&quot;,&quot;orange&quot;), cex=0.8)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.2lines)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -9.0099
# b[2] is the estimate of beta_1:  1.4385
# b[3] is the estimate of beta_2: -14.5107
# b[4] is the estimate of beta_3: 1.3214
curve(b[1] + b[2]*x, col=&quot;skyblue&quot;, lwd=2, add=TRUE)  #baseline (in blue)
curve((b[1] + b[3]) + (b[2] + b[4])*x, col=&quot;orange&quot;, lwd=2, add=TRUE) #changed line (in orange)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.2lines)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -9.0099
# b[2] is the estimate of beta_1:  1.4385
# b[3] is the estimate of beta_2: -14.5107
# b[4] is the estimate of beta_3: 1.3214

ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) +
  geom_point(pch=21, bg=&quot;gray83&quot;) +
  #geom_smooth(method=&quot;lm&quot;, se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x, color=&quot;skyblue&quot;) + #am==0 line
  stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=&quot;orange&quot;) + #am==1 line 
  scale_color_manual(name=&quot;Transmission (am)&quot;, values=c(&quot;skyblue&quot;,&quot;orange&quot;)) +
  labs(title=&quot;Two-lines Model using mtcars data set&quot;) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
</td>
</tr>
</table>
</div>
</p>
</div>
<div id="LearnMorethreeDModel" class="tabcontent">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/volcano-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
 Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{1i}X_{2i}}_{E\{Y_i\}}}^\text{3D Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p>The so called “3D” regression model uses two different quantitative x-variables, an <span class="math inline">\(X_{1i}\)</span> and an <span class="math inline">\(X_{2i}\)</span>. Unlike the two-lines model where <span class="math inline">\(X_{2i}\)</span> could only be a 0 or a 1, this <span class="math inline">\(X_{2i}\)</span> variable is quantitative, and can take on any quantitative value.</p>
<table style="width:100%;">
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_1\)</span> direction.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_2\)</span> direction.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_3\)</span></td>
<td>Interaction term that allows the model, which is a plane in three-dimensional space, to “bend”. If this term is zero, then the regression surface is just a flat plane.</td>
</tr>
</tbody>
</table>
<p><span style="padding-left:15px;"><a href="javascript:showhide('threedmodelexample')" style="font-size:1.1em;color:skyblue;">(Show Example…)</a></span></p>
<div id="threedmodelexample" style="display:none;">
<p><strong>An Example</strong></p>
<p>Here is what a 3D regression looks like when there is no interaction term. The two x-variables of <code>Month</code> and <code>Temp</code> are being used to predict the y-variable of <code>Ozone</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Ozone} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} + \overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}} \underbrace{X_{1i}}_\text{Temp} + \overbrace{\beta_2}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{Month} + \epsilon_i
\]</span></p>
<pre class="r"><code>air_lm &lt;- lm(Ozone ~ Temp + Month, data= airquality)
pander(air_lm$coefficients)</code></pre>
<table style="width:42%;">
<colgroup>
<col width="19%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">(Intercept)</th>
<th align="center">Temp</th>
<th align="center">Month</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-139.6</td>
<td align="center">2.659</td>
<td align="center">-3.522</td>
</tr>
</tbody>
</table>
<p>Notice how the slope, <span class="math inline">\(\beta_1\)</span>, in the “Temp” direction is estimated to be 2.659 and the slope in the “Month” direction, <span class="math inline">\(\beta_2\)</span>, is estimated to be -3.522. Also, the y-intercept, <span class="math inline">\(\beta_0\)</span>, is estimated to be -139.6.</p>
<pre class="r"><code>## Hint: library(car) has a scatterplot 3d function which is simple to use
#  but the code should only be run in your console, not knit.

## library(car)
## scatter3d(Y ~ X1 + X2, data=yourdata)



## To embed the 3d-scatterplot inside of your html document is harder.
#library(plotly)
#library(reshape2)

#Perform the multiple regression
air_lm &lt;- lm(Ozone ~ Temp + Month, data= airquality)

#Graph Resolution (more important for more complex shapes)
graph_reso &lt;- 0.5

#Setup Axis
axis_x &lt;- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso)
axis_y &lt;- seq(min(airquality$Month), max(airquality$Month), by = graph_reso)

#Sample points
air_surface &lt;- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F)
air_surface$Z &lt;- predict.lm(air_lm, newdata = air_surface)
air_surface &lt;- acast(air_surface, Month ~ Temp, value.var = &quot;Z&quot;) #y ~ x

#Create scatterplot
plot_ly(airquality, 
        x = ~Temp, 
        y = ~Month, 
        z = ~Ozone,
        text = rownames(airquality), 
        type = &quot;scatter3d&quot;, 
        mode = &quot;markers&quot;) %&gt;%
  add_trace(z = air_surface,
            x = axis_x,
            y = axis_y,
            type = &quot;surface&quot;)</code></pre>
<div id="htmlwidget-db3ca3e9190d5bc6beb2" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-db3ca3e9190d5bc6beb2">{"x":{"visdat":{"120d723f53c2":["function () ","plotlyVisDat"]},"cur_data":"120d723f53c2","attrs":{"120d723f53c2":{"x":{},"y":{},"z":{},"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"},"120d723f53c2.1":{"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-8.29301563460328,-6.96328021008852,-5.63354478557377,-4.30380936105902,-2.9740739365443,-1.64433851202955,-0.314603087514794,1.01513233699996,2.34486776151471,3.67460318602946,5.00433861054421,6.33407403505896,7.66380945957372,8.99354488408847,10.3232803086032,11.653015733118,12.9827511576327,14.3124865821475,15.6422220066622,16.971957431177,18.3016928556917,19.6314282802065,20.9611637047212,22.290899129236,23.6206345537507,24.9503699782655,26.2801054027802,27.609840827295,28.9395762518097,30.2693116763245,31.5990471008392,32.928782525354,34.2585179498687,35.5882533743835,36.9179887988982,38.247724223413,39.5774596479277,40.9071950724425,42.2369304969572,43.566665921472,44.8964013459867,46.2261367705015,47.5558721950162,48.885607619531,50.2153430440457,51.5450784685605,52.8748138930752,54.20454931759,55.5342847421047,56.8640201666195,58.1937555911342,59.523491015649,60.8532264401638,62.1829618646785,63.5126972891933,64.842432713708,66.1721681382227,67.5019035627375,68.8316389872522,70.161374411767,71.4911098362817,72.8208452607965,74.1505806853112,75.480316109826,76.8100515343407,78.1397869588555,79.4695223833702,80.799257807885,82.1289932323998,83.4587286569145,84.7884640814293,86.118199505944,87.4479349304588,88.7776703549735,90.1074057794883,91.437141204003,92.7668766285178,94.0966120530325,95.4263474775473,96.756082902062,98.0858183265767,99.4155537510915,100.745289175606],[-10.0539719686697,-8.72423654415491,-7.39450111964016,-6.06476569512541,-4.73503027061069,-3.40529484609593,-2.07555942158118,-0.745823997066431,0.583911427448321,1.91364685196307,3.24338227647782,4.57311770099258,5.90285312550733,7.23258855002208,8.56232397453683,9.89205939905158,11.2217948235663,12.5515302480811,13.8812656725958,15.2110010971106,16.5407365216253,17.8704719461401,19.2002073706548,20.5299427951696,21.8596782196843,23.1894136441991,24.5191490687139,25.8488844932286,27.1786199177434,28.5083553422581,29.8380907667728,31.1678261912876,32.4975616158023,33.8272970403171,35.1570324648318,36.4867678893466,37.8165033138613,39.1462387383761,40.4759741628908,41.8057095874056,43.1354450119203,44.4651804364351,45.7949158609498,47.1246512854646,48.4543867099794,49.7841221344941,51.1138575590089,52.4435929835236,53.7733284080384,55.1030638325531,56.4327992570679,57.7625346815826,59.0922701060974,60.4220055306121,61.7517409551269,63.0814763796416,64.4112118041564,65.7409472286711,67.0706826531859,68.4004180777006,69.7301535022154,71.0598889267301,72.3896243512449,73.7193597757596,75.0490952002744,76.3788306247891,77.7085660493039,79.0383014738186,80.3680368983334,81.6977723228481,83.0275077473629,84.3572431718776,85.6869785963924,87.0167140209071,88.3464494454219,89.6761848699366,91.0059202944514,92.3356557189661,93.6653911434809,94.9951265679956,96.3248619925104,97.6545974170251,98.9843328415399],[-11.8149283027361,-10.4851928782213,-9.15545745370655,-7.8257220291918,-6.49598660467707,-5.16625118016232,-3.83651575564757,-2.50678033113282,-1.17704490661807,0.152690517896684,1.48242594241144,2.81216136692619,4.14189679144094,5.47163221595569,6.80136764047044,8.13110306498519,9.46083848949995,10.7905739140147,12.1203093385294,13.4500447630442,14.779780187559,16.1095156120737,17.4392510365885,18.7689864611032,20.098721885618,21.4284573101327,22.7581927346475,24.0879281591622,25.417663583677,26.7473990081917,28.0771344327064,29.4068698572212,30.7366052817359,32.0663407062507,33.3960761307655,34.7258115552802,36.055546979795,37.3852824043097,38.7150178288245,40.0447532533392,41.374488677854,42.7042241023687,44.0339595268835,45.3636949513982,46.693430375913,48.0231658004277,49.3529012249425,50.6826366494572,52.012372073972,53.3421074984867,54.6718429230015,56.0015783475162,57.331313772031,58.6610491965457,59.9907846210605,61.3205200455752,62.65025547009,63.9799908946047,65.3097263191195,66.6394617436342,67.969197168149,69.2989325926637,70.6286680171785,71.9584034416932,73.288138866208,74.6178742907227,75.9476097152375,77.2773451397522,78.607080564267,79.9368159887817,81.2665514132965,82.5962868378112,83.926022262326,85.2557576868407,86.5854931113555,87.9152285358702,89.244963960385,90.5746993848997,91.9044348094145,93.2341702339292,94.563905658444,95.8936410829587,97.2233765074735],[-13.5758846368024,-12.2461492122877,-10.9164137877729,-9.58667836325819,-8.25694293874346,-6.92720751422871,-5.59747208971396,-4.26773666519921,-2.93800124068446,-1.60826581616971,-0.278530391654954,1.0512050328598,2.38094045737455,3.7106758818893,5.04041130640405,6.3701467309188,7.69988215543356,9.02961757994831,10.3593530044631,11.6890884289778,13.0188238534926,14.3485592780073,15.6782947025221,17.0080301270368,18.3377655515516,19.6675009760663,20.9972364005811,22.3269718250958,23.6567072496106,24.9864426741253,26.3161780986401,27.6459135231548,28.9756489476696,30.3053843721843,31.6351197966991,32.9648552212138,34.2945906457286,35.6243260702433,36.9540614947581,38.2837969192728,39.6135323437876,40.9432677683023,42.2730031928171,43.6027386173318,44.9324740418466,46.2622094663613,47.5919448908761,48.9216803153908,50.2514157399056,51.5811511644203,52.9108865889351,54.2406220134498,55.5703574379646,56.9000928624793,58.2298282869941,59.5595637115088,60.8892991360236,62.2190345605383,63.5487699850531,64.8785054095678,66.2082408340826,67.5379762585973,68.8677116831121,70.1974471076268,71.5271825321416,72.8569179566563,74.1866533811711,75.5163888056858,76.8461242302006,78.1758596547153,79.5055950792301,80.8353305037448,82.1650659282596,83.4948013527743,84.8245367772891,86.1542722018038,87.4840076263186,88.8137430508334,90.1434784753481,91.4732138998629,92.8029493243776,94.1326847488923,95.4624201734071],[-15.3368409708688,-14.0071055463541,-12.6773701218393,-11.3476346973246,-10.0178992728099,-8.6881638482951,-7.35842842378035,-6.0286929992656,-4.69895757475085,-3.36922215023609,-2.03948672572134,-0.709751301206591,0.619984123308161,1.94971954782291,3.27945497233766,4.60919039685242,5.93892582136717,7.26866124588192,8.59839667039667,9.92813209491142,11.2578675194262,12.5876029439409,13.9173383684557,15.2470737929704,16.5768092174852,17.9065446419999,19.2362800665147,20.5660154910294,21.8957509155442,23.2254863400589,24.5552217645737,25.8849571890884,27.2146926136032,28.5444280381179,29.8741634626327,31.2038988871474,32.5336343116622,33.8633697361769,35.1931051606917,36.5228405852064,37.8525760097212,39.1823114342359,40.5120468587507,41.8417822832654,43.1715177077802,44.5012531322949,45.8309885568097,47.1607239813244,48.4904594058392,49.8201948303539,51.1499302548687,52.4796656793834,53.8094011038982,55.1391365284129,56.4688719529277,57.7986073774424,59.1283428019572,60.4580782264719,61.7878136509867,63.1175490755014,64.4472845000162,65.7770199245309,67.1067553490457,68.4364907735604,69.7662261980752,71.0959616225899,72.4256970471047,73.7554324716194,75.0851678961342,76.4149033206489,77.7446387451637,79.0743741696784,80.4041095941932,81.733845018708,83.0635804432227,84.3933158677375,85.7230512922522,87.052786716767,88.3825221412817,89.7122575657965,91.0419929903112,92.3717284148259,93.7014638393407],[-17.0977973049352,-15.7680618804205,-14.4383264559057,-13.108591031391,-11.7788556068762,-10.4491201823615,-9.11938475784674,-7.78964933333199,-6.45991390881724,-5.13017848430248,-3.80044305978773,-2.47070763527298,-1.14097221075823,0.188763213756523,1.51849863827128,2.84823406278603,4.17796948730078,5.50770491181553,6.83744033633028,8.16717576084503,9.49691118535979,10.8266466098745,12.1563820343893,13.486117458904,14.8158528834188,16.1455883079335,17.4753237324483,18.805059156963,20.1347945814778,21.4645300059925,22.7942654305073,24.124000855022,25.4537362795368,26.7834717040515,28.1132071285663,29.442942553081,30.7726779775958,32.1024134021105,33.4321488266253,34.76188425114,36.0916196756548,37.4213551001695,38.7510905246843,40.080825949199,41.4105613737138,42.7402967982285,44.0700322227433,45.3997676472581,46.7295030717728,48.0592384962876,49.3889739208023,50.7187093453171,52.0484447698318,53.3781801943466,54.7079156188613,56.037651043376,57.3673864678908,58.6971218924055,60.0268573169203,61.356592741435,62.6863281659498,64.0160635904645,65.3457990149793,66.675534439494,68.0052698640088,69.3350052885235,70.6647407130383,71.9944761375531,73.3242115620678,74.6539469865826,75.9836824110973,77.3134178356121,78.6431532601268,79.9728886846416,81.3026241091563,82.6323595336711,83.9620949581858,85.2918303827006,86.6215658072153,87.9513012317301,89.2810366562448,90.6107720807595,91.9405075052743],[-18.8587536390016,-17.5290182144869,-16.1992827899721,-14.8695473654574,-13.5398119409426,-12.2100765164279,-10.8803410919131,-9.55060566739838,-8.22087024288362,-6.89113481836887,-5.56139939385412,-4.23166396933937,-2.90192854482462,-1.57219312030987,-0.242457695795114,1.08727772871964,2.41701315323439,3.74674857774914,5.07648400226389,6.40621942677864,7.7359548512934,9.06569027580815,10.3954257003229,11.7251611248377,13.0548965493524,14.3846319738672,15.7143673983819,17.0441028228967,18.3738382474114,19.7035736719261,21.0333090964409,22.3630445209556,23.6927799454704,25.0225153699851,26.3522507944999,27.6819862190146,29.0117216435294,30.3414570680441,31.6711924925589,33.0009279170737,34.3306633415884,35.6603987661032,36.9901341906179,38.3198696151327,39.6496050396474,40.9793404641622,42.3090758886769,43.6388113131917,44.9685467377064,46.2982821622212,47.6280175867359,48.9577530112507,50.2874884357654,51.6172238602802,52.9469592847949,54.2766947093097,55.6064301338244,56.9361655583392,58.2659009828539,59.5956364073687,60.9253718318834,62.2551072563982,63.5848426809129,64.9145781054277,66.2443135299424,67.5740489544572,68.9037843789719,70.2335198034867,71.5632552280014,72.8929906525162,74.2227260770309,75.5524615015457,76.8821969260604,78.2119323505752,79.5416677750899,80.8714031996047,82.2011386241194,83.5308740486342,84.8606094731489,86.1903448976637,87.5200803221784,88.8498157466932,90.1795511712079],[-20.619709973068,-19.2899745485532,-17.9602391240385,-16.6305036995237,-15.300768275009,-13.9710328504943,-12.6412974259795,-11.3115620014648,-9.98182657695001,-8.65209115243526,-7.32235572792051,-5.99262030340576,-4.66288487889101,-3.33314945437625,-2.0034140298615,-0.673678605346751,0.656056819168001,1.98579224368275,3.3155276681975,4.64526309271226,5.97499851722701,7.30473394174176,8.63446936625651,9.96420479077126,11.293940215286,12.6236756398008,13.9534110643155,15.2831464888303,16.612881913345,17.9426173378597,19.2723527623745,20.6020881868892,21.931823611404,23.2615590359188,24.5912944604335,25.9210298849483,27.250765309463,28.5805007339778,29.9102361584925,31.2399715830073,32.569707007522,33.8994424320368,35.2291778565515,36.5589132810663,37.888648705581,39.2183841300958,40.5481195546105,41.8778549791253,43.20759040364,44.5373258281548,45.8670612526695,47.1967966771843,48.526532101699,49.8562675262138,51.1860029507285,52.5157383752433,53.845473799758,55.1752092242728,56.5049446487875,57.8346800733023,59.164415497817,60.4941509223318,61.8238863468465,63.1536217713613,64.483357195876,65.8130926203908,67.1428280449055,68.4725634694203,69.802298893935,71.1320343184498,72.4617697429645,73.7915051674793,75.121240591994,76.4509760165088,77.7807114410235,79.1104468655383,80.440182290053,81.7699177145678,83.0996531390825,84.4293885635973,85.759123988112,87.0888594126268,88.4185948371415],[-22.3806663071344,-21.0509308826196,-19.7211954581049,-18.3914600335901,-17.0617246090754,-15.7319891845607,-14.4022537600459,-13.0725183355312,-11.7427829110164,-10.4130474865017,-9.0833120619869,-7.75357663747215,-6.4238412129574,-5.09410578844264,-3.76437036392789,-2.43463493941314,-1.10489951489839,0.224835909616363,1.55457133413111,2.88430675864587,4.21404218316062,5.54377760767537,6.87351303219012,8.20324845670487,9.53298388121963,10.8627193057344,12.1924547302491,13.5221901547639,14.8519255792786,16.1816610037934,17.5113964283081,18.8411318528229,20.1708672773376,21.5006027018524,22.8303381263671,24.1600735508819,25.4898089753966,26.8195443999114,28.1492798244261,29.4790152489409,30.8087506734556,32.1384860979704,33.4682215224851,34.7979569469999,36.1276923715146,37.4574277960294,38.7871632205441,40.1168986450589,41.4466340695736,42.7763694940884,44.1061049186031,45.4358403431179,46.7655757676326,48.0953111921474,49.4250466166621,50.7547820411769,52.0845174656916,53.4142528902064,54.7439883147211,56.0737237392359,57.4034591637506,58.7331945882654,60.0629300127801,61.3926654372949,62.7224008618096,64.0521362863244,65.3818717108391,66.7116071353539,68.0413425598686,69.3710779843834,70.7008134088981,72.0305488334129,73.3602842579276,74.6900196824424,76.0197551069571,77.3494905314719,78.6792259559867,80.0089613805014,81.3386968050162,82.6684322295309,83.9981676540456,85.3279030785604,86.6576385030751]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Temp"},"yaxis":{"title":"Month"},"zaxis":{"title":"Ozone"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[67,72,74,62,66,65,59,61,74,69,66,68,58,64,66,57,68,62,59,73,61,61,67,81,79,76,82,90,87,82,77,72,65,73,76,84,85,81,83,83,88,92,92,89,73,81,80,81,82,84,87,85,74,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,86,82,80,77,79,76,78,78,77,72,79,81,86,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,75,76,68],"y":[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9],"z":[41,36,12,18,28,23,19,8,7,16,11,14,18,14,34,6,30,11,1,11,4,32,23,45,115,37,29,71,39,23,21,37,20,12,13,135,49,32,64,40,77,97,97,85,10,27,7,48,35,61,79,63,16,80,108,20,52,82,50,64,59,39,9,16,78,35,66,122,89,110,44,28,65,22,59,23,31,44,21,9,45,168,73,76,118,84,85,96,78,73,91,47,32,20,23,21,24,44,21,28,9,13,46,18,13,24,16,13,23,36,7,14,30,14,18,20],"text":["1","2","3","4","6","7","8","9","11","12","13","14","15","16","17","18","19","20","21","22","23","24","28","29","30","31","38","40","41","44","47","48","49","50","51","62","63","64","66","67","68","69","70","71","73","74","76","77","78","79","80","81","82","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","104","105","106","108","109","110","111","112","113","114","116","117","118","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","151","152","153"],"mode":"markers","type":"scatter3d","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Ozone","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0861298230472048","rgba(72,33,112,1)"],["0.12633720862017","rgba(71,45,122,1)"],["0.164986537402877","rgba(68,57,128,1)"],["0.201205593892304","rgba(65,68,134,1)"],["0.238810687991314","rgba(61,79,138,1)"],["0.276560033623094","rgba(57,89,140,1)"],["0.313072064048994","rgba(53,99,141,1)"],["0.351040022415396","rgba(48,108,142,1)"],["0.388426503779784","rgba(45,117,142,1)"],["0.42464556026921","rgba(41,125,142,1)"],["0.463537916281245","rgba(38,135,141,1)"],["0.5","rgba(37,144,140,1)"],["0.536462083718755","rgba(33,153,138,1)"],["0.57535443973079","rgba(37,162,134,1)"],["0.611573496220217","rgba(45,171,129,1)"],["0.648959977584604","rgba(51,179,124,1)"],["0.686927935951006","rgba(67,187,116,1)"],["0.723439966376906","rgba(86,194,106,1)"],["0.761189312008686","rgba(103,202,94,1)"],["0.798794406107696","rgba(124,208,82,1)"],["0.835013462597123","rgba(148,214,69,1)"],["0.873662791379829","rgba(171,220,52,1)"],["0.913870176952795","rgba(197,224,43,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-8.29301563460328,-6.96328021008852,-5.63354478557377,-4.30380936105902,-2.9740739365443,-1.64433851202955,-0.314603087514794,1.01513233699996,2.34486776151471,3.67460318602946,5.00433861054421,6.33407403505896,7.66380945957372,8.99354488408847,10.3232803086032,11.653015733118,12.9827511576327,14.3124865821475,15.6422220066622,16.971957431177,18.3016928556917,19.6314282802065,20.9611637047212,22.290899129236,23.6206345537507,24.9503699782655,26.2801054027802,27.609840827295,28.9395762518097,30.2693116763245,31.5990471008392,32.928782525354,34.2585179498687,35.5882533743835,36.9179887988982,38.247724223413,39.5774596479277,40.9071950724425,42.2369304969572,43.566665921472,44.8964013459867,46.2261367705015,47.5558721950162,48.885607619531,50.2153430440457,51.5450784685605,52.8748138930752,54.20454931759,55.5342847421047,56.8640201666195,58.1937555911342,59.523491015649,60.8532264401638,62.1829618646785,63.5126972891933,64.842432713708,66.1721681382227,67.5019035627375,68.8316389872522,70.161374411767,71.4911098362817,72.8208452607965,74.1505806853112,75.480316109826,76.8100515343407,78.1397869588555,79.4695223833702,80.799257807885,82.1289932323998,83.4587286569145,84.7884640814293,86.118199505944,87.4479349304588,88.7776703549735,90.1074057794883,91.437141204003,92.7668766285178,94.0966120530325,95.4263474775473,96.756082902062,98.0858183265767,99.4155537510915,100.745289175606],[-10.0539719686697,-8.72423654415491,-7.39450111964016,-6.06476569512541,-4.73503027061069,-3.40529484609593,-2.07555942158118,-0.745823997066431,0.583911427448321,1.91364685196307,3.24338227647782,4.57311770099258,5.90285312550733,7.23258855002208,8.56232397453683,9.89205939905158,11.2217948235663,12.5515302480811,13.8812656725958,15.2110010971106,16.5407365216253,17.8704719461401,19.2002073706548,20.5299427951696,21.8596782196843,23.1894136441991,24.5191490687139,25.8488844932286,27.1786199177434,28.5083553422581,29.8380907667728,31.1678261912876,32.4975616158023,33.8272970403171,35.1570324648318,36.4867678893466,37.8165033138613,39.1462387383761,40.4759741628908,41.8057095874056,43.1354450119203,44.4651804364351,45.7949158609498,47.1246512854646,48.4543867099794,49.7841221344941,51.1138575590089,52.4435929835236,53.7733284080384,55.1030638325531,56.4327992570679,57.7625346815826,59.0922701060974,60.4220055306121,61.7517409551269,63.0814763796416,64.4112118041564,65.7409472286711,67.0706826531859,68.4004180777006,69.7301535022154,71.0598889267301,72.3896243512449,73.7193597757596,75.0490952002744,76.3788306247891,77.7085660493039,79.0383014738186,80.3680368983334,81.6977723228481,83.0275077473629,84.3572431718776,85.6869785963924,87.0167140209071,88.3464494454219,89.6761848699366,91.0059202944514,92.3356557189661,93.6653911434809,94.9951265679956,96.3248619925104,97.6545974170251,98.9843328415399],[-11.8149283027361,-10.4851928782213,-9.15545745370655,-7.8257220291918,-6.49598660467707,-5.16625118016232,-3.83651575564757,-2.50678033113282,-1.17704490661807,0.152690517896684,1.48242594241144,2.81216136692619,4.14189679144094,5.47163221595569,6.80136764047044,8.13110306498519,9.46083848949995,10.7905739140147,12.1203093385294,13.4500447630442,14.779780187559,16.1095156120737,17.4392510365885,18.7689864611032,20.098721885618,21.4284573101327,22.7581927346475,24.0879281591622,25.417663583677,26.7473990081917,28.0771344327064,29.4068698572212,30.7366052817359,32.0663407062507,33.3960761307655,34.7258115552802,36.055546979795,37.3852824043097,38.7150178288245,40.0447532533392,41.374488677854,42.7042241023687,44.0339595268835,45.3636949513982,46.693430375913,48.0231658004277,49.3529012249425,50.6826366494572,52.012372073972,53.3421074984867,54.6718429230015,56.0015783475162,57.331313772031,58.6610491965457,59.9907846210605,61.3205200455752,62.65025547009,63.9799908946047,65.3097263191195,66.6394617436342,67.969197168149,69.2989325926637,70.6286680171785,71.9584034416932,73.288138866208,74.6178742907227,75.9476097152375,77.2773451397522,78.607080564267,79.9368159887817,81.2665514132965,82.5962868378112,83.926022262326,85.2557576868407,86.5854931113555,87.9152285358702,89.244963960385,90.5746993848997,91.9044348094145,93.2341702339292,94.563905658444,95.8936410829587,97.2233765074735],[-13.5758846368024,-12.2461492122877,-10.9164137877729,-9.58667836325819,-8.25694293874346,-6.92720751422871,-5.59747208971396,-4.26773666519921,-2.93800124068446,-1.60826581616971,-0.278530391654954,1.0512050328598,2.38094045737455,3.7106758818893,5.04041130640405,6.3701467309188,7.69988215543356,9.02961757994831,10.3593530044631,11.6890884289778,13.0188238534926,14.3485592780073,15.6782947025221,17.0080301270368,18.3377655515516,19.6675009760663,20.9972364005811,22.3269718250958,23.6567072496106,24.9864426741253,26.3161780986401,27.6459135231548,28.9756489476696,30.3053843721843,31.6351197966991,32.9648552212138,34.2945906457286,35.6243260702433,36.9540614947581,38.2837969192728,39.6135323437876,40.9432677683023,42.2730031928171,43.6027386173318,44.9324740418466,46.2622094663613,47.5919448908761,48.9216803153908,50.2514157399056,51.5811511644203,52.9108865889351,54.2406220134498,55.5703574379646,56.9000928624793,58.2298282869941,59.5595637115088,60.8892991360236,62.2190345605383,63.5487699850531,64.8785054095678,66.2082408340826,67.5379762585973,68.8677116831121,70.1974471076268,71.5271825321416,72.8569179566563,74.1866533811711,75.5163888056858,76.8461242302006,78.1758596547153,79.5055950792301,80.8353305037448,82.1650659282596,83.4948013527743,84.8245367772891,86.1542722018038,87.4840076263186,88.8137430508334,90.1434784753481,91.4732138998629,92.8029493243776,94.1326847488923,95.4624201734071],[-15.3368409708688,-14.0071055463541,-12.6773701218393,-11.3476346973246,-10.0178992728099,-8.6881638482951,-7.35842842378035,-6.0286929992656,-4.69895757475085,-3.36922215023609,-2.03948672572134,-0.709751301206591,0.619984123308161,1.94971954782291,3.27945497233766,4.60919039685242,5.93892582136717,7.26866124588192,8.59839667039667,9.92813209491142,11.2578675194262,12.5876029439409,13.9173383684557,15.2470737929704,16.5768092174852,17.9065446419999,19.2362800665147,20.5660154910294,21.8957509155442,23.2254863400589,24.5552217645737,25.8849571890884,27.2146926136032,28.5444280381179,29.8741634626327,31.2038988871474,32.5336343116622,33.8633697361769,35.1931051606917,36.5228405852064,37.8525760097212,39.1823114342359,40.5120468587507,41.8417822832654,43.1715177077802,44.5012531322949,45.8309885568097,47.1607239813244,48.4904594058392,49.8201948303539,51.1499302548687,52.4796656793834,53.8094011038982,55.1391365284129,56.4688719529277,57.7986073774424,59.1283428019572,60.4580782264719,61.7878136509867,63.1175490755014,64.4472845000162,65.7770199245309,67.1067553490457,68.4364907735604,69.7662261980752,71.0959616225899,72.4256970471047,73.7554324716194,75.0851678961342,76.4149033206489,77.7446387451637,79.0743741696784,80.4041095941932,81.733845018708,83.0635804432227,84.3933158677375,85.7230512922522,87.052786716767,88.3825221412817,89.7122575657965,91.0419929903112,92.3717284148259,93.7014638393407],[-17.0977973049352,-15.7680618804205,-14.4383264559057,-13.108591031391,-11.7788556068762,-10.4491201823615,-9.11938475784674,-7.78964933333199,-6.45991390881724,-5.13017848430248,-3.80044305978773,-2.47070763527298,-1.14097221075823,0.188763213756523,1.51849863827128,2.84823406278603,4.17796948730078,5.50770491181553,6.83744033633028,8.16717576084503,9.49691118535979,10.8266466098745,12.1563820343893,13.486117458904,14.8158528834188,16.1455883079335,17.4753237324483,18.805059156963,20.1347945814778,21.4645300059925,22.7942654305073,24.124000855022,25.4537362795368,26.7834717040515,28.1132071285663,29.442942553081,30.7726779775958,32.1024134021105,33.4321488266253,34.76188425114,36.0916196756548,37.4213551001695,38.7510905246843,40.080825949199,41.4105613737138,42.7402967982285,44.0700322227433,45.3997676472581,46.7295030717728,48.0592384962876,49.3889739208023,50.7187093453171,52.0484447698318,53.3781801943466,54.7079156188613,56.037651043376,57.3673864678908,58.6971218924055,60.0268573169203,61.356592741435,62.6863281659498,64.0160635904645,65.3457990149793,66.675534439494,68.0052698640088,69.3350052885235,70.6647407130383,71.9944761375531,73.3242115620678,74.6539469865826,75.9836824110973,77.3134178356121,78.6431532601268,79.9728886846416,81.3026241091563,82.6323595336711,83.9620949581858,85.2918303827006,86.6215658072153,87.9513012317301,89.2810366562448,90.6107720807595,91.9405075052743],[-18.8587536390016,-17.5290182144869,-16.1992827899721,-14.8695473654574,-13.5398119409426,-12.2100765164279,-10.8803410919131,-9.55060566739838,-8.22087024288362,-6.89113481836887,-5.56139939385412,-4.23166396933937,-2.90192854482462,-1.57219312030987,-0.242457695795114,1.08727772871964,2.41701315323439,3.74674857774914,5.07648400226389,6.40621942677864,7.7359548512934,9.06569027580815,10.3954257003229,11.7251611248377,13.0548965493524,14.3846319738672,15.7143673983819,17.0441028228967,18.3738382474114,19.7035736719261,21.0333090964409,22.3630445209556,23.6927799454704,25.0225153699851,26.3522507944999,27.6819862190146,29.0117216435294,30.3414570680441,31.6711924925589,33.0009279170737,34.3306633415884,35.6603987661032,36.9901341906179,38.3198696151327,39.6496050396474,40.9793404641622,42.3090758886769,43.6388113131917,44.9685467377064,46.2982821622212,47.6280175867359,48.9577530112507,50.2874884357654,51.6172238602802,52.9469592847949,54.2766947093097,55.6064301338244,56.9361655583392,58.2659009828539,59.5956364073687,60.9253718318834,62.2551072563982,63.5848426809129,64.9145781054277,66.2443135299424,67.5740489544572,68.9037843789719,70.2335198034867,71.5632552280014,72.8929906525162,74.2227260770309,75.5524615015457,76.8821969260604,78.2119323505752,79.5416677750899,80.8714031996047,82.2011386241194,83.5308740486342,84.8606094731489,86.1903448976637,87.5200803221784,88.8498157466932,90.1795511712079],[-20.619709973068,-19.2899745485532,-17.9602391240385,-16.6305036995237,-15.300768275009,-13.9710328504943,-12.6412974259795,-11.3115620014648,-9.98182657695001,-8.65209115243526,-7.32235572792051,-5.99262030340576,-4.66288487889101,-3.33314945437625,-2.0034140298615,-0.673678605346751,0.656056819168001,1.98579224368275,3.3155276681975,4.64526309271226,5.97499851722701,7.30473394174176,8.63446936625651,9.96420479077126,11.293940215286,12.6236756398008,13.9534110643155,15.2831464888303,16.612881913345,17.9426173378597,19.2723527623745,20.6020881868892,21.931823611404,23.2615590359188,24.5912944604335,25.9210298849483,27.250765309463,28.5805007339778,29.9102361584925,31.2399715830073,32.569707007522,33.8994424320368,35.2291778565515,36.5589132810663,37.888648705581,39.2183841300958,40.5481195546105,41.8778549791253,43.20759040364,44.5373258281548,45.8670612526695,47.1967966771843,48.526532101699,49.8562675262138,51.1860029507285,52.5157383752433,53.845473799758,55.1752092242728,56.5049446487875,57.8346800733023,59.164415497817,60.4941509223318,61.8238863468465,63.1536217713613,64.483357195876,65.8130926203908,67.1428280449055,68.4725634694203,69.802298893935,71.1320343184498,72.4617697429645,73.7915051674793,75.121240591994,76.4509760165088,77.7807114410235,79.1104468655383,80.440182290053,81.7699177145678,83.0996531390825,84.4293885635973,85.759123988112,87.0888594126268,88.4185948371415],[-22.3806663071344,-21.0509308826196,-19.7211954581049,-18.3914600335901,-17.0617246090754,-15.7319891845607,-14.4022537600459,-13.0725183355312,-11.7427829110164,-10.4130474865017,-9.0833120619869,-7.75357663747215,-6.4238412129574,-5.09410578844264,-3.76437036392789,-2.43463493941314,-1.10489951489839,0.224835909616363,1.55457133413111,2.88430675864587,4.21404218316062,5.54377760767537,6.87351303219012,8.20324845670487,9.53298388121963,10.8627193057344,12.1924547302491,13.5221901547639,14.8519255792786,16.1816610037934,17.5113964283081,18.8411318528229,20.1708672773376,21.5006027018524,22.8303381263671,24.1600735508819,25.4898089753966,26.8195443999114,28.1492798244261,29.4790152489409,30.8087506734556,32.1384860979704,33.4682215224851,34.7979569469999,36.1276923715146,37.4574277960294,38.7871632205441,40.1168986450589,41.4466340695736,42.7763694940884,44.1061049186031,45.4358403431179,46.7655757676326,48.0953111921474,49.4250466166621,50.7547820411769,52.0845174656916,53.4142528902064,54.7439883147211,56.0737237392359,57.4034591637506,58.7331945882654,60.0629300127801,61.3926654372949,62.7224008618096,64.0521362863244,65.3818717108391,66.7116071353539,68.0413425598686,69.3710779843834,70.7008134088981,72.0305488334129,73.3602842579276,74.6900196824424,76.0197551069571,77.3494905314719,78.6792259559867,80.0089613805014,81.3386968050162,82.6684322295309,83.9981676540456,85.3279030785604,86.6576385030751]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":[]}</script>
<p>Here is a second view of this same regression with what is called a contour plot, contour map, or density plot.</p>
<pre class="r"><code>mycolorpalette &lt;- colorRampPalette(c(&quot;skyblue2&quot;, &quot;orange&quot;))
filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26))</code></pre>
<p><strong>Including the Interaction Term</strong></p>
<p>Here is what a 3D regression looks like when the interaction term is present. The two x-variables of <code>Month</code> and <code>Temp</code> are being used to predict the y-variable of <code>Ozone</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Ozone} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} + \overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}} \underbrace{X_{1i}}_\text{Temp} + \overbrace{\beta_2}^{\stackrel{\text{change in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{Month} + \overbrace{\beta_3}^{\stackrel{\text{change in}}{\text{slope}}} \underbrace{X_{1i}X_{2i}}_\text{Temp:Month} + \epsilon_i
\]</span></p>
<pre class="r"><code>air_lm &lt;- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality)
pander(air_lm$coefficients)</code></pre>
<table style="width:58%;">
<colgroup>
<col width="19%" />
<col width="9%" />
<col width="12%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">(Intercept)</th>
<th align="center">Temp</th>
<th align="center">Month</th>
<th align="center">Temp:Month</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-3.915</td>
<td align="center">0.77</td>
<td align="center">-23.01</td>
<td align="center">0.2678</td>
</tr>
</tbody>
</table>
<p>Notice how all coefficient estimates have changed. The y-intercept, <span class="math inline">\(\beta_0\)</span> is now estimated to be <span class="math inline">\(-3.915\)</span>. The slope term, <span class="math inline">\(\beta_1\)</span>, in the Temp-direction is estimated as <span class="math inline">\(0.77\)</span>, while the slope term, <span class="math inline">\(\beta_2\)</span>, in the Month-direction is estimated to be <span class="math inline">\(-23.01\)</span>. This change in estimated coefficiets is due to the presence of the interaction term’s coefficient, <span class="math inline">\(\beta_3\)</span>, which is estimated to be <span class="math inline">\(0.2678\)</span>. As you should notice in the graphic, the interaction model allows the “slopes” in each direction to change, creating a “curved” surface for the regression surface instead of a flat surface.</p>
<pre class="r"><code>#Perform the multiple regression
air_lm &lt;- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality)

#Graph Resolution (more important for more complex shapes)
graph_reso &lt;- 0.5

#Setup Axis
axis_x &lt;- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso)
axis_y &lt;- seq(min(airquality$Month), max(airquality$Month), by = graph_reso)

#Sample points
air_surface &lt;- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F)
air_surface &lt;- air_surface %&gt;% mutate(Z=predict.lm(air_lm, newdata = air_surface))
air_surface &lt;- acast(air_surface, Month ~ Temp, value.var = &quot;Z&quot;) #y ~ x

#Create scatterplot
plot_ly(airquality, 
        x = ~Temp, 
        y = ~Month, 
        z = ~Ozone,
        text = rownames(airquality), 
        type = &quot;scatter3d&quot;, 
        mode = &quot;markers&quot;) %&gt;%
  add_trace(z = air_surface,
            x = axis_x,
            y = axis_y,
            type = &quot;surface&quot;)</code></pre>
<div id="htmlwidget-6c0d63077dc4fc0c9733" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-6c0d63077dc4fc0c9733">{"x":{"visdat":{"120d3a0e3595":["function () ","plotlyVisDat"]},"cur_data":"120d3a0e3595","attrs":{"120d3a0e3595":{"x":{},"y":{},"z":{},"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"},"120d3a0e3595.1":{"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-0.834420893928069,0.220128376868956,1.274677647666,2.32922691846304,3.38377618926008,4.43832546005713,5.49287473085415,6.54742400165119,7.60197327244825,8.65652254324527,9.71107181404233,10.7656210848394,11.8201703556364,12.8747196264334,13.9292688972305,14.9838181680275,16.0383674388246,17.0929167096216,18.1474659804186,19.2020152512157,20.2565645220127,21.3111137928098,22.3656630636068,23.4202123344039,24.4747616052009,25.5293108759979,26.583860146795,27.638409417592,28.6929586883891,29.7475079591861,30.8020572299831,31.8566065007802,32.9111557715772,33.9657050423742,35.0202543131713,36.0748035839683,37.1293528547654,38.1839021255624,39.2384513963595,40.2930006671565,41.3475499379535,42.4020992087506,43.4566484795476,44.5111977503446,45.5657470211417,46.6202962919387,47.6748455627358,48.7293948335328,49.7839441043298,50.8384933751269,51.8930426459239,52.947591916721,54.002141187518,55.0566904583151,56.1112397291121,57.1657889999091,58.2203382707062,59.2748875415032,60.3294368123002,61.3839860830973,62.4385353538943,63.4930846246914,64.5476338954884,65.6021831662854,66.6567324370825,67.7112817078795,68.7658309786766,69.8203802494736,70.8749295202706,71.9294787910677,72.9840280618647,74.0385773326618,75.0931266034588,76.1476758742559,77.2022251450529,78.2567744158499,79.311323686647,80.365872957444,81.4204222282411,82.4749714990381,83.5295207698351,84.5840700406322,85.6386193114292],[-4.83862279325234,-3.71712097280195,-2.59561915235156,-1.47411733190117,-0.352615511450779,0.768886308999626,1.89038812945,3.01188994990041,4.1333917703508,5.25489359080117,6.37639541125158,7.49789723170197,8.61939905215236,9.74090087260275,10.8624026930531,11.9839045135035,13.1054063339539,14.2269081544043,15.3484099748547,16.4699117953051,17.5914136157555,18.7129154362059,19.8344172566563,20.9559190771067,22.077420897557,23.1989227180075,24.3204245384578,25.4419263589082,26.5634281793586,27.684929999809,28.8064318202594,29.9279336407098,31.0494354611602,32.1709372816106,33.292439102061,34.4139409225114,35.5354427429618,36.6569445634121,37.7784463838625,38.8999482043129,40.0214500247633,41.1429518452137,42.2644536656641,43.3859554861145,44.5074573065649,45.6289591270153,46.7504609474657,47.8719627679161,48.9934645883665,50.1149664088169,51.2364682292672,52.3579700497176,53.479471870168,54.6009736906184,55.7224755110688,56.8439773315192,57.9654791519696,59.08698097242,60.2084827928704,61.3299846133208,62.4514864337712,63.5729882542215,64.6944900746719,65.8159918951223,66.9374937155727,68.0589955360231,69.1804973564735,70.3019991769239,71.4235009973743,72.5450028178247,73.6665046382751,74.7880064587255,75.9095082791759,77.0310100996262,78.1525119200766,79.274013740527,80.3955155609774,81.5170173814278,82.6385192018782,83.7600210223286,84.881522842779,86.0030246632294,87.1245264836798],[-8.8428246925766,-7.65437032247286,-6.46591595236912,-5.27746158226537,-4.08900721216163,-2.90055284205788,-1.71209847195415,-0.523644101850394,0.664810268253348,1.85326463835708,3.04171900846084,4.23017337856457,5.41862774866831,6.60708211877207,7.7955364888758,8.98399085897954,10.1724452290833,11.360899599187,12.5493539692908,13.7378083393945,14.9262627094983,16.114717079602,17.3031714497057,18.4916258198095,19.6800801899132,20.868534560017,22.0569889301207,23.2454433002245,24.4338976703282,25.6223520404319,26.8108064105357,27.9992607806394,29.1877151507432,30.3761695208469,31.5646238909507,32.7530782610544,33.9415326311582,35.1299870012619,36.3184413713656,37.5068957414694,38.6953501115731,39.8838044816769,41.0722588517806,42.2607132218843,43.4491675919881,44.6376219620918,45.8260763321956,47.0145307022993,48.2029850724031,49.3914394425068,50.5798938126105,51.7683481827143,52.956802552818,54.1452569229218,55.3337112930255,56.5221656631292,57.710620033233,58.8990744033368,60.0875287734405,61.2759831435442,62.464437513648,63.6528918837517,64.8413462538555,66.0298006239592,67.2182549940629,68.4067093641667,69.5951637342704,70.7836181043742,71.9720724744779,73.1605268445817,74.3489812146854,75.5374355847892,76.7258899548929,77.9143443249966,79.1027986951004,80.2912530652041,81.4797074353079,82.6681618054116,83.8566161755153,85.0450705456191,86.2335249157228,87.4219792858266,88.6104336559303],[-12.8470265919009,-11.5916196721438,-10.3362127523867,-9.08080583262961,-7.82539891287252,-6.56999199311541,-5.31458507335833,-4.05917815360122,-2.80377123384413,-1.54836431408705,-0.292957394329932,0.962449525427147,2.21785644518425,3.47326336494135,4.72867028469842,5.98407720445553,7.23948412421262,8.49489104396972,9.75029796372682,11.0057048834839,12.261111803241,13.5165187229981,14.7719256427552,16.0273325625123,17.2827394822694,18.5381464020265,19.7935533217836,21.0489602415407,22.3043671612978,23.5597740810548,24.8151810008119,26.070587920569,27.3259948403261,28.5814017600832,29.8368086798403,31.0922155995974,32.3476225193545,33.6030294391116,34.8584363588687,36.1138432786258,37.3692501983829,38.62465711814,39.8800640378971,41.1354709576542,42.3908778774113,43.6462847971683,44.9016917169254,46.1570986366825,47.4125055564396,48.6679124761967,49.9233193959538,51.1787263157109,52.434133235468,53.6895401552251,54.9449470749822,56.2003539947393,57.4557609144964,58.7111678342535,59.9665747540106,61.2219816737677,62.4773885935248,63.7327955132819,64.988202433039,66.243609352796,67.4990162725531,68.7544231923102,70.0098301120673,71.2652370318244,72.5206439515815,73.7760508713386,75.0314577910957,76.2868647108528,77.5422716306099,78.797678550367,80.0530854701241,81.3084923898812,82.5638993096383,83.8193062293954,85.0747131491525,86.3301200689096,87.5855269886666,88.8409339084238,90.0963408281808],[-16.8512284912251,-15.5288690218147,-14.2065095524043,-12.8841500829938,-11.5617906135834,-10.2394311441729,-8.91707167476247,-7.59471220535202,-6.27235273594158,-4.94999326653114,-3.62763379712068,-2.30527432771025,-0.982914858299793,0.33944461111065,1.66180408052109,2.98416354993154,4.30652301934198,5.62888248875244,6.95124195816288,8.27360142757331,9.59596089698377,10.9183203663942,12.2406798358046,13.5630393052151,14.8853987746255,16.207758244036,17.5301177134464,18.8524771828569,20.1748366522673,21.4971961216778,22.8195555910882,24.1419150604987,25.4642745299091,26.7866339993196,28.10899346873,29.4313529381404,30.7537124075509,32.0760718769614,33.3984313463718,34.7207908157822,36.0431502851927,37.3655097546031,38.6878692240136,40.010228693424,41.3325881628345,42.6549476322449,43.9773071016553,45.2996665710658,46.6220260404763,47.9443855098867,49.2667449792971,50.5891044487076,51.911463918118,53.2338233875284,54.5561828569389,55.8785423263494,57.2009017957598,58.5232612651702,59.8456207345807,61.1679802039911,62.4903396734016,63.812699142812,65.1350586122225,66.4574180816329,67.7797775510434,69.1021370204538,70.4244964898643,71.7468559592747,73.0692154286851,74.3915748980956,75.7139343675061,77.0362938369165,78.3586533063269,79.6810127757374,81.0033722451478,82.3257317145583,83.6480911839687,84.9704506533792,86.2928101227896,87.6151695922,88.9375290616105,90.2598885310209,91.5822480004314],[-20.8554303905494,-19.4661183714856,-18.0768063524219,-16.687494333358,-15.2981823142943,-13.9088702952304,-12.5195582761666,-11.1302462571029,-9.74093423803906,-8.35162221897527,-6.96231019991146,-5.57299818084766,-4.18368616178388,-2.79437414272007,-1.40506212365628,-0.0157501045924846,1.37356191447131,2.76287393353512,4.15218595259893,5.54149797166269,6.9308099907265,8.32012200979031,9.70943402885408,11.0987460479179,12.4880580669817,13.8773700860455,15.2666821051093,16.6559941241731,18.0453061432369,19.4346181623007,20.8239301813645,22.2132422004283,23.602554219492,24.9918662385559,26.3811782576197,27.7704902766835,29.1598022957473,30.549114314811,31.9384263338749,33.3277383529386,34.7170503720024,36.1063623910663,37.49567441013,38.8849864291938,40.2742984482576,41.6636104673214,43.0529224863852,44.442234505449,45.8315465245128,47.2208585435766,48.6101705626404,49.9994825817042,51.388794600768,52.7781066198318,54.1674186388956,55.5567306579594,56.9460426770232,58.335354696087,59.7246667151508,61.1139787342146,62.5032907532784,63.8926027723422,65.281914791406,66.6712268104698,68.0605388295336,69.4498508485974,70.8391628676612,72.228474886725,73.6177869057888,75.0070989248525,76.3964109439164,77.7857229629802,79.1750349820439,80.5643470011077,81.9536590201715,83.3429710392353,84.7322830582991,86.1215950773629,87.5109070964267,88.9002191154905,90.2895311345543,91.6788431536181,93.0681551726819],[-24.8596322898737,-23.4033677211565,-21.9471031524394,-20.4908385837222,-19.0345740150051,-17.5783094462879,-16.1220448775708,-14.6657803088537,-13.2095157401365,-11.7532511714194,-10.2969866027022,-8.84072203398506,-7.38445746526793,-5.92819289655077,-4.47192832783361,-3.01566375911645,-1.55939919039932,-0.103134621682187,1.35312994703497,2.80939451575213,4.26565908446929,5.72192365318642,7.17818822190355,8.63445279062071,10.0907173593379,11.546981928055,13.0032464967722,14.4595110654893,15.9157756342065,17.3720402029236,18.8283047716408,20.2845693403579,21.740833909075,23.1970984777922,24.6533630465094,26.1096276152265,27.5658921839436,29.0221567526608,30.4784213213779,31.9346858900951,33.3909504588122,34.8472150275294,36.3034795962465,37.7597441649637,39.2160087336808,40.672273302398,42.1285378711151,43.5848024398323,45.0410670085494,46.4973315772666,47.9535961459837,49.4098607147009,50.866125283418,52.3223898521352,53.7786544208523,55.2349189895695,56.6911835582866,58.1474481270038,59.6037126957209,61.059977264438,62.5162418331552,63.9725064018723,65.4287709705895,66.8850355393066,68.3413001080238,69.7975646767409,71.2538292454581,72.7100938141752,74.1663583828924,75.6226229516095,77.0788875203267,78.5351520890438,79.991416657761,81.4476812264781,82.9039457951953,84.3602103639124,85.8164749326296,87.2727395013467,88.7290040700639,90.185268638781,91.6415332074981,93.0977977762153,94.5540623449325],[-28.8638341891979,-27.3406170708274,-25.817399952457,-24.2941828340865,-22.770965715716,-21.2477485973454,-19.724531478975,-18.2013143606045,-16.6780972422339,-15.1548801238634,-13.631663005493,-12.1084458871225,-10.585228768752,-9.06201165038144,-7.53879453201097,-6.01557741364044,-4.49236029526995,-2.96914317689948,-1.44592605852895,0.0772910598415422,1.60050817821207,3.12372529658253,4.64694241495303,6.17015953332356,7.69337665169402,9.21659377006455,10.739810888435,12.2630280068055,13.786245125176,15.3094622435465,16.8326793619171,18.3558964802876,19.879113598658,21.4023307170285,22.925547835399,24.4487649537695,25.97198207214,27.4951991905105,29.018416308881,30.5416334272515,32.064850545622,33.5880676639925,35.111284782363,36.6345019007335,38.157719019104,39.6809361374745,41.204153255845,42.7273703742155,44.250587492586,45.7738046109565,47.297021729327,48.8202388476975,50.343455966068,51.8666730844385,53.389890202809,54.9131073211795,56.43632443955,57.9595415579205,59.482758676291,61.0059757946615,62.529192913032,64.0524100314025,65.575627149773,67.0988442681435,68.622061386514,70.1452785048845,71.668495623255,73.1917127416255,74.714929859996,76.2381469783665,77.761364096737,79.2845812151076,80.807798333478,82.3310154518485,83.854232570219,85.3774496885895,86.90066680696,88.4238839253305,89.947101043701,91.4703181620715,92.993535280442,94.5167523988125,96.039969517183],[-32.8680360885222,-31.2778664204984,-29.6876967524745,-28.0975270844507,-26.5073574164268,-24.917187748403,-23.3270180803791,-21.7368484123553,-20.1466787443314,-18.5565090763076,-16.9663394082837,-15.3761697402599,-13.786000072236,-12.1958304042122,-10.6056607361883,-9.01549106816449,-7.42532140014063,-5.83515173211674,-4.24498206409291,-2.65481239606908,-1.06464272804524,0.525526939978647,2.11569660800248,3.70586627602634,5.29603594405017,6.88620561207404,8.4763752800979,10.0665449481217,11.6567146161456,13.2468842841694,14.8370539521933,16.4272236202171,18.017393288241,19.6075629562648,21.1977326242887,22.7879022923125,24.3780719603364,25.9682416283602,27.5584112963841,29.148580964408,30.7387506324318,32.3289203004556,33.9190899684795,35.5092596365034,37.0994293045272,38.689598972551,40.2797686405749,41.8699383085988,43.4601079766226,45.0502776446465,46.6404473126703,48.2306169806942,49.820786648718,51.4109563167418,53.0011259847657,54.5912956527896,56.1814653208134,57.7716349888373,59.3618046568611,60.951974324885,62.5421439929088,64.1323136609327,65.7224833289565,67.3126529969804,68.9028226650042,70.4929923330281,72.0831620010519,73.6733316690757,75.2635013370996,76.8536710051235,78.4438406731473,80.0340103411712,81.624180009195,83.2143496772189,84.8045193452427,86.3946890132666,87.9848586812904,89.5750283493143,91.1651980173381,92.755367685362,94.3455373533859,95.9357070214097,97.5258766894335]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Temp"},"yaxis":{"title":"Month"},"zaxis":{"title":"Ozone"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[67,72,74,62,66,65,59,61,74,69,66,68,58,64,66,57,68,62,59,73,61,61,67,81,79,76,82,90,87,82,77,72,65,73,76,84,85,81,83,83,88,92,92,89,73,81,80,81,82,84,87,85,74,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,86,82,80,77,79,76,78,78,77,72,79,81,86,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,75,76,68],"y":[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9],"z":[41,36,12,18,28,23,19,8,7,16,11,14,18,14,34,6,30,11,1,11,4,32,23,45,115,37,29,71,39,23,21,37,20,12,13,135,49,32,64,40,77,97,97,85,10,27,7,48,35,61,79,63,16,80,108,20,52,82,50,64,59,39,9,16,78,35,66,122,89,110,44,28,65,22,59,23,31,44,21,9,45,168,73,76,118,84,85,96,78,73,91,47,32,20,23,21,24,44,21,28,9,13,46,18,13,24,16,13,23,36,7,14,30,14,18,20],"text":["1","2","3","4","6","7","8","9","11","12","13","14","15","16","17","18","19","20","21","22","23","24","28","29","30","31","38","40","41","44","47","48","49","50","51","62","63","64","66","67","68","69","70","71","73","74","76","77","78","79","80","81","82","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","104","105","106","108","109","110","111","112","113","114","116","117","118","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","151","152","153"],"mode":"markers","type":"scatter3d","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Ozone","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.128805265326035","rgba(71,46,123,1)"],["0.184892301923836","rgba(67,63,131,1)"],["0.230582958106785","rgba(61,76,137,1)"],["0.266374974750986","rgba(58,86,139,1)"],["0.30151479141047","rgba(54,96,141,1)"],["0.335586199432184","rgba(49,105,142,1)"],["0.369339889020712","rgba(47,112,142,1)"],["0.404074323034363","rgba(44,121,142,1)"],["0.439345488687855","rgba(39,129,142,1)"],["0.474340560319133","rgba(38,137,141,1)"],["0.508104385092475","rgba(36,146,139,1)"],["0.541649366514187","rgba(33,154,138,1)"],["0.576277093802103","rgba(37,163,134,1)"],["0.609896714599061","rgba(44,170,129,1)"],["0.645295352505824","rgba(50,178,124,1)"],["0.679805766871174","rgba(62,186,118,1)"],["0.713269203973009","rgba(81,192,108,1)"],["0.748776934235017","rgba(98,199,98,1)"],["0.782366915230986","rgba(113,206,88,1)"],["0.817055718406786","rgba(137,211,76,1)"],["0.85202401334191","rgba(159,216,62,1)"],["0.885732440887525","rgba(178,222,46,1)"],["0.922417129474898","rgba(203,225,42,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-0.834420893928069,0.220128376868956,1.274677647666,2.32922691846304,3.38377618926008,4.43832546005713,5.49287473085415,6.54742400165119,7.60197327244825,8.65652254324527,9.71107181404233,10.7656210848394,11.8201703556364,12.8747196264334,13.9292688972305,14.9838181680275,16.0383674388246,17.0929167096216,18.1474659804186,19.2020152512157,20.2565645220127,21.3111137928098,22.3656630636068,23.4202123344039,24.4747616052009,25.5293108759979,26.583860146795,27.638409417592,28.6929586883891,29.7475079591861,30.8020572299831,31.8566065007802,32.9111557715772,33.9657050423742,35.0202543131713,36.0748035839683,37.1293528547654,38.1839021255624,39.2384513963595,40.2930006671565,41.3475499379535,42.4020992087506,43.4566484795476,44.5111977503446,45.5657470211417,46.6202962919387,47.6748455627358,48.7293948335328,49.7839441043298,50.8384933751269,51.8930426459239,52.947591916721,54.002141187518,55.0566904583151,56.1112397291121,57.1657889999091,58.2203382707062,59.2748875415032,60.3294368123002,61.3839860830973,62.4385353538943,63.4930846246914,64.5476338954884,65.6021831662854,66.6567324370825,67.7112817078795,68.7658309786766,69.8203802494736,70.8749295202706,71.9294787910677,72.9840280618647,74.0385773326618,75.0931266034588,76.1476758742559,77.2022251450529,78.2567744158499,79.311323686647,80.365872957444,81.4204222282411,82.4749714990381,83.5295207698351,84.5840700406322,85.6386193114292],[-4.83862279325234,-3.71712097280195,-2.59561915235156,-1.47411733190117,-0.352615511450779,0.768886308999626,1.89038812945,3.01188994990041,4.1333917703508,5.25489359080117,6.37639541125158,7.49789723170197,8.61939905215236,9.74090087260275,10.8624026930531,11.9839045135035,13.1054063339539,14.2269081544043,15.3484099748547,16.4699117953051,17.5914136157555,18.7129154362059,19.8344172566563,20.9559190771067,22.077420897557,23.1989227180075,24.3204245384578,25.4419263589082,26.5634281793586,27.684929999809,28.8064318202594,29.9279336407098,31.0494354611602,32.1709372816106,33.292439102061,34.4139409225114,35.5354427429618,36.6569445634121,37.7784463838625,38.8999482043129,40.0214500247633,41.1429518452137,42.2644536656641,43.3859554861145,44.5074573065649,45.6289591270153,46.7504609474657,47.8719627679161,48.9934645883665,50.1149664088169,51.2364682292672,52.3579700497176,53.479471870168,54.6009736906184,55.7224755110688,56.8439773315192,57.9654791519696,59.08698097242,60.2084827928704,61.3299846133208,62.4514864337712,63.5729882542215,64.6944900746719,65.8159918951223,66.9374937155727,68.0589955360231,69.1804973564735,70.3019991769239,71.4235009973743,72.5450028178247,73.6665046382751,74.7880064587255,75.9095082791759,77.0310100996262,78.1525119200766,79.274013740527,80.3955155609774,81.5170173814278,82.6385192018782,83.7600210223286,84.881522842779,86.0030246632294,87.1245264836798],[-8.8428246925766,-7.65437032247286,-6.46591595236912,-5.27746158226537,-4.08900721216163,-2.90055284205788,-1.71209847195415,-0.523644101850394,0.664810268253348,1.85326463835708,3.04171900846084,4.23017337856457,5.41862774866831,6.60708211877207,7.7955364888758,8.98399085897954,10.1724452290833,11.360899599187,12.5493539692908,13.7378083393945,14.9262627094983,16.114717079602,17.3031714497057,18.4916258198095,19.6800801899132,20.868534560017,22.0569889301207,23.2454433002245,24.4338976703282,25.6223520404319,26.8108064105357,27.9992607806394,29.1877151507432,30.3761695208469,31.5646238909507,32.7530782610544,33.9415326311582,35.1299870012619,36.3184413713656,37.5068957414694,38.6953501115731,39.8838044816769,41.0722588517806,42.2607132218843,43.4491675919881,44.6376219620918,45.8260763321956,47.0145307022993,48.2029850724031,49.3914394425068,50.5798938126105,51.7683481827143,52.956802552818,54.1452569229218,55.3337112930255,56.5221656631292,57.710620033233,58.8990744033368,60.0875287734405,61.2759831435442,62.464437513648,63.6528918837517,64.8413462538555,66.0298006239592,67.2182549940629,68.4067093641667,69.5951637342704,70.7836181043742,71.9720724744779,73.1605268445817,74.3489812146854,75.5374355847892,76.7258899548929,77.9143443249966,79.1027986951004,80.2912530652041,81.4797074353079,82.6681618054116,83.8566161755153,85.0450705456191,86.2335249157228,87.4219792858266,88.6104336559303],[-12.8470265919009,-11.5916196721438,-10.3362127523867,-9.08080583262961,-7.82539891287252,-6.56999199311541,-5.31458507335833,-4.05917815360122,-2.80377123384413,-1.54836431408705,-0.292957394329932,0.962449525427147,2.21785644518425,3.47326336494135,4.72867028469842,5.98407720445553,7.23948412421262,8.49489104396972,9.75029796372682,11.0057048834839,12.261111803241,13.5165187229981,14.7719256427552,16.0273325625123,17.2827394822694,18.5381464020265,19.7935533217836,21.0489602415407,22.3043671612978,23.5597740810548,24.8151810008119,26.070587920569,27.3259948403261,28.5814017600832,29.8368086798403,31.0922155995974,32.3476225193545,33.6030294391116,34.8584363588687,36.1138432786258,37.3692501983829,38.62465711814,39.8800640378971,41.1354709576542,42.3908778774113,43.6462847971683,44.9016917169254,46.1570986366825,47.4125055564396,48.6679124761967,49.9233193959538,51.1787263157109,52.434133235468,53.6895401552251,54.9449470749822,56.2003539947393,57.4557609144964,58.7111678342535,59.9665747540106,61.2219816737677,62.4773885935248,63.7327955132819,64.988202433039,66.243609352796,67.4990162725531,68.7544231923102,70.0098301120673,71.2652370318244,72.5206439515815,73.7760508713386,75.0314577910957,76.2868647108528,77.5422716306099,78.797678550367,80.0530854701241,81.3084923898812,82.5638993096383,83.8193062293954,85.0747131491525,86.3301200689096,87.5855269886666,88.8409339084238,90.0963408281808],[-16.8512284912251,-15.5288690218147,-14.2065095524043,-12.8841500829938,-11.5617906135834,-10.2394311441729,-8.91707167476247,-7.59471220535202,-6.27235273594158,-4.94999326653114,-3.62763379712068,-2.30527432771025,-0.982914858299793,0.33944461111065,1.66180408052109,2.98416354993154,4.30652301934198,5.62888248875244,6.95124195816288,8.27360142757331,9.59596089698377,10.9183203663942,12.2406798358046,13.5630393052151,14.8853987746255,16.207758244036,17.5301177134464,18.8524771828569,20.1748366522673,21.4971961216778,22.8195555910882,24.1419150604987,25.4642745299091,26.7866339993196,28.10899346873,29.4313529381404,30.7537124075509,32.0760718769614,33.3984313463718,34.7207908157822,36.0431502851927,37.3655097546031,38.6878692240136,40.010228693424,41.3325881628345,42.6549476322449,43.9773071016553,45.2996665710658,46.6220260404763,47.9443855098867,49.2667449792971,50.5891044487076,51.911463918118,53.2338233875284,54.5561828569389,55.8785423263494,57.2009017957598,58.5232612651702,59.8456207345807,61.1679802039911,62.4903396734016,63.812699142812,65.1350586122225,66.4574180816329,67.7797775510434,69.1021370204538,70.4244964898643,71.7468559592747,73.0692154286851,74.3915748980956,75.7139343675061,77.0362938369165,78.3586533063269,79.6810127757374,81.0033722451478,82.3257317145583,83.6480911839687,84.9704506533792,86.2928101227896,87.6151695922,88.9375290616105,90.2598885310209,91.5822480004314],[-20.8554303905494,-19.4661183714856,-18.0768063524219,-16.687494333358,-15.2981823142943,-13.9088702952304,-12.5195582761666,-11.1302462571029,-9.74093423803906,-8.35162221897527,-6.96231019991146,-5.57299818084766,-4.18368616178388,-2.79437414272007,-1.40506212365628,-0.0157501045924846,1.37356191447131,2.76287393353512,4.15218595259893,5.54149797166269,6.9308099907265,8.32012200979031,9.70943402885408,11.0987460479179,12.4880580669817,13.8773700860455,15.2666821051093,16.6559941241731,18.0453061432369,19.4346181623007,20.8239301813645,22.2132422004283,23.602554219492,24.9918662385559,26.3811782576197,27.7704902766835,29.1598022957473,30.549114314811,31.9384263338749,33.3277383529386,34.7170503720024,36.1063623910663,37.49567441013,38.8849864291938,40.2742984482576,41.6636104673214,43.0529224863852,44.442234505449,45.8315465245128,47.2208585435766,48.6101705626404,49.9994825817042,51.388794600768,52.7781066198318,54.1674186388956,55.5567306579594,56.9460426770232,58.335354696087,59.7246667151508,61.1139787342146,62.5032907532784,63.8926027723422,65.281914791406,66.6712268104698,68.0605388295336,69.4498508485974,70.8391628676612,72.228474886725,73.6177869057888,75.0070989248525,76.3964109439164,77.7857229629802,79.1750349820439,80.5643470011077,81.9536590201715,83.3429710392353,84.7322830582991,86.1215950773629,87.5109070964267,88.9002191154905,90.2895311345543,91.6788431536181,93.0681551726819],[-24.8596322898737,-23.4033677211565,-21.9471031524394,-20.4908385837222,-19.0345740150051,-17.5783094462879,-16.1220448775708,-14.6657803088537,-13.2095157401365,-11.7532511714194,-10.2969866027022,-8.84072203398506,-7.38445746526793,-5.92819289655077,-4.47192832783361,-3.01566375911645,-1.55939919039932,-0.103134621682187,1.35312994703497,2.80939451575213,4.26565908446929,5.72192365318642,7.17818822190355,8.63445279062071,10.0907173593379,11.546981928055,13.0032464967722,14.4595110654893,15.9157756342065,17.3720402029236,18.8283047716408,20.2845693403579,21.740833909075,23.1970984777922,24.6533630465094,26.1096276152265,27.5658921839436,29.0221567526608,30.4784213213779,31.9346858900951,33.3909504588122,34.8472150275294,36.3034795962465,37.7597441649637,39.2160087336808,40.672273302398,42.1285378711151,43.5848024398323,45.0410670085494,46.4973315772666,47.9535961459837,49.4098607147009,50.866125283418,52.3223898521352,53.7786544208523,55.2349189895695,56.6911835582866,58.1474481270038,59.6037126957209,61.059977264438,62.5162418331552,63.9725064018723,65.4287709705895,66.8850355393066,68.3413001080238,69.7975646767409,71.2538292454581,72.7100938141752,74.1663583828924,75.6226229516095,77.0788875203267,78.5351520890438,79.991416657761,81.4476812264781,82.9039457951953,84.3602103639124,85.8164749326296,87.2727395013467,88.7290040700639,90.185268638781,91.6415332074981,93.0977977762153,94.5540623449325],[-28.8638341891979,-27.3406170708274,-25.817399952457,-24.2941828340865,-22.770965715716,-21.2477485973454,-19.724531478975,-18.2013143606045,-16.6780972422339,-15.1548801238634,-13.631663005493,-12.1084458871225,-10.585228768752,-9.06201165038144,-7.53879453201097,-6.01557741364044,-4.49236029526995,-2.96914317689948,-1.44592605852895,0.0772910598415422,1.60050817821207,3.12372529658253,4.64694241495303,6.17015953332356,7.69337665169402,9.21659377006455,10.739810888435,12.2630280068055,13.786245125176,15.3094622435465,16.8326793619171,18.3558964802876,19.879113598658,21.4023307170285,22.925547835399,24.4487649537695,25.97198207214,27.4951991905105,29.018416308881,30.5416334272515,32.064850545622,33.5880676639925,35.111284782363,36.6345019007335,38.157719019104,39.6809361374745,41.204153255845,42.7273703742155,44.250587492586,45.7738046109565,47.297021729327,48.8202388476975,50.343455966068,51.8666730844385,53.389890202809,54.9131073211795,56.43632443955,57.9595415579205,59.482758676291,61.0059757946615,62.529192913032,64.0524100314025,65.575627149773,67.0988442681435,68.622061386514,70.1452785048845,71.668495623255,73.1917127416255,74.714929859996,76.2381469783665,77.761364096737,79.2845812151076,80.807798333478,82.3310154518485,83.854232570219,85.3774496885895,86.90066680696,88.4238839253305,89.947101043701,91.4703181620715,92.993535280442,94.5167523988125,96.039969517183],[-32.8680360885222,-31.2778664204984,-29.6876967524745,-28.0975270844507,-26.5073574164268,-24.917187748403,-23.3270180803791,-21.7368484123553,-20.1466787443314,-18.5565090763076,-16.9663394082837,-15.3761697402599,-13.786000072236,-12.1958304042122,-10.6056607361883,-9.01549106816449,-7.42532140014063,-5.83515173211674,-4.24498206409291,-2.65481239606908,-1.06464272804524,0.525526939978647,2.11569660800248,3.70586627602634,5.29603594405017,6.88620561207404,8.4763752800979,10.0665449481217,11.6567146161456,13.2468842841694,14.8370539521933,16.4272236202171,18.017393288241,19.6075629562648,21.1977326242887,22.7879022923125,24.3780719603364,25.9682416283602,27.5584112963841,29.148580964408,30.7387506324318,32.3289203004556,33.9190899684795,35.5092596365034,37.0994293045272,38.689598972551,40.2797686405749,41.8699383085988,43.4601079766226,45.0502776446465,46.6404473126703,48.2306169806942,49.820786648718,51.4109563167418,53.0011259847657,54.5912956527896,56.1814653208134,57.7716349888373,59.3618046568611,60.951974324885,62.5421439929088,64.1323136609327,65.7224833289565,67.3126529969804,68.9028226650042,70.4929923330281,72.0831620010519,73.6733316690757,75.2635013370996,76.8536710051235,78.4438406731473,80.0340103411712,81.624180009195,83.2143496772189,84.8045193452427,86.3946890132666,87.9848586812904,89.5750283493143,91.1651980173381,92.755367685362,94.3455373533859,95.9357070214097,97.5258766894335]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":[]}</script>
<p>And here is that same plot as a contour plot.</p>
<pre class="r"><code>air_surface &lt;- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F)
air_surface$Z &lt;- predict.lm(air_lm, newdata = air_surface)
mycolorpalette &lt;- colorRampPalette(c(&quot;skyblue2&quot;, &quot;orange&quot;))
filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27))</code></pre>
</div>
</p>
</div>
<div id="LearnMoreHDModel" class="tabcontent" style="display:none;">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-64-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
  Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_{p-1}X_{p-1,i}}_{E\{Y_i\}}}^\text{&quot;High Dimensional Models&quot;} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p>The so called “HD”, or “High Dimensional”, regression model uses three or more different quantitative x-variables, an <span class="math inline">\(X_{1i}\)</span>, an <span class="math inline">\(X_{2i}\)</span>, and at least an <span class="math inline">\(X_{3i}\)</span>, but could use many, many other variables as well. Unlike the 3D model where the final regression could be shown as either a contour plot or a 3D-graphic, the high dimensional model exists in 4 or more dimensions. Thus, it is impossible to graph this model in its full form. Further, it isn’t really even possible to “mentally connect” with this type of model is it exists beyond what our 3D minds can really comprehend.</p>
<table style="width:100%;">
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_1\)</span> direction.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_2\)</span> direction.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(...\)</span></td>
<td>Slopes in other directions depending on how many other variables are included in the model.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_{p-1}\)</span></td>
<td>Final term in the model where there are <span class="math inline">\(p\)</span> total <span class="math inline">\(\beta\)</span>’s. The reason for the <span class="math inline">\(p-1\)</span> on the last term is because we started with <span class="math inline">\(\beta_0\)</span> for the first term, leaving <span class="math inline">\(\beta_{p-1}\)</span> as the last term.</td>
</tr>
</tbody>
</table>
<p><span style="padding-left:15px;"><a href="javascript:showhide('hdmodelexample')" style="font-size:1.1em;color:skyblue;">(Show Example…)</a></span></p>
<div id="hdmodelexample" style="display:none;">
<p><strong>An Example</strong></p>
<p>Suppose we used three x-variables of <code>Wind</code>, <code>Temp</code>, and <code>Solar.R</code> to predict the y-variable of <code>Ozone</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Ozone} \underbrace{=}_{\sim} \overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} + \overbrace{\beta_1}^{\stackrel{\text{slope in}}{\text{Wind Direction}}} \underbrace{X_{1i}}_\text{Wind} + \overbrace{\beta_2}^{\stackrel{\text{slope in}}{\text{Temp Direction}}}  \underbrace{X_{2i}}_\text{Temp} + \overbrace{\beta_3}^{\stackrel{\text{slope in}}{\text{Solar.R Direction}}}  \underbrace{X_{3i}}_\text{Solar.R} + \epsilon_i
\]</span></p>
<pre class="r"><code>air_lm &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality)
pander(air_lm$coefficients)</code></pre>
<table style="width:56%;">
<colgroup>
<col width="19%" />
<col width="12%" />
<col width="11%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">(Intercept)</th>
<th align="center">Wind</th>
<th align="center">Temp</th>
<th align="center">Solar.R</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-64.34</td>
<td align="center">-3.334</td>
<td align="center">1.652</td>
<td align="center">0.05982</td>
</tr>
</tbody>
</table>
<p>Notice how the slope, <span class="math inline">\(\beta_1\)</span>, in the “Wind” direction is estimated to be -3.334. The slope in the “Temp” direction, <span class="math inline">\(\beta_2\)</span>, is estimated to be 1.652. The slope in the “Solar.R” direction, <span class="math inline">\(\beta_3\)</span>, is estimated to be 0.05982. Also, the y-intercept, <span class="math inline">\(\beta_0\)</span>, is estimated to be -64.34.</p>
<p>Visualizing this model is not really possible in its full form. However, we can draw the regression from three different angles or vantage points. This is a limited view of the full regression model, but at least provides some visual understanding. To do this, we draw <span class="math inline">\(Y\)</span> against each <span class="math inline">\(X\)</span>-variable in separate scatterplots, one for each <span class="math inline">\(X\)</span>-variable used in our model.</p>
<pre class="r"><code>b &lt;- coef(air_lm)

par(mfrow=c(1,3))

  plot(Ozone ~ Wind, data=airquality)
  curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=&quot;skyblue&quot;)
  # The x-variable of this plot is &quot;Wind&quot;
  # The values of Temp=79 and Solar.R=205 are fixed at some interesting value,
  # in this case, their respective medians.

  plot(Ozone ~ Temp, data=airquality)
  curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=&quot;orange&quot;)
  # The x-variable of this plot is &quot;Temp&quot;
  # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value,
  # in this case, their respective medians.
  
  plot(Ozone ~ Solar.R, data=airquality)
  curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<pre class="r"><code>  # The x-variable of this plot is &quot;Solar.R&quot;
  # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value,
  # in this case, their respective medians.</code></pre>
</div>
</p>
</div>
<hr />
<p>The coefficient <span class="math inline">\(\beta_j\)</span> is interpreted as the change in the expected value of <span class="math inline">\(Y\)</span> for a unit increase in <span class="math inline">\(X_{j}\)</span>, holding all other variables constant, for <span class="math inline">\(j=1,\ldots,p-1\)</span>.</p>
<p>See the <strong>Explanation</strong> tab for details about possible hypotheses here.</p>
<hr />
</div>
</div>
<div id="r-instructions-1" class="section level3">
<h3>R Instructions</h3>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p><strong>Finding Variables</strong></p>
<a href="javascript:showhide('PairsPlot')">
<div class="hoverchunk">
<p><span class="tooltipr"> pairs( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> cbind( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> Res =  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> mylm$ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> residuals,  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> YourDataSet), <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr">  panel =  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> panel.smooth,  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> col =  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> as.factor( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> YourDataSet$ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> X)) <span class="tooltiprtext">EXPLANATION.</span> </span></p>
</div>
<p></a></p>
<div id="PairsPlot" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
</div>
<p><strong>Perform the Regression</strong></p>
<p>Everything is the same as in simple linear regression except that more variables are allowed in the call to <code>lm()</code>.</p>
<a href="javascript:showhide('multiplelm')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm &lt;- lm( <span class="tooltiprtext"><code>mylm</code> is some name you come up with to store the results of the <code>lm()</code> test. Note that <code>lm()</code> stands for “linear model.”</span> </span><span class="tooltipr"> Y <span class="tooltiprtext"><code>Y</code> must be a “numeric” vector of the quantitative response variable.</span> </span><span class="tooltipr">  ~  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> X1 + X2 <span class="tooltiprtext"><code>X1</code> and <code>X2</code> are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. R will automatcially recode qualitative variables to become “numeric” variables using a 0,1 encoding. See the Explanation tab for details.</span> </span><span class="tooltipr">  + X1:X2 <span class="tooltiprtext"><code>X1:X2</code> is called the interaction term. See the Explanation tab for details.</span> </span><span class="tooltipr">  + …, <span class="tooltiprtext">* <code>...</code> emphasizes that as many explanatory variables as are desired can be included in the model.</span> </span><span class="tooltipr">  data = YourDataSet) <span class="tooltiprtext"><code>YourDataSet</code> is the name of your data set.</span> </span><br/><span class="tooltipr"> summary( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> mylm <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for summary(…) function.</span> </span></p>
</div>
<p></a></p>
<div id="multiplelm" style="display:none;">
<p>Example output from a regression. Hover each piece to learn more.</p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Call:<br/> lm(formula = mpg ~ hp + am + hp:am, data = mtcars) <span class="tooltiprouttext">This is simply a statement of your original lm(…) “call” that you made when performing your regression. It allows you to verify that you ran what you thought you ran in the lm(…).</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Residuals: <span class="tooltiprouttext">Residuals are the vertical difference between each point and the line, <span class="math inline">\(Y_i - \hat{Y}_i\)</span>. The residuals are supposed to be normally distributed, so a quick glance at their five-number summary can give us insight about any skew present in the residuals. </span>
</td>
</tr>
<tr>
<td align="right">
<span class="tooltiprout"> min<br/>   -4.3818 <span class="tooltiprouttext">“min” gives the value of the residual that is furthest below the regression line. Ideally, the magnitude of this value would be about equal to the magnitude of the largest positive residual (the max) because the hope is that the residuals are normally distributed around the line.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1Q<br/>   -2.2696 <span class="tooltiprouttext">“1Q” gives the first quartile of the residuals, which will always be negative, and ideally would be about equal in magnitude to the third quartile.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Median<br/>   0.1344 <span class="tooltiprouttext">“Median” gives the median of the residuals, which would ideally would be about equal to zero. Note that because the regression line is the least squares line, the mean of the residuals will ALWAYS be zero, so it is never included in the output summary. This particular median value of -0.0191 is a little smaller than zero than we would hope for and suggests a right skew in the data because the mean (0) is greater than the median (-0.0191) witnessing the residuals are right skewed. This can also be seen in the maximum being much larger in magnitude than the minimum.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3Q<br/>   1.7058 <span class="tooltiprouttext">“3Q” gives the third quartile of the residuals, which would ideally would be about equal in magnitude to the first quartile. In this case, it is pretty close, which helps us see that the first quartile of residuals on either side of the line is behaving fairly normally.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Max</br>   5.8752 <span class="tooltiprouttext">“Max” gives the maximum positive residuals, which would ideally would be about equal in magnitude to the minimum residual. In this case, it is much larger than the minimum, which helps us see that the residuals are likely right skewed.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Coefficients: <span class="tooltiprouttext">Notice that in your lm(…) you used only <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. You did type out any coefficients, i.e., the <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span> of the regression model. These coefficients are estimated by the lm(…) function and displayed in this part of the output along with standard errors, t-values, and p-values.</span> </span>
</td>
</tr>
<tr>
<td align="left">
</td>
<td align="right">
<span class="tooltiprout">   Estimate <span class="tooltiprouttext">To learn more about the “Estimates” of the “Coefficients” see the “Explanation” tab, “Estimating the Model Parameters” section for details.</span>
</td>
<td align="right">
<span class="tooltiprout">   Std. Error <span class="tooltiprouttext">To learn more about the “Standard Errors” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   t value <span class="tooltiprouttext">To learn more about the “t value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   Pr(&gt;|t|) <span class="tooltiprouttext">The “Pr” stands for “Probability” and the “(&gt; |t|)” stands for “more extreme than the observed t-value”. Thus, this is the p-value for the hypothesis test of each coefficient being zero.<br/> To learn more about the “p-value” of the “Coefficients” see the “Explanation” tab, “Inference for the Model Parameters” section. </span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span class="tooltiprouttext">This always says “Intercept” for any lm(…) you run in R. That is because R always assumes there is a y-intercept for your regression function.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   26.6248479 <span class="tooltiprouttext">This is the estimate of the y-intercept, <span class="math inline">\(\beta_0\)</span>. It is called <span class="math inline">\(b_0\)</span>. It is the average y-value when X is zero.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   2.1829432 <span class="tooltiprouttext">This is the standard error of <span class="math inline">\(b_0\)</span>. It tells you how much <span class="math inline">\(b_0\)</span> varies from sample to sample. The closer to zero, the better.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 12.197 <span class="tooltiprouttext">This is the test statistic t for the test of <span class="math inline">\(\beta_0 = 0\)</span>. It is calculated by dividing the “Estimate” of the intercept (30.8735) by its standard error (3.1882). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of 30.8735 is 3.1882 standard errors (9.684) from zero, which is a fairly surprising distance as shown by the p-value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1.01e-12 <span class="tooltiprouttext">This is the p-value of the test of the hypothesis that <span class="math inline">\(\beta_0 = 0\)</span>. It measures the probability of observing a t-value as extreme as the one observed. To compute it yourself in R, use <code>pt(-abs(your t-value), df of your regression)*2</code>.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> *** <span class="tooltiprouttext">This is called a “star”. Three stars means significant at the 0 level of <span class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> hp <span class="tooltiprouttext">This is always the name of your X-variable in your lm(Y ~ X, …).</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   -0.0591370 <span class="tooltiprouttext">This is the estimate of the slope, <span class="math inline">\(\beta_1\)</span>. It is called <span class="math inline">\(b_1\)</span>. It is the change in the average y-value as X is increased by 1 unit.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.0129449 <span class="tooltiprouttext">This is the standard error of <span class="math inline">\(b_1\)</span>. It tells you how much <span class="math inline">\(b_1\)</span> varies from sample to sample. The closer to zero, the better.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> -4.568 <span class="tooltiprouttext">This is the test statistic t for the test of <span class="math inline">\(\beta_1 = 0\)</span>. It is calculated by dividing the “Estimate” of the slope (-1.9757) by its standard error (0.4485). It gives the “number of standard errors” away from zero that the “estimate” has landed. In this case, the estimate of -1.9757 is -4.405 standard errors (0.4485) from zero, which is a really surprising distance as shown by the smallness of the p-value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 9.02e-05 <span class="tooltiprouttext">This is the p-value of the test of the hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>. To compute it yourself in R, use <code>pt(-abs(your t-value), df of your regression)*2</code></span> </span>
</td>
<td align="left">
<span class="tooltiprout"> *** <span class="tooltiprouttext">This is called a “star”. Three stars means significant at the 0.01 level of <span class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> am <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   5.2176534 <span class="tooltiprouttext">EXPLANATION</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   2.6650931 <span class="tooltiprouttext">EXPLANATION</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1.958 <span class="tooltiprouttext">EXPLANATION</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.0603 <span class="tooltiprouttext">EXPLANATION</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> . <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> hp:am <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.0004029 <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.0164602 <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.024 <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.9806 <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span> --- </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘’ 1 <span class="tooltiprouttext">These “codes” explain what significance level the p-value is smaller than based on how many “stars” * the p-value is labeled with in the Coefficients table above.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Residual standard error: <span class="tooltiprouttext">This is the estimate of <span class="math inline">\(\sigma\)</span> in the regression model <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)</span> where <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>. It is the square root of the MSE.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  2.939 <span class="tooltiprouttext">For this particular regression, the estimate of <span class="math inline">\(\sigma\)</span> is 2.939. Squaring this number gives you the MSE, which is the estimate of <span class="math inline">\(\sigma^2\)</span>.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 28 degrees of freedom <span class="tooltiprouttext">This is <span class="math inline">\(n-p\)</span> where <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(p\)</span> is the number of parameters in the regression model. In this case, there is a sample size of 32 and two parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, so 32-4 = 28.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Multiple R-squared: <span class="tooltiprouttext">This is <span class="math inline">\(R^2\)</span>, the percentage of variation in <span class="math inline">\(Y\)</span> that is explained by the regression model. It is equal to the SSR/SSTO or, equivalently, 1 - SSE/SSTO.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.7852, <span class="tooltiprouttext">In this particular regression, 78.52% of the variation in stopping distance <code>dist</code> is explained by the regression model using speed of the car.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  Adjusted R-squared: <span class="tooltiprouttext">The adjusted R-squared will always be at least slightly smaller than <span class="math inline">\(R^2\)</span>. The closer to R-squared that it is, the better. When it differs dramatically from <span class="math inline">\(R^2\)</span>, it is a sign that the regression model is over-fitting the data.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.7621 <span class="tooltiprouttext">In this case, the value of 0.7621 is quite close to the original <span class="math inline">\(R^2\)</span> value, so there is no fear of over-fitting with this particular model. That is good.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> F-statistic: <span class="tooltiprouttext">EXPLANATION</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  34.11 <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 3 and 28 DF, <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  p-value: 1.73e-09 <span class="tooltiprouttext">EXPLANATION.</span> </span>
</td>
</tr>
</table>
</div>
<p><br/></p>
<p><strong>Plotting the Regression Lines</strong></p>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'BaseMultiplelm')">
Base R
</button>
<button class="tablinks" onclick="openTab(event, 'ggplotMultiplelm')">
ggplot2
</button>
</div>
<div id="BaseMultiplelm" class="tabcontent">
<p>
<p>To add two regression lines to a scatterplot use two <code>abline(...)</code> commands:</p>
<a href="javascript:showhide('multipleregression')">
<div class="hoverchunk">
<p><span class="tooltipr"> plot( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> Y <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr">  ~  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> <span class="math inline">\(X_1\)</span>, <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> col = <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> as.factor( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>), <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> data = YourDataSet) <span class="tooltiprtext">EXPLANATION.</span> </span><br/><span class="tooltipr"> b <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr">  &lt;-  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> mylm <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> $ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> coefficients <span class="tooltiprtext">EXPLANATION.</span> </span><br/><span class="tooltipr"> abline( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[1], <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[2]) <span class="tooltiprtext">EXPLANATION.</span> </span><br/><span class="tooltipr"> abline( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> (b[1]+ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[3]),  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> (b[2]+ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[4]),  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> col = <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr">  “red”) <span class="tooltiprtext">EXPLANATION.</span> </span></p>
</div>
<p></a></p>
<div id="multipleregression" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
</div>
<p>Customize the look:</p>
<a href="javascript:showhide('multipleregressionCustom')">
<div class="hoverchunk">
<p><span class="tooltipr"> b <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr">  &lt;-  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> mylm <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> $ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> coefficients <span class="tooltiprtext">EXPLANATION.</span> </span><br/><span class="tooltipr"> palette( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> c( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> “SomeColor”, <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> “DifferentColor”)) <span class="tooltiprtext">EXPLANATION.</span> </span><br/><span class="tooltipr"> plot( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> Y <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr">  ~  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> <span class="math inline">\(X_1\)</span>, <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> col = <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> as.factor( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>), <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> data = YourDataSet, <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> pch = 16) <span class="tooltiprtext">EXPLANATION.</span> </span><br/><span class="tooltipr"> abline( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[1], <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[2], <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> col = palette()[1], <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> lty = 1, <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> lwd = 1) <span class="tooltiprtext">EXPLANATION.</span> </span><br/><span class="tooltipr"> abline( <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> (b[1]+ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[3]),  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> (b[2]+ <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> b[4]),  <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> col = palette()[2], <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> lty = 2, <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> lwd = 2) <span class="tooltiprtext">EXPLANATION.</span> </span></p>
</div>
<p></a></p>
<div id="multipleregressionCustom" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
</div>
</p>
</div>
<div id="ggplotMultiplelm" class="tabcontent">
<p>
<a href="javascript:showhide('ggplotMultipleRegression')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">ggplot() provides a framework which you add layers to. Layers usually take the form of different <em>geoms</em>, like geom_point() or geom_boxplot(). Running ggplot() without any layers produces simply a blank graph.</span> </span><span class="tooltipr"> data =  <span class="tooltiprtext">“Data =” tells ggplot() that you are going to declare the data set that you will use in the graph.</span> </span><span class="tooltipr"> YourDataSet, <span class="tooltiprtext"><em>YourDataSet</em> is literally the name of the data set that you want to use to make the graph, like mtcars or KidsFeet.</span> </span><span class="tooltipr">  aes( <span class="tooltiprtext">aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the framework, like x and y variables.</span> </span><span class="tooltipr"> x =  <span class="tooltiprtext">“x =” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x =” and “y =” are optional phrases in the ggplot2 syntax.</span> </span><span class="tooltipr"> <span class="math inline">\(X_{1}\)</span>, <span class="tooltiprtext"><span class="math inline">\(X_{1}\)</span> is an explanatory variable of the regression, and is typically quantitative or numeric. It is the name of a “numeric” column of YourDataSet.</span> </span><span class="tooltipr">  y =  <span class="tooltiprtext">“y =” declares which variable will become the y-axis of the graphic.</span> </span><span class="tooltipr"> <span class="math inline">\(Y\)</span>, <span class="tooltiprtext"><span class="math inline">\(Y\)</span> is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet.</span> </span><span class="tooltipr">  color = <span class="tooltiprtext">“color =” tells ggplot() that you are about to declare the way in which you will color the elements in the graph. Notice that, in this case, the color argument goes inside of the aesthetic. This is because you are coloring by a variable.</span> </span><span class="tooltipr">  factor( <span class="tooltiprtext">Using factor() tells R to treat qualitative variables as factors, where each unique value or category of the variable is a level.</span> </span><span class="tooltipr"> <span class="math inline">\(X_{2}\)</span> <span class="tooltiprtext"><span class="math inline">\(X_{2}\)</span> is a qualitative explanatory variable of the regression. It is the name of a column of YourDataSet. Casting <span class="math inline">\(X_{2}\)</span> as a factor() </span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of factor().</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of aes().</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of ggplot().</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.</span> </span><br/><span class="tooltipr">   geom_point() <span class="tooltiprtext">geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">Here the + is used to add yet another layer to ggplot().</span> </span><br/><span class="tooltipr">   geom_smooth( <span class="tooltiprtext">geom_smooth() is a smoothing function that you can use to add lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot.</span> </span><span class="tooltipr"> method = <span class="tooltiprtext">Use “method =” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line(s) or curve(s).</span> </span><span class="tooltipr">  “lm,” <span class="tooltiprtext">lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line (or multiple lines) to the data. In this case, the regression lines are modeled using <span class="math inline">\(Y\)</span> ~ <span class="math inline">\(X_1\)</span> + <span class="math inline">\(X_2\)</span>, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.</span> </span><span class="tooltipr">  se = FALSE <span class="tooltiprtext">se stands for “standard error.” Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for geom_smooth().</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="ggplotMultipleRegression" style="display:none;">
<pre><code>ggplot(data = mtcars, aes(x = hp, y = mpg, color = factor(am))) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
</div>
<p>Customize the look:</p>
<a href="javascript:showhide('ggplotMultipleCustom')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">ggplot() provides a framework which you add layers to. Layers usually take the form of different <em>geoms</em>, like geom_point() or geom_boxplot(). Running ggplot() without any layers produces simply a blank graph.</span> </span><span class="tooltipr"> data =  <span class="tooltiprtext">“Data =” tells ggplot() that you are going to declare the data set that you will use in the graph.</span> </span><span class="tooltipr"> YourDataSet, <span class="tooltiprtext"><em>YourDataSet</em> is literally the name of the data set that you want to use to make the graph, like mtcars or KidsFeet.</span> </span><span class="tooltipr">  aes( <span class="tooltiprtext">aes stands for aesthetic. Inside of aes(), you place elements that you want to map to the framework, like x and y variables.</span> </span><span class="tooltipr"> x =  <span class="tooltiprtext">“x =” declares which variable will become the x-axis of the graphic, your explanatory variable. Both “x =” and “y =” are optional phrases in the ggplot2 syntax.</span> </span><span class="tooltipr"> <span class="math inline">\(X_{1}\)</span>, <span class="tooltiprtext"><span class="math inline">\(X_{1}\)</span> is an explanatory variable of the regression, and is typically quantitative or numeric. It is the name of a “numeric” column of YourDataSet.</span> </span><span class="tooltipr">  y =  <span class="tooltiprtext">“y =” declares which variable will become the y-axis of the graphic.</span> </span><span class="tooltipr"> <span class="math inline">\(Y\)</span>, <span class="tooltiprtext"><span class="math inline">\(Y\)</span> is the response variable of the regression: the variable that you are interested in predicting. It is the name of a “numeric” column of YourDataSet.</span> </span><span class="tooltipr">  color = <span class="tooltiprtext">“color =” tells ggplot() that you are about to declare the way in which you will color the elements in the graph. Notice that, in this case, the color argument goes inside of the aesthetic. This is because you are coloring by a variable.</span> </span><span class="tooltipr">  factor( <span class="tooltiprtext">Using factor() tells R to treat qualitative variables as factors, where each unique value or category of the variable is a level.</span> </span><span class="tooltipr"> <span class="math inline">\(X_{2}\)</span> <span class="tooltiprtext"><span class="math inline">\(X_{2}\)</span> is a qualitative explanatory variable of the regression. It is the name of a column of YourDataSet. Casting <span class="math inline">\(X_{2}\)</span> as a factor() </span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of factor().</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of aes().</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of ggplot().</span> </span><span class="tooltipr">  + <span class="tooltiprtext">The + allows you to add more layers to the framework provided by ggplot(). In this case, you use + to add a geom_point() layer on the next line.</span> </span><br/><span class="tooltipr">   geom_point() <span class="tooltiprtext">geom_point() allows you to add a layer of points, a scatterplot, over the ggplot() framework. The x and y coordinates are received from the previously specified x and y variables declared in the ggplot() aesthetic.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">Here the + is used to add yet another layer to ggplot().</span> </span><br/><span class="tooltipr">   geom_smooth( <span class="tooltiprtext">geom_smooth() is a smoothing function that you can use to add lines or curves to ggplot(). In this case, you will use it to add the least-squares regression line to the scatterplot.</span> </span><span class="tooltipr"> method = <span class="tooltiprtext">Use “method =” to tell geom_smooth() that you are going to declare a specific smoothing function, or method, to alter the line(s) or curve(s).</span> </span><span class="tooltipr">  “lm,” <span class="tooltiprtext">lm stands for linear model. Using method = “lm” tells geom_smooth() to fit a least-squares regression line (or multiple lines) to the data. In this case, the regression lines are modeled using <span class="math inline">\(Y\)</span> ~ <span class="math inline">\(X_1\)</span> + <span class="math inline">\(X_2\)</span>, which variables were declared in the initial ggplot() aesthetic. There are several other methods that could be used here.</span> </span><span class="tooltipr">  se = FALSE <span class="tooltiprtext">se stands for “standard error.” Specifying FALSE turns this feature off. When TRUE, a gray band showing the “confidence band” for the regression is shown. Unless you know how to interpret this confidence band, leave it turned off.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for geom_smooth().</span> </span><span class="tooltipr">  + <span class="tooltiprtext">Here the + is used to add yet another layer to ggplot().</span> </span><br><span class="tooltipr">   scale_color_manual( <span class="tooltiprtext">This function allows you to override the default ggplot colors and specify your own colors.</span> </span><span class="tooltipr"> values = <span class="tooltiprtext">This tells scale_color_manual() that you are about to declare the values for the custom colors.</span> </span><span class="tooltipr">  c( <span class="tooltiprtext">This is the combine, or “backpack” function, that combines values into a vector or list. Put the necessary number of color values in c().</span> </span><span class="tooltipr"> “color1”, “color2” <span class="tooltiprtext">These are literally the colors that you want to show up in your plot, like “skyblue” or “orange.”</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of the c() function.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of the scale_color_manual() function.</span> </span><span class="tooltipr">  + <span class="tooltiprtext">Here the + is used to add yet another layer to ggplot().</span> </span><br><span class="tooltipr">   labs( <span class="tooltiprtext">labs stands for labels. Modify the labels on your graph with the labs() function.</span> </span><span class="tooltipr"> title = “YourTitle”, <span class="tooltiprtext">Declare the title with <em>title =</em> and then write the title in quotes.</span> </span><br><span class="tooltipr">     subtitle = “YourSubtitle”, <span class="tooltiprtext">Declare a subtitle with <em>subtitle =</em> and then write the subtitle in quotes.</span> </span><br><span class="tooltipr">     caption = “Caption”, <span class="tooltiprtext">Declare a caption with <em>caption =</em> and then write the caption in quotes. By default, captions appear at the bottom right of the graphic.</span> </span><br><span class="tooltipr">     x = “X”, <span class="tooltiprtext">Use <em>x =</em> to declare the label for the x-axis of your graph. “X” is the actual label that you want to appear on the x-axis.</span> </span><br><span class="tooltipr">     y = “Y”, <span class="tooltiprtext">Use <em>y =</em> to declare the label for the y-axis of your graph. “Y” is the actual label that you want to appear on the y-axis.</span> </span><br><span class="tooltipr">     color = “Legend Title” <span class="tooltiprtext">Use <em>color =</em> to tell labs() that you are going modify the color legend title.</span> </span><span class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for labs().</span> </span><span class="tooltipr">  + <span class="tooltiprtext">Here the + is used to add yet another layer to ggplot().</span> </span><br><span class="tooltipr">   theme_bw() <span class="tooltiprtext">Add a theme to ggplot().</span> </span><span class="tooltipr" style="float:right;font-size:.8em;">  Click to Show Output  <span class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="ggplotMultipleCustom" style="display:none;">
<pre><code>ggplot(data = mtcars, aes(x = hp, y = mpg, color = factor(am))) +
  geom_point(size = 2) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  scale_color_manual(values = c(&quot;skyblue&quot;, &quot;orange&quot;)) +
  labs(title = &quot;Title&quot;,
       subtitle = &quot;Subtitle&quot;,
       caption = &quot;Caption&quot;,
       x = &quot;X&quot;,
       y = &quot;Y&quot;,
       color = &quot;Legend Title&quot;
       ) +
  theme_bw()</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
</div>
</p>
</div>
<p><br/></p>
<p><strong>Making Predictions</strong></p>
<a href="javascript:showhide('predict2')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a previously performed lm(…) that was saved into the name <code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr"> data.frame( <span class="tooltiprtext">To specify the values of <span class="math inline">\(x\)</span> that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…).</span> </span><span class="tooltipr"> <span class="math inline">\(X_1\)</span>= <span class="tooltiprtext">The value for <code>X=</code> should be whatever x-variable name was used in the original regression. For example, if <code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data=mtcars)</code> was the original regression, then this code would read <code>hp =</code> instead of <code>X1 =</code>… Further, the value of <span class="math inline">\(X_{1h}\)</span> should be some specific number, like <code>hp=123</code> for example.</span> </span><span class="tooltipr"> <span class="math inline">\(X_{1h}\)</span>, <span class="tooltiprtext">The value of <span class="math inline">\(X_{1h}\)</span> should be some specific number, like <code>123</code>, as in <code>hp=123</code> for example.</span> </span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>= <span class="tooltiprtext">This is the value of the second x-variable, say <code>am</code>.</span> </span><span class="tooltipr"> <span class="math inline">\(X_{2h}\)</span>), <span class="tooltiprtext">Since the <code>am</code> column can only be a 1 or 0, we would try <code>am=1</code> for example, or <code>am=0</code>.</span> </span><span class="tooltipr"> type = “response”) <span class="tooltiprtext">Closing parenthesis.</span> </span></p>
</div>
<p></a></p>
<div id="predict2" style="display:none;">
<p><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)</code></p>
<p><code>predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = &quot;response&quot;)</code></p>
<pre><code>##        1 
## 24.79441</code></pre>
<p>The value given is the “fitted-value” or “predicted-value” for the specified x-value. In this case, a car with a speed of 12 is predicted to have a stopping distance of 29.60981 feet.</p>
</div>
<a href="javascript:showhide('predict2Interval')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a previously performed lm(…) that was saved into the name <code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr"> data.frame( <span class="tooltiprtext">To specify the values of <span class="math inline">\(x\)</span> that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…).</span> </span><span class="tooltipr"> <span class="math inline">\(X_1\)</span>= <span class="tooltiprtext">The value for <code>X=</code> should be whatever x-variable name was used in the original regression. For example, if <code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original regression, then this code would read <code>speed =</code> instead of <code>X=</code>… Further, the value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>speed=12</code> for example.</span> </span><span class="tooltipr"> <span class="math inline">\(X_1\)</span>h, <span class="tooltiprtext">The value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>12</code>, as in <code>speed=12</code> for example.</span> </span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>= <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>h), <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> interval = “prediction”) <span class="tooltiprtext">EXPLANATION.</span> </span></p>
</div>
<p></a></p>
<div id="predict2Interval" style="display:none;">
<p><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)</code></p>
<p><code>predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = &quot;prediction&quot;)</code></p>
<pre class="r"><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)
predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = &quot;prediction&quot;)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 24.79441 18.49923 31.08959</code></pre>
<p>The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound.</p>
<p>In this case, a car with a speed of 12 mph is predicted to have a stopping distance of 29.60981 feet. However, we are wise enough to recognize that the stopping distance for individual cars will vary anywhere from -1.749529 (or 0 because distance can’t go negative) feet to 60.96915 feet.</p>
</div>
<a href="javascript:showhide('predict2Confidence')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R function predict(…) allows you to use an lm(…) object to make predictions for specified x-values.</span> </span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a previously performed lm(…) that was saved into the name <code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr"> data.frame( <span class="tooltiprtext">To specify the values of <span class="math inline">\(x\)</span> that you want to use in the prediction, you have to put those x-values into a data set, or more specifally, a data.frame(…).</span> </span><span class="tooltipr"> <span class="math inline">\(X_1\)</span>= <span class="tooltiprtext">The value for <code>X=</code> should be whatever x-variable name was used in the original regression. For example, if <code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original regression, then this code would read <code>speed =</code> instead of <code>X=</code>… Further, the value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>speed=12</code> for example.</span> </span><span class="tooltipr"> <span class="math inline">\(X_1\)</span>h, <span class="tooltiprtext">The value of <span class="math inline">\(Xh\)</span> should be some specific number, like <code>12</code>, as in <code>speed=12</code> for example.</span> </span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>= <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>h), <span class="tooltiprtext">EXPLANATION.</span> </span><span class="tooltipr"> interval = “confidence”) <span class="tooltiprtext">EXPLANATION.</span> </span></p>
</div>
<p></a></p>
<div id="predict2Confidence" style="display:none;">
<p><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)</code></p>
<p><code>predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = &quot;confidence&quot;)</code></p>
<pre class="r"><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)
predict(mylm, data.frame(hp = 120, am = 1), interval = &quot;confidence&quot;)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 24.79441 23.10635 26.48247</code></pre>
<p>The “fit” is the predicted value. The “lwr” is the lower bound. The “upr” is the upper bound.</p>
<p>In this case, cars with a speed of 12 mph are predicted to have an average stopping distance of 29.60981 feet, where the average could be anywhere from 24.39514 feet to 34.82448 feet.</p>
</div>
<hr />
</div>
</div>
<div id="explanation-1" class="section level3">
<h3>Explanation</h3>
<div style="padding-left:125px;">
<h4 id="assessing-the-model-fit-expand">Assessing the Model Fit <a href="javascript:showhide('assessingFit2')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, AIC, BIC…</span></p>
<div id="assessingFit2" style="display:none;">
<p>There are many measures of the quality of a regression model. One of the most popular measurements is the <span class="math inline">\(R^2\)</span> value (“R-squared”). The <span class="math inline">\(R^2\)</span> value is a measure of the proportion of variation of the <span class="math inline">\(Y\)</span>-variable that is explained by the model. Specifically, <span class="math display">\[
  R^2 = \frac{\text{SSR}}{\text{SSTO}} = 1-\frac{\text{SSE}}{\text{SSTO}}
\]</span> The range of <span class="math inline">\(R^2\)</span> is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model.</p>
<p>One difficulty of <span class="math inline">\(R^2\)</span> in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the <span class="math inline">\(R^2\)</span> value to protect against this difficulty. The value of the adjusted <span class="math inline">\(R^2\)</span> is given by <span class="math display">\[
  R^2_{adj} = 1 - \frac{(n-1)}{(n-p)}\frac{\text{SSE}}{\text{SSTO}}
\]</span> The interpretation of <span class="math inline">\(R^2_{adj}\)</span> is essentially the same as the interpretation of <span class="math inline">\(R^2\)</span>, with the understanding that a correction has been made for the number of parameters included in the model, <span class="math inline">\((n-p)\)</span>.</p>
<p>Consider the models below. The value of <span class="math inline">\(R^2\)</span> always gets higher as the model adds more parameters. However, the value of <span class="math inline">\(R^2_{adj}\)</span> sometimes goes down, emphasizing the idea that the model is becoming more complex than needed to capture the pattern in Y.</p>
<pre class="r"><code>par(mfrow=c(1,5), mai=c(0,.1,.4,.1))
plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Simple Linear&quot;)
lm1 &lt;- lm(dist ~ speed, data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Quadratic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Cubic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Quartic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Quintic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-75-1.png" width="672" /></p>
<p><br/></p>
<p>The “simplest” but “best” model of those shown above would be the Quadratic. This is because it has the best <span class="math inline">\(R^2_{adj}\)</span> (0.653) other than the far more complicated Quartic model (0.655). But the <span class="math inline">\(R^2_{adj}\)</span> for the Quadratic model is a good improvement over that of the <span class="math inline">\(R^2_{adj}\)</span> for the Simple Linear model, with a value of 0.653 compared to 0.644, respectively. So moving to the complexity of the Quadratic model is justified over the Simple Linear Model. But there is not enough of an improvement in the <span class="math inline">\(R^2_{adj}\)</span> to warrant moving to the complexity of the Quartic Model. Further, the pattern in the Quadratic seems to generalize better to data outside the range of the current data than does the Quartic model.</p>
<p><span class="math display">\[
  \text{\emph{Quadratic Model}:}\quad Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \epsilon_i
\]</span></p>
<p><span class="math display">\[
  \text{\emph{Quartic Model}:}\quad Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \underbrace{\beta_3 X_i^3 + \beta_4 X_i^4}_\text{Cubic and Quartic Terms} + \epsilon_i
\]</span></p>
<p><strong>AIC and BIC</strong></p>
<p>Two other measurements, or information criterion, are popular for use in the model selection process. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are easily computed in R using <code>AIC(yourlm)</code> and <code>BIC(yourlm)</code>.</p>
</div>
<p><br/></p>
<h4 id="model-validation-expand">Model Validation <a href="javascript:showhide('validation')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">Verifying a model’s ability to generalize to new data…</span></p>
<div id="validation" style="display:none;">
<p>The following graph shows three things: (1) a true regression model, (2) a simple linear regression model that doesn’t quite capture the full pattern in the data, and (3) a complicated model that seems to overly fit the data as it fits better than even the true model.</p>
<pre class="r"><code>set.seed(123) #gives us the same randomness 
n &lt;- 20 #sample size
X &lt;- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8
# Coefficients for the true model:
beta0 &lt;- 2
beta1 &lt;- -2.5
beta2 &lt;- 1
beta3 &lt;- 3
beta4 &lt;- -0.8
# Get y-value using a true model
Y &lt;- beta0 + beta1*X + beta2*X^2 + beta3*X^3 + beta4*X^4 + rnorm(n, 0, 0.5) #normal errors

# Plot it
par(mai=c(.1,.5,.2,.1))
plot(Y ~ X, pch=21, col=&quot;lightgray&quot;, bg=&quot;steelblue&quot;, cex=1.3, ylim=c(-5,22), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylab=&quot;&quot;, xlab=&quot;&quot;)
mtext(side=3, text=&quot;Original Data (Training Data)&quot;, cex=0.7, at=-.8, line=.1)

# Draw true model
curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4)
lmt &lt;- lm(Y ~ X + I(X^2) + I(X^3) + I(X^4)) #for later

# Draw simple linear model
lms &lt;- lm(Y ~ X)
b &lt;- coef(lms)
curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2)

# Draw overly complicated model
lmo &lt;- lm(Y ~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) + I(X^10) + I(X^11) + I(X^12) + I(X^13) + I(X^14))
b &lt;- coef(lmo)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2)

# Add legend
legend(&quot;topleft&quot;, legend=c(&quot;True Model&quot;, &quot;Simple Model&quot;, &quot;Complicated Model&quot;), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty=&#39;n&#39;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-76-1.png" width="480" /></p>
<pre class="r"><code>## Table is printed using the Rmd code:
#| Model   | $R^2$ | Adjusted $R^2$ |
#|---------|-------|----------------| 
#| True    | `r summary(lmt)$r.squared`  | `r summary(lmt)$adj.r.squared` |
#| Simple  | `r summary(lms)$r.squared`  | `r summary(lms)$adj.r.squared` |
#| Complicated | `r summary(lmo)$r.squared`  | `r summary(lmo)$adj.r.squared` |</code></pre>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th><span class="math inline">\(R^2\)</span></th>
<th>Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>True</td>
<td>0.9958725</td>
<td>0.9947718</td>
</tr>
<tr class="even">
<td>Simple</td>
<td>0.8114836</td>
<td>0.8010105</td>
</tr>
<tr class="odd">
<td>Complicated</td>
<td>0.9984527</td>
<td>0.9941204</td>
</tr>
</tbody>
</table>
<p>Now, let’s remind ourselves why we use regression models in the first place. The main goal is to capture the “essence” of the data. In other words, the general pattern is what we are after. We want a model that tells us how “all such” data is created, not just the specific data we have sampled. So, the great test of a model is to see how well it works on a new sample of data.</p>
<p>This is precisely <strong>model validation</strong>, the verification that a model fit on one sample of data, continues to perform well on a new sample of data.</p>
<pre class="r"><code>set.seed(14551) #get same random sample
# Get a new sample of data from the true model
Xnew &lt;- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8
Ynew &lt;- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors

# Plot it
par(mai=c(.1,.5,.2,.1))
plot(Y ~ X, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylab=&quot;&quot;, xlab=&quot;&quot;)
mtext(side=3, text=&quot;New Data (Testing Data)&quot;, cex=0.7, at=-.8, line=.1)

# Draw true model
curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4)
lmt &lt;- lm(Y ~ X + I(X^2) + I(X^3) + I(X^4)) #for later

# Draw simple linear model
lms &lt;- lm(Y ~ X)
b &lt;- coef(lms)
curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2)

# Draw overly complicated model
lmc &lt;- lm(Y ~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) + I(X^10) + I(X^11) + I(X^12) + I(X^13) + I(X^14))
b &lt;- coef(lmc)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2)

# Add new data to plot
points(Ynew ~ Xnew, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=&quot;orange&quot;, cex=1.3)

# Add legend
legend(&quot;topleft&quot;, legend=c(&quot;True Model&quot;, &quot;Simple Model&quot;, &quot;Complicated Model&quot;), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty=&#39;n&#39;)

# Add dot legend
legend(&quot;bottomright&quot;, legend=c(&quot;Original Sample&quot;, &quot;New Sample&quot;), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),&quot;orange&quot;), bty=&#39;n&#39;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-77-1.png" width="480" /></p>
<pre class="r"><code># Compute R-squared for each validation

  # Get y-hat for each model on new data.
  yht &lt;- predict(lmt, newdata=data.frame(X=Xnew))
  yhs &lt;- predict(lms, newdata=data.frame(X=Xnew))
  yhc &lt;- predict(lmc, newdata=data.frame(X=Xnew))
  
  # Compute y-bar
  ybar &lt;- mean(Ynew) #Yi is given by Ynew
  
  # Compute SSTO
  SSTO &lt;- sum( (Ynew - ybar)^2 )
  
  # Compute SSE for each model
  SSEt &lt;- sum( (Ynew - yht)^2 )
  SSEs &lt;- sum( (Ynew - yhs)^2 )
  SSEc &lt;- sum( (Ynew - yhc)^2 )
  
  # Compute R-squared for each
  rst &lt;- 1 - SSEt/SSTO
  rss &lt;- 1 - SSEs/SSTO
  rsc &lt;- 1 - SSEc/SSTO
  
  # Compute adjusted R-squared for each
  n &lt;- length(Ynew)
  pt &lt;- length(coef(lmt))
  ps &lt;- length(coef(lms))
  pc &lt;- length(coef(lmc))
  rsta &lt;- 1 - (n-1)/(n-pt)*SSEt/SSTO
  rssa &lt;- 1 - (n-1)/(n-ps)*SSEs/SSTO
  rsca &lt;- 1 - (n-1)/(n-pc)*SSEc/SSTO
  
## Table is printed using the Rmd code:
#| Model   | $R^2$ | Adjusted $R^2$ |
#|---------|-------|----------------|
#| True    | `r rst`  | `r rsta` |
#| Simple  | `r rss`  | `r rssa` |
#| Complicated | &lt;span style=&quot;background-color:yellow;&quot;&gt;`r rsc`&lt;/span&gt;  | &lt;span style=&quot;background-color:yellow;&quot;&gt;`r rsca`&lt;/span&gt; |</code></pre>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th><span class="math inline">\(R^2\)</span></th>
<th>Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>True</td>
<td>0.9927525</td>
<td>0.9908199</td>
</tr>
<tr class="even">
<td>Simple</td>
<td>0.8002201</td>
<td>0.7891212</td>
</tr>
<tr class="odd">
<td>Complicated</td>
<td><span style="background-color:yellow;">0.8686253</span></td>
<td><span style="background-color:yellow;">0.5007763</span></td>
</tr>
</tbody>
</table>
<p>Notice how the <span class="math inline">\(R^2\)</span> for the complicated model dropped fairly dramatically from its original value of 0.9985 to 0.8686, and the adjusted <span class="math inline">\(R^2\)</span> dropped from 0.994 to 0.501! On the other hand, the <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span> values for the True and Simple model were relatively unchanged. This is clear evidence that the “complicated model” is overfitting the original data. It does not capture the “essence” of the data, so it is not a generalizable model. It does not fit new data very well, even though it fit the original sample of data quite well. This is what we mean by <strong>over fitting</strong> a model to a particular sample of data.</p>
</div>
<p><br/></p>
<h4 id="interpretation-expand">Interpretation <a href="javascript:showhide('interpretationMultiple')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span class="math inline">\(\beta_j\)</span> is the change in the average y-value…</span></p>
<div id="interpretationMultiple" style="display:none;">
<p>The only change to interpretation from the simple linear regression model is that each coefficient, <span class="math inline">\(\beta_j\)</span> <span class="math inline">\(j=1,\ldots,p\)</span>, represents the change in the <span class="math inline">\(E\{Y\}\)</span> for a unit change in <span class="math inline">\(X_j\)</span>, <em>holding all other variables constant.</em></p>
</div>
<p><br /></p>
<h4 id="added-variable-plots-expand">Added Variable Plots <a href="javascript:showhide('addedVariablePlots')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">When to add another <span class="math inline">\(X\)</span>-variable to the model…</span></p>
<div id="addedVariablePlots" style="display:none;">
<p>The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption.</p>
<ol style="list-style-type: decimal">
<li>The regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered fixed and measured without error.</li>
<li>The error terms are independent.</li>
<li>All important variables are included in the model.</li>
</ol>
<p><br /></p>
<h4 id="check">Checking the Assumptions</h4>
<p>The process of checking assumptions is the same for multiple linear regression as it is for simple linear regression, with the addition of one more tool, the added variable plot. Added variable plots can be used to determine if a new variable should be included in the model.</p>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-78-1.png" width="144" />
</td>
<td width="75%">
<p>Let <span class="math inline">\(X_{new}\)</span> be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against <span class="math inline">\(X_{new}\)</span> allows us to determine if <span class="math inline">\(X_{new}\)</span> has any information to add to the current model. If there is a trend in the plot, then <span class="math inline">\(X_{new}\)</span> should be added to the model. If there is no trend in the plot, then the <span class="math inline">\(X_{new}\)</span> should be left out.</p>
<p>| <a href="javascript:showhide('addedvariableplots')" style="font-size:.8em;color:steelblue2;">Show Examples</a> |</p>
</td>
</tr>
</table>
<div id="addedvariableplots" style="display:none;">
<p><a href="javascript:showhide('addedvariableplotsread')" style="font-size:.8em;color:skyblue;">(Read more…)</a></p>
<div id="addedvariableplotsread" style="display:none;">
<p>An added variable plot checks to see if a new variable has any information to add to the current multiple regression model.</p>
<p>The plot is made by taking the residuals from the current multiple regression model (<span class="math inline">\(y\)</span>-axis) and plotting them against the new explanatory variable (<span class="math inline">\(x\)</span>-axis).</p>
<ul>
<li><p>If there is a trend in the added variable plot, then the new explanatory variable contains extra information that is not already contained in the current multiple regression. The new variable should be included in the model.</p></li>
<li><p>If there is no trend in the added variable plot, then the information provided by the new explanatory variable is already contained in the current multiple regression model. The new variable should continue to be left out of the model.</p></li>
</ul>
<p>The left column of plots below show scenarios where the new explanatory variable should be included in the model. The right column of plots show scenarios where the new explanatory variable should not be included in the model.</p>
</div>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-79-1.png" width="672" /><img src="LinearRegression_files/figure-html/unnamed-chunk-79-2.png" width="672" /></p>
</div>
</div>
<p><br /></p>
<h4 id="inference-for-the-model-parameters-expand-1">Inference for the Model Parameters <a href="javascript:showhide('inferenceMultiple')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">t Tests and F tests in multiple regression…</span></p>
<div id="inferenceMultiple" style="display:none;">
<p>Inference in the multiple regression model can be for any of the model coefficients, <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_p\)</span> or for several coefficients simultaneously.</p>
<p><br /></p>
<h5 id="t-tests">t Tests</h5>
<p>The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as <span class="math display">\[
  H_0: \beta_j = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0
\]</span> Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance.</p>
<p><br /></p>
<h5 id="f-tests">F Tests</h5>
<p>Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously.</p>
<p>The most commonly used F Test is the one given by the hypotheses <span class="math display">\[
  H_0: \beta_1 = \cdots = \beta_p = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0 \ \text{for at least one}\ j \in \{1,\ldots,p\}
\]</span> However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class.</p>
</div>
<p><br /> <br /></p>
<hr />
</div>
</div>
</div>
<div id="section-1" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a href="./Analyses/Linear%20Regression/Examples/CivicVsCorollaMLR.html">Civic Vs Corolla</a> <a href="./Analyses/Linear%20Regression/Examples/cadillacsMLR.html">cadillacs</a></p>
</div>
<hr />
<footer>
</footer>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
